#!/usr/bin/env python3
"""
ç®€æŠ¥è´¨é‡æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. æ•°æ®ä¸€è‡´æ€§ï¼štitleã€linkã€summary æ˜¯å¦åŒ¹é…
2. é“¾æ¥æœ‰æ•ˆæ€§ï¼šURL æ ¼å¼æ˜¯å¦æ­£ç¡®
3. ç‰ˆé¢å®Œæ•´æ€§ï¼šå¿…è¦çš„æ¿å—æ˜¯å¦å­˜åœ¨
4. å†…å®¹è´¨é‡ï¼šsummary æ˜¯å¦ä¸ºç©ºæˆ–è¿‡çŸ­
5. åˆ†ç±»æ­£ç¡®æ€§ï¼šè®ºæ–‡æ˜¯å¦åœ¨è®ºæ–‡æ¿å—ï¼Œæ–°é—»æ˜¯å¦åœ¨å¤´æ¡æ¿å—
"""

import sys
import re
import json
from pathlib import Path
from typing import Dict, List, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class ReportQualityTester:
    """ç®€æŠ¥è´¨é‡æµ‹è¯•å™¨"""
    
    def __init__(self, html_path: str, json_path: str = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            html_path: HTMLæŠ¥å‘Šè·¯å¾„
            json_path: åŸå§‹JSONæ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.html_path = Path(html_path)
        self.json_path = Path(json_path) if json_path else None
        self.soup = None
        self.original_data = None
        self.errors = []
        self.warnings = []
        self.stats = {
            'total_items': 0,
            'headlines': 0,
            'papers': 0,
            'empty_summaries': 0,
            'invalid_urls': 0,
            'data_mismatches': 0
        }
        
    def load_files(self):
        """åŠ è½½æ–‡ä»¶"""
        # åŠ è½½HTML
        with open(self.html_path, 'r', encoding='utf-8') as f:
            self.soup = BeautifulSoup(f.read(), 'html.parser')
        
        # åŠ è½½åŸå§‹JSONï¼ˆå¦‚æœæä¾›ï¼‰
        if self.json_path and self.json_path.exists():
            with open(self.json_path, 'r', encoding='utf-8') as f:
                self.original_data = json.load(f)
    
    def test_layout_completeness(self) -> bool:
        """æµ‹è¯•ç‰ˆé¢å®Œæ•´æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ“ æµ‹è¯•1: ç‰ˆé¢å®Œæ•´æ€§")
        print("=" * 80)
        
        passed = True
        
        # æ£€æŸ¥å¿…è¦çš„æ¿å—
        required_sections = {
            'ä»Šæ—¥å¤´æ¡': 'ğŸ”¥ ä»Šæ—¥å¤´æ¡',
            'æ·±åº¦': 'ğŸ“„ æ·±åº¦'
        }
        
        for name, pattern in required_sections.items():
            section = self.soup.find('h2', string=lambda x: x and pattern in x)
            if section:
                print(f"  âœ… {name}æ¿å—å­˜åœ¨")
            else:
                print(f"  âŒ {name}æ¿å—ç¼ºå¤±")
                self.errors.append(f"ç¼ºå°‘{name}æ¿å—")
                passed = False
        
        # æ£€æŸ¥å¿…è¦çš„å…ƒç´ 
        required_elements = {
            'title': ('title', 'é¡µé¢æ ‡é¢˜'),
            'container': ('div', 'ä¸»å®¹å™¨', {'class': 'container'}),
            'item-card': ('div', 'å†…å®¹å¡ç‰‡', {'class': 'item-card'})
        }
        
        for key, (tag, desc, *attrs) in required_elements.items():
            kwargs = attrs[0] if attrs else {}
            element = self.soup.find(tag, **kwargs)
            if element:
                print(f"  âœ… {desc}å­˜åœ¨")
            else:
                print(f"  âŒ {desc}ç¼ºå¤±")
                self.errors.append(f"ç¼ºå°‘{desc}")
                passed = False
        
        return passed
    
    def test_data_consistency(self) -> bool:
        """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ” æµ‹è¯•2: æ•°æ®ä¸€è‡´æ€§ï¼ˆTitle vs Summaryï¼‰")
        print("=" * 80)
        
        passed = True
        items = self.soup.find_all('div', class_='item-card')
        self.stats['total_items'] = len(items)
        
        print(f"\næ€»è®¡æ£€æŸ¥: {len(items)} æ¡å†…å®¹\n")
        
        for i, item in enumerate(items, 1):
            title_div = item.find('div', class_='item-title')
            if not title_div:
                continue
            
            title = title_div.text.strip()
            # ç§»é™¤ä¼˜å…ˆçº§æ ‡ç­¾
            title = re.sub(r'ğŸ“Š\s*\d+/10', '', title).strip()
            
            content_div = item.find('div', class_='item-content')
            if not content_div:
                self.warnings.append(f"æ¡ç›® {i} ç¼ºå°‘å†…å®¹åŒºåŸŸ")
                continue
            
            # è·å–æ‘˜è¦
            paras = content_div.find_all('p')
            summary = ""
            for p in paras:
                text = p.text
                if 'æ‘˜è¦' in text or 'ğŸ“' in text:
                    summary = text.replace('ğŸ“ æ‘˜è¦ï¼š', '').replace('æ‘˜è¦ï¼š', '').strip()
                    break
            
            if not summary:
                self.warnings.append(f"æ¡ç›® {i} æ²¡æœ‰æ‘˜è¦")
                continue
            
            # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
            is_consistent = self._check_consistency(title, summary)
            
            if is_consistent:
                print(f"  âœ… æ¡ç›® {i}: {title[:50]}...")
            else:
                print(f"  âš ï¸  æ¡ç›® {i}: {title[:50]}...")
                print(f"      æ‘˜è¦: {summary[:80]}...")
                self.warnings.append(f"æ¡ç›® {i} æ•°æ®ä¸€è‡´æ€§å¯ç–‘: {title[:30]}")
                self.stats['data_mismatches'] += 1
        
        if self.stats['data_mismatches'] > 0:
            print(f"\nâš ï¸  å‘ç° {self.stats['data_mismatches']} æ¡æ½œåœ¨ä¸ä¸€è‡´")
            passed = False
        
        return passed
    
    def _check_consistency(self, title: str, summary: str) -> bool:
        """
        æ£€æŸ¥titleå’Œsummaryçš„ä¸€è‡´æ€§
        
        Args:
            title: æ ‡é¢˜
            summary: æ‘˜è¦
            
        Returns:
            æ˜¯å¦ä¸€è‡´
        """
        # æ¸…ç†æ–‡æœ¬
        title_clean = re.sub(r'[^\w\s]', ' ', title.lower())
        summary_clean = re.sub(r'[^\w\s]', ' ', summary.lower())
        
        # æå–å…³é”®è¯ï¼ˆé•¿åº¦>3ï¼Œæ’é™¤å¸¸è§è¯ï¼‰
        stop_words = {'the', 'and', 'for', 'with', 'from', 'that', 'this', 'what', 
                     'when', 'where', 'like', 'have', 'been', 'will', 'can', 'are'}
        title_words = [w for w in title_clean.split() if len(w) > 3 and w not in stop_words]
        
        if not title_words:
            return True  # æ— æ³•åˆ¤æ–­
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…ç‡
        match_count = sum(1 for word in title_words if word in summary_clean)
        match_rate = match_count / len(title_words)
        
        # åŒ¹é…ç‡ä½äº20%è®¤ä¸ºä¸ä¸€è‡´
        return match_rate >= 0.2
    
    def test_url_validity(self) -> bool:
        """æµ‹è¯•URLæœ‰æ•ˆæ€§"""
        print("\n" + "=" * 80)
        print("ğŸ”— æµ‹è¯•3: URLæœ‰æ•ˆæ€§")
        print("=" * 80)
        
        passed = True
        items = self.soup.find_all('div', class_='item-card')
        
        print(f"\næ€»è®¡æ£€æŸ¥: {len(items)} æ¡é“¾æ¥\n")
        
        for i, item in enumerate(items, 1):
            link = item.find('a', class_='item-link')
            if not link:
                print(f"  âŒ æ¡ç›® {i} ç¼ºå°‘é“¾æ¥")
                self.errors.append(f"æ¡ç›® {i} ç¼ºå°‘é“¾æ¥")
                self.stats['invalid_urls'] += 1
                passed = False
                continue
            
            url = link.get('href', '')
            if not url:
                print(f"  âŒ æ¡ç›® {i} é“¾æ¥ä¸ºç©º")
                self.errors.append(f"æ¡ç›® {i} é“¾æ¥ä¸ºç©º")
                self.stats['invalid_urls'] += 1
                passed = False
                continue
            
            # éªŒè¯URLæ ¼å¼
            try:
                parsed = urlparse(url)
                if not parsed.scheme or not parsed.netloc:
                    print(f"  âŒ æ¡ç›® {i} URLæ ¼å¼æ— æ•ˆ: {url}")
                    self.errors.append(f"æ¡ç›® {i} URLæ ¼å¼æ— æ•ˆ")
                    self.stats['invalid_urls'] += 1
                    passed = False
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„æœ‰æ•ˆåŸŸå
                    valid_domains = [
                        'arxiv.org', 'huggingface.co', 'paperswithcode.com',
                        'github.com', 'techcrunch.com', 'venturebeat.com',
                        'theverge.com', 'news.ycombinator.com', 'blog.google',
                        'openai.com', 'anthropic.com', 'simonwillison.net'
                    ]
                    
                    is_known_domain = any(domain in parsed.netloc for domain in valid_domains)
                    
                    if is_known_domain or len(parsed.netloc.split('.')) >= 2:
                        print(f"  âœ… æ¡ç›® {i}: {parsed.netloc}")
                    else:
                        print(f"  âš ï¸  æ¡ç›® {i}: æœªçŸ¥åŸŸå {parsed.netloc}")
                        self.warnings.append(f"æ¡ç›® {i} ä½¿ç”¨æœªçŸ¥åŸŸå: {parsed.netloc}")
            except Exception as e:
                print(f"  âŒ æ¡ç›® {i} URLè§£æå¤±è´¥: {str(e)}")
                self.errors.append(f"æ¡ç›® {i} URLè§£æå¤±è´¥")
                self.stats['invalid_urls'] += 1
                passed = False
        
        return passed
    
    def test_content_quality(self) -> bool:
        """æµ‹è¯•å†…å®¹è´¨é‡"""
        print("\n" + "=" * 80)
        print("ğŸ“ æµ‹è¯•4: å†…å®¹è´¨é‡")
        print("=" * 80)
        
        passed = True
        items = self.soup.find_all('div', class_='item-card')
        
        print(f"\næ€»è®¡æ£€æŸ¥: {len(items)} æ¡å†…å®¹\n")
        
        english_summaries = 0
        
        for i, item in enumerate(items, 1):
            title_div = item.find('div', class_='item-title')
            if not title_div:
                continue
            
            title = title_div.text.strip()
            title = re.sub(r'ğŸ“Š\s*\d+/10', '', title).strip()
            
            content_div = item.find('div', class_='item-content')
            if not content_div:
                print(f"  âŒ æ¡ç›® {i} ç¼ºå°‘å†…å®¹: {title[:40]}...")
                self.errors.append(f"æ¡ç›® {i} ç¼ºå°‘å†…å®¹")
                passed = False
                continue
            
            # æ£€æŸ¥æ‘˜è¦
            paras = content_div.find_all('p')
            summary = ""
            for p in paras:
                text = p.text
                if 'æ‘˜è¦' in text or 'ğŸ“' in text:
                    summary = text.replace('ğŸ“ æ‘˜è¦ï¼š', '').replace('æ‘˜è¦ï¼š', '').strip()
                    break
            
            # æ£€æŸ¥æ‘˜è¦è´¨é‡
            if not summary:
                print(f"  âŒ æ¡ç›® {i} æ‘˜è¦ä¸ºç©º: {title[:40]}...")
                self.errors.append(f"æ¡ç›® {i} æ‘˜è¦ä¸ºç©º")
                self.stats['empty_summaries'] += 1
                passed = False
            elif len(summary) < 20:
                print(f"  âš ï¸  æ¡ç›® {i} æ‘˜è¦è¿‡çŸ­ ({len(summary)}å­—): {title[:40]}...")
                self.warnings.append(f"æ¡ç›® {i} æ‘˜è¦è¿‡çŸ­")
                self.stats['empty_summaries'] += 1
            elif summary == '...':
                print(f"  âŒ æ¡ç›® {i} æ‘˜è¦ä¸ºå ä½ç¬¦: {title[:40]}...")
                self.errors.append(f"æ¡ç›® {i} æ‘˜è¦ä¸ºå ä½ç¬¦")
                self.stats['empty_summaries'] += 1
                passed = False
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡æ‘˜è¦
                is_chinese = self._is_chinese_text(summary)
                if not is_chinese:
                    print(f"  âš ï¸  æ¡ç›® {i} æ‘˜è¦éä¸­æ–‡: {title[:40]}...")
                    print(f"      æ‘˜è¦: {summary[:60]}...")
                    self.warnings.append(f"æ¡ç›® {i} æ‘˜è¦éä¸­æ–‡")
                    english_summaries += 1
                else:
                    print(f"  âœ… æ¡ç›® {i}: æ‘˜è¦é•¿åº¦ {len(summary)} å­— (ä¸­æ–‡)")
        
        if english_summaries > 0:
            print(f"\nâš ï¸  å‘ç° {english_summaries} æ¡éä¸­æ–‡æ‘˜è¦")
        
        return passed
    
    def _is_chinese_text(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºä¸­æ–‡
        
        Args:
            text: å¾…æ£€æŸ¥æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºä¸­æ–‡æ–‡æœ¬
        """
        if not text:
            return False
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        # ç»Ÿè®¡è‹±æ–‡å­—æ¯æ•°é‡
        english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
        
        total_chars = len(text.replace(' ', '').replace('\n', ''))
        
        if total_chars == 0:
            return False
        
        # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æœ¬
        chinese_ratio = chinese_chars / total_chars
        
        # æˆ–è€…è‹±æ–‡å­—ç¬¦å¾ˆå°‘ï¼ˆå…è®¸ä¸€äº›ä¸“ä¸šæœ¯è¯­å¦‚RAGã€LLMï¼‰
        if chinese_ratio > 0.3 or (english_chars < 50 and chinese_chars > 10):
            return True
        
        return False
    
    def test_categorization(self) -> bool:
        """æµ‹è¯•åˆ†ç±»æ­£ç¡®æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ·ï¸  æµ‹è¯•5: åˆ†ç±»æ­£ç¡®æ€§")
        print("=" * 80)
        
        passed = True
        
        # æ£€æŸ¥ä»Šæ—¥å¤´æ¡
        headlines_section = self.soup.find('h2', string=lambda x: x and 'ä»Šæ—¥å¤´æ¡' in x)
        if headlines_section:
            items = []
            next_elem = headlines_section.find_next_sibling()
            while next_elem and next_elem.name == 'div' and 'item-card' in next_elem.get('class', []):
                items.append(next_elem)
                next_elem = next_elem.find_next_sibling()
            
            self.stats['headlines'] = len(items)
            print(f"\nğŸ“° ä»Šæ—¥å¤´æ¡: {len(items)} æ¡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è®ºæ–‡æ··å…¥
            for i, item in enumerate(items, 1):
                category = item.get('data-item-category', '')
                source = item.get('data-item-source', '')
                title = item.find('div', class_='item-title')
                title_text = title.text.strip() if title else ''
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯è®ºæ–‡
                is_paper = (category == 'paper' or 
                           'arxiv' in source.lower() or
                           'hugging face papers' in source.lower() or
                           'papers with code' in source.lower())
                
                if is_paper:
                    print(f"  âŒ æ¡ç›® {i} æ˜¯è®ºæ–‡ä½†åœ¨å¤´æ¡æ¿å—: {title_text[:40]}...")
                    self.errors.append(f"è®ºæ–‡æ··å…¥å¤´æ¡æ¿å—: {title_text[:30]}")
                    passed = False
        
        # æ£€æŸ¥æ·±åº¦/è®ºæ–‡æ¿å—
        papers_section = self.soup.find('h2', string=lambda x: x and 'æ·±åº¦' in x)
        if papers_section:
            items = []
            next_elem = papers_section.find_next_sibling()
            while next_elem and next_elem.name == 'div' and 'item-card' in next_elem.get('class', []):
                items.append(next_elem)
                next_elem = next_elem.find_next_sibling()
            
            self.stats['papers'] = len(items)
            print(f"\nğŸ“„ æ·±åº¦ï¼ˆè®ºæ–‡ï¼‰: {len(items)} ç¯‡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éè®ºæ–‡æ··å…¥
            for i, item in enumerate(items, 1):
                category = item.get('data-item-category', '')
                source = item.get('data-item-source', '')
                title = item.find('div', class_='item-title')
                title_text = title.text.strip() if title else ''
                
                # æ£€æŸ¥æ˜¯å¦ä¸æ˜¯è®ºæ–‡
                is_not_paper = (category not in ['paper', 'Paper'] and
                               'arxiv' not in source.lower() and
                               'hugging face papers' not in source.lower() and
                               'papers with code' not in source.lower())
                
                if is_not_paper:
                    print(f"  âš ï¸  æ¡ç›® {i} ä¸æ˜¯è®ºæ–‡ä½†åœ¨è®ºæ–‡æ¿å—: {title_text[:40]}...")
                    self.warnings.append(f"éè®ºæ–‡æ··å…¥è®ºæ–‡æ¿å—: {title_text[:30]}")
        
        return passed
    
    def test_metadata_completeness(self) -> bool:
        """æµ‹è¯•å…ƒæ•°æ®å®Œæ•´æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ æµ‹è¯•6: å…ƒæ•°æ®å®Œæ•´æ€§")
        print("=" * 80)
        
        passed = True
        items = self.soup.find_all('div', class_='item-card')
        
        print(f"\næ€»è®¡æ£€æŸ¥: {len(items)} æ¡å†…å®¹\n")
        
        required_metadata = ['data-item-id', 'data-item-title', 'data-item-url', 
                            'data-item-source', 'data-item-category']
        
        for i, item in enumerate(items, 1):
            missing = []
            for attr in required_metadata:
                if not item.get(attr):
                    missing.append(attr.replace('data-item-', ''))
            
            if missing:
                title = item.find('div', class_='item-title')
                title_text = title.text.strip()[:40] if title else 'Unknown'
                print(f"  âš ï¸  æ¡ç›® {i} ç¼ºå°‘å…ƒæ•°æ®: {', '.join(missing)}")
                print(f"      æ ‡é¢˜: {title_text}...")
                self.warnings.append(f"æ¡ç›® {i} ç¼ºå°‘å…ƒæ•°æ®: {', '.join(missing)}")
            else:
                print(f"  âœ… æ¡ç›® {i} å…ƒæ•°æ®å®Œæ•´")
        
        return passed
    
    def test_model_configuration(self) -> bool:
        """æµ‹è¯•æ¨¡å‹é…ç½®"""
        print("\n" + "=" * 80)
        print("ğŸ¤– æµ‹è¯•7: æ¨¡å‹é…ç½®")
        print("=" * 80)
        
        passed = True
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        import os
        model = os.getenv('DEVELOPER_MODEL', 'Claude-Sonnet-4.5')
        print(f"  â„¹ï¸  ç¯å¢ƒå˜é‡ DEVELOPER_MODEL: {model}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨èæ¨¡å‹
        if 'Sonnet' in model or 'sonnet' in model:
            print(f"  âœ… ä½¿ç”¨æ¨èæ¨¡å‹: {model}")
        elif 'Haiku' in model or 'haiku' in model:
            print(f"  âš ï¸  å½“å‰ä½¿ç”¨ Haiku æ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨ Sonnet ä»¥è·å¾—æ›´å¥½çš„ä¸­æ–‡æ‘˜è¦è´¨é‡")
            self.warnings.append(f"å½“å‰ä½¿ç”¨ {model}ï¼Œå»ºè®®ä½¿ç”¨ Sonnet")
        else:
            print(f"  â„¹ï¸  å½“å‰æ¨¡å‹: {model}")
        
        return passed
    
    def run_all_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "=" * 80)
        print("ğŸ§ª ç®€æŠ¥è´¨é‡æµ‹è¯•")
        print("=" * 80)
        print(f"æ–‡ä»¶: {self.html_path}")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åŠ è½½æ–‡ä»¶
        try:
            self.load_files()
        except Exception as e:
            print(f"\nâŒ æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            return False
        
        # è¿è¡Œæµ‹è¯•
        results = {
            'ç‰ˆé¢å®Œæ•´æ€§': self.test_layout_completeness(),
            'æ•°æ®ä¸€è‡´æ€§': self.test_data_consistency(),
            'URLæœ‰æ•ˆæ€§': self.test_url_validity(),
            'å†…å®¹è´¨é‡': self.test_content_quality(),
            'åˆ†ç±»æ­£ç¡®æ€§': self.test_categorization(),
            'å…ƒæ•°æ®å®Œæ•´æ€§': self.test_metadata_completeness(),
            'æ¨¡å‹é…ç½®': self.test_model_configuration()
        }
        
        # æ‰“å°æ€»ç»“
        self.print_summary(results)
        
        # è¿”å›æ€»ä½“ç»“æœ
        return all(results.values()) and len(self.errors) == 0
    
    def print_summary(self, results: Dict[str, bool]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("=" * 80)
        
        # æµ‹è¯•ç»“æœ
        print("\næµ‹è¯•ç»“æœ:")
        for test_name, passed in results.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"  {test_name}: {status}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\nç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ¡ç›®æ•°: {self.stats['total_items']}")
        print(f"  ä»Šæ—¥å¤´æ¡: {self.stats['headlines']} æ¡")
        print(f"  è®ºæ–‡: {self.stats['papers']} ç¯‡")
        print(f"  ç©ºæ‘˜è¦: {self.stats['empty_summaries']} æ¡")
        print(f"  æ— æ•ˆURL: {self.stats['invalid_urls']} æ¡")
        print(f"  æ•°æ®ä¸ä¸€è‡´: {self.stats['data_mismatches']} æ¡")
        
        # é”™è¯¯åˆ—è¡¨
        if self.errors:
            print(f"\nâŒ é”™è¯¯ ({len(self.errors)} ä¸ª):")
            for error in self.errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {error}")
            if len(self.errors) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.errors) - 10} ä¸ªé”™è¯¯")
        
        # è­¦å‘Šåˆ—è¡¨
        if self.warnings:
            print(f"\nâš ï¸  è­¦å‘Š ({len(self.warnings)} ä¸ª):")
            for warning in self.warnings[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {warning}")
            if len(self.warnings) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.warnings) - 10} ä¸ªè­¦å‘Š")
        
        # æ€»ä½“è¯„ä»·
        print("\n" + "=" * 80)
        if len(self.errors) == 0:
            if len(self.warnings) == 0:
                print("âœ… æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ç®€æŠ¥è´¨é‡ä¼˜ç§€ï¼")
            else:
                print(f"âš ï¸  æµ‹è¯•é€šè¿‡ï¼Œä½†æœ‰ {len(self.warnings)} ä¸ªè­¦å‘Šéœ€è¦å…³æ³¨")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥ï¼å‘ç° {len(self.errors)} ä¸ªé”™è¯¯")
        print("=" * 80 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç®€æŠ¥è´¨é‡æµ‹è¯•')
    parser.add_argument('--html', type=str, 
                       default='output/weekly_report_2025-11-17.html',
                       help='HTMLæŠ¥å‘Šè·¯å¾„')
    parser.add_argument('--json', type=str,
                       default='output/collected_items_2025-11-17_163447.json',
                       help='åŸå§‹JSONæ•°æ®è·¯å¾„')
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ReportQualityTester(args.html, args.json)
    
    # è¿è¡Œæµ‹è¯•
    success = tester.run_all_tests()
    
    # è¿”å›é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()

