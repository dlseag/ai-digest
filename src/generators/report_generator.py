"""
Report Generator
æŠ¥å‘Šç”Ÿæˆå™¨ï¼šä½¿ç”¨Jinja2æ¨¡æ¿ç”ŸæˆMarkdownå‘¨æŠ¥
"""

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
import os

from src.learning.weight_adjuster import WeightAdjuster
from src.learning.reranker import ContentReranker, ProjectActivityTracker
from src.memory.user_profile_manager import UserProfileManager
from src.utils.dedupe import make_dedupe_key, mark_unique

logger = logging.getLogger(__name__)


class ReportGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        template_dir: Optional[str] = None,
        version: str = "0.1.0",
        headline_source_limit: int = 2,
    ):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            template_dir: æ¨¡æ¿ç›®å½•è·¯å¾„
            version: ç³»ç»Ÿç‰ˆæœ¬å·
        """
        # é»˜è®¤æ¨¡æ¿ç›®å½•
        if template_dir is None:
            project_root = Path(__file__).parent.parent.parent
            template_dir = str(project_root / "templates")
        else:
            project_root = Path(template_dir).parent
        
        self.template_dir = template_dir
        self.version = version
        self.headline_source_limit = max(1, headline_source_limit)
        self.project_root = project_root
        deep_dive_env = os.getenv("DEEP_DIVE_ENABLED", "false").strip().lower()
        self.deep_dive_enabled = deep_dive_env in ("1", "true", "yes", "on")
        
        # åˆå§‹åŒ–æƒé‡è°ƒæ•´å™¨
        try:
            self.weight_adjuster = WeightAdjuster()
            logger.info("âœ“ æƒé‡è°ƒæ•´å™¨å·²åŠ è½½")
        except Exception as e:
            logger.warning(f"âš ï¸  æƒé‡è°ƒæ•´å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.weight_adjuster = None
        
        # åˆå§‹åŒ–ç”¨æˆ·ç”»åƒç®¡ç†å™¨ï¼ˆç”¨äºé‡æ’ï¼‰
        try:
            profile_path = self.project_root / "config" / "user_profile.yaml"
            if profile_path.exists():
                self.profile_manager = UserProfileManager(profile_path=profile_path)
                logger.info("âœ“ ç”¨æˆ·ç”»åƒç®¡ç†å™¨å·²åŠ è½½")
            else:
                logger.info("â„¹ï¸  ç”¨æˆ·ç”»åƒæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡é‡æ’åŠŸèƒ½")
                self.profile_manager = None
        except Exception as e:
            logger.warning(f"âš ï¸  ç”¨æˆ·ç”»åƒç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.profile_manager = None
        
        # åˆå§‹åŒ–é‡æ’å™¨
        try:
            self.reranker = ContentReranker(
                profile_manager=self.profile_manager,
                weight_adjuster=self.weight_adjuster,
            )
            logger.info("âœ“ å†…å®¹é‡æ’å™¨å·²åŠ è½½")
        except Exception as e:
            logger.warning(f"âš ï¸  å†…å®¹é‡æ’å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.reranker = None
        
        # åˆå§‹åŒ–Jinja2ç¯å¢ƒ
        self.env = Environment(
            loader=FileSystemLoader(template_dir),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # æ·»åŠ è‡ªå®šä¹‰è¿‡æ»¤å™¨
        import hashlib
        def md5_filter(text):
            return hashlib.md5(str(text).encode()).hexdigest()
        self.env.filters['md5'] = md5_filter
        
        logger.info(f"âœ“ æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡æ¿ç›®å½•ï¼š{template_dir}ï¼‰")
    
    def generate_report(
        self,
        processed_items: List,
        action_items: Dict[str, List[str]],
        leaderboard_data: List[Dict] = None,
        leaderboard_update_time: str = '',
        market_insights: List[Dict] = None,
        output_path: Optional[str] = None,
        learning_results: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        ç”Ÿæˆå‘¨æŠ¥
        
        Args:
            processed_items: AIå¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            action_items: è¡ŒåŠ¨æ¸…å•
            leaderboard_data: LMSYSæ’è¡Œæ¦œæ•°æ®
            leaderboard_update_time: æ’è¡Œæ¦œæ›´æ–°æ—¶é—´
            market_insights: å¸‚åœºæ´å¯Ÿæ•°æ®
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„Markdownæ–‡æœ¬
        """
        # æŒ‰ç±»åˆ«åˆ†ç»„
        categorized = self._categorize_items(processed_items)

        # å»é‡æ§åˆ¶ï¼šä¼˜å…ˆçº§é«˜çš„æ¿å—å…ˆå ä½
        used_keys: Set[str] = set()

        sorted_by_priority = sorted(
            processed_items,
            key=lambda x: getattr(x, 'personal_priority', getattr(x, 'relevance_score', 0)),
            reverse=True
        )

        must_read_items: List[Any] = []
        source_counts: Dict[str, int] = {}  # è¿½è¸ªæ¯ä¸ªæ¥æºçš„æ•°é‡
        max_per_source = 2  # æ¯ä¸ªæ¥æºæœ€å¤š2æ¡
        
        for item in sorted_by_priority:
            if getattr(item, 'personal_priority', 0) < 8:
                continue
            if getattr(item, 'is_release', False) and not getattr(item, 'promote_release', False):
                continue

            if not mark_unique(item, used_keys, self._make_dedupe_key):
                continue
            
            # æ£€æŸ¥æ¥æºå¤šæ ·æ€§
            source = getattr(item, 'source', '')
            source_key = self._normalize_source(source)
            
            # é™åˆ¶arXivè®ºæ–‡æ•°é‡ï¼ˆæœ€å¤š2æ¡ï¼‰
            if 'arxiv' in source_key.lower():
                if source_counts.get('arxiv', 0) >= max_per_source:
                    continue
            
            # é™åˆ¶åŒä¸€æ¥æºçš„æ•°é‡
            if source_counts.get(source_key, 0) >= max_per_source:
                continue

            must_read_items.append(item)
            setattr(item, 'exploration_pick', False)
            source_counts[source_key] = source_counts.get(source_key, 0) + 1
            
            # ç‰¹æ®Šå¤„ç†ï¼šarXivç»Ÿä¸€è®¡æ•°
            if 'arxiv' in source_key.lower():
                source_counts['arxiv'] = source_counts.get('arxiv', 0) + 1
            
            if len(must_read_items) >= 5:
                break
        
        # Phase 1.3: åº”ç”¨ç›¸å…³æ€§é‡æ’
        if self.reranker and must_read_items:
            try:
                logger.info(f"ğŸ”„ å¯¹ {len(must_read_items)} æ¡å¿…çœ‹å†…å®¹è¿›è¡Œé‡æ’...")
                must_read_items = self.reranker.rerank_items(must_read_items)
                logger.info("âœ“ é‡æ’å®Œæˆ")
            except Exception as e:
                logger.warning(f"âš ï¸  é‡æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåº: {e}")

        # å¤´æ¡éœ€åœ¨å¿…çœ‹ä¹‹åç­›é€‰ï¼Œé¿å…é‡å¤
        top_headlines = self._select_top_headlines(processed_items, top_count=10, used_keys=used_keys)

        selected_titles = {item.title for item in top_headlines}
        selected_titles.update(item.title for item in must_read_items)

        appendix_items: List[Any] = []
        for item in sorted_by_priority:
            if item.title in selected_titles:
                continue
            priority = getattr(item, 'personal_priority', 0)
            if not (6 <= priority <= 8):
                continue
            if getattr(item, 'is_release', False) and not getattr(item, 'promote_release', False):
                continue

            if not mark_unique(item, used_keys, self._make_dedupe_key):
                continue
            
            appendix_items.append(item)
            if len(appendix_items) >= 15:
                break

        selected_titles.update(item.title for item in appendix_items)
        
        # è¿‡æ»¤action_itemsï¼Œæ’é™¤å·²åœ¨å¿…çœ‹å†…å®¹å’Œå¤´æ¡ä¸­å‡ºç°çš„æ¡ç›®
        filtered_action_items = self._filter_action_items(action_items, must_read_items, top_headlines)
        
        # å‡†å¤‡æ¨¡æ¿æ•°æ®
        paper_radar = self._build_paper_radar(processed_items, used_keys)

        framework_items: List[Any] = []
        for item in categorized.get('framework', []):
            if item.title in selected_titles:
                continue
            if not mark_unique(item, used_keys, self._make_dedupe_key):
                continue
            framework_items.append(item)
            if len(framework_items) >= 5:
                break

        model_items: List[Any] = []
        for item in categorized.get('model', []):
            if item.title in selected_titles:
                continue
            if not mark_unique(item, used_keys, self._make_dedupe_key):
                continue
            model_items.append(item)
            if len(model_items) >= 5:
                break

        article_items: List[Any] = []
        for item in categorized.get('article', []):
            if item.title in selected_titles:
                continue
            if not mark_unique(item, used_keys, self._make_dedupe_key):
                continue
            article_items.append(item)
            if len(article_items) >= 3:
                break

        project_items: List[Any] = []
        for item in categorized.get('project', []):
            if item.title in selected_titles:
                continue
            if not mark_unique(item, used_keys, self._make_dedupe_key):
                continue
            project_items.append(item)
            if len(project_items) >= 3:
                break
        
        template_data = {
            'report_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'version': self.version,
            'deep_dive_enabled': self.deep_dive_enabled,
            'top_headlines': top_headlines,
            'action_items': filtered_action_items,
            'must_read_items': must_read_items,
            'appendix_items': appendix_items,
            'leaderboard_data': leaderboard_data if leaderboard_data else [],
            'leaderboard_update_time': leaderboard_update_time,
            'market_insights': market_insights if market_insights else [],
            'framework_items': framework_items,
            'model_items': model_items,
            'article_items': article_items,
            'project_items': project_items,
            'stats': self._generate_stats(processed_items, categorized),
            'learning_results': learning_results or {},
            'paper_radar': paper_radar,
        }
        
        # æ¸²æŸ“æ¨¡æ¿
        template = self.env.get_template('report_template.md.jinja')
        report_markdown = template.render(**template_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_path:
            self._save_report(report_markdown, output_path)
        
        logger.info("âœ“ å‘¨æŠ¥ç”Ÿæˆå®Œæˆ")
        return report_markdown
    
    def generate_html_report(
        self,
        processed_items: List,
        action_items: Dict[str, List[str]],
        leaderboard_data: List[Dict] = None,
        leaderboard_update_time: str = '',
        market_insights: List[Dict] = None,
        output_path: Optional[str] = None,
        learning_results: Optional[Dict[str, Any]] = None,
        report_id: Optional[str] = None,
    ) -> str:
        """
        ç”ŸæˆHTMLç‰ˆæœ¬çš„å‘¨æŠ¥ï¼ˆå¸¦è¯„åˆ†åŠŸèƒ½ï¼‰
        
        Args:
            processed_items: AIå¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            action_items: è¡ŒåŠ¨æ¸…å•
            leaderboard_data: LMSYSæ’è¡Œæ¦œæ•°æ®
            leaderboard_update_time: æ’è¡Œæ¦œæ›´æ–°æ—¶é—´
            market_insights: å¸‚åœºæ´å¯Ÿæ•°æ®
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            learning_results: å­¦ä¹ ç»“æœ
            report_id: æŠ¥å‘ŠIDï¼ˆç”¨äºè¿½è¸ªï¼‰
            
        Returns:
            ç”Ÿæˆçš„HTMLæ–‡æœ¬
        """
        import hashlib
        from datetime import datetime
        
        # ä¸ºæ¯ä¸ªitemç”Ÿæˆå”¯ä¸€IDå¹¶ç¡®ä¿æœ‰urlå±æ€§ï¼ˆç¡®ä¿æ‰€æœ‰iteméƒ½æœ‰idå’Œurlå±æ€§ï¼‰
        for item in processed_items:
            # ç¡®ä¿æœ‰ url å±æ€§ï¼ˆä» link è½¬æ¢ï¼‰
            if not hasattr(item, 'url') or not getattr(item, 'url', None):
                link = getattr(item, 'link', '')
                if link:
                    setattr(item, 'url', link)
                else:
                    setattr(item, 'url', '')
            
            url = getattr(item, 'url', '')
            title = getattr(item, 'title', '')
            unique_str = f"{url}{title}"
            item_id = hashlib.md5(unique_str.encode()).hexdigest()[:12]
            # åŠ¨æ€æ·»åŠ idå±æ€§
            if not hasattr(item, 'id'):
                setattr(item, 'id', item_id)
            elif not getattr(item, 'id', None):
                setattr(item, 'id', item_id)
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåªä¿ç•™å¤´æ¡å’Œè®ºæ–‡ä¸¤ä¸ªæ¿å—
        used_keys: Set[str] = set()
        
        # é€‰æ‹©å¤´æ¡ï¼ˆæ‰©å±•è‡³15-20æ¡ï¼‰
        top_headlines = self._select_top_headlines(processed_items, top_count=20, used_keys=used_keys)
        
        # é€‰æ‹©è®ºæ–‡ç²¾é€‰
        featured_papers = self._select_featured_papers(processed_items, top_count=10, used_keys=used_keys)
        
        # é€‰æ‹© Fintech ç›¸å…³å†…å®¹ï¼ˆç‹¬ç«‹å»é‡ï¼Œä¸å…±äº«used_keysï¼Œå…è®¸ä¸å¤´æ¡é‡å¤ï¼‰
        fintech_items = self._select_fintech_items(processed_items, top_count=15, used_keys=set())
        
        # å‡†å¤‡æ¨¡æ¿æ•°æ®
        report_date = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        generation_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # è¿‡æ»¤ action_itemsï¼ˆå¦‚æœæä¾›ï¼‰
        filtered_action_items = {}
        if action_items:
            filtered_action_items = self._filter_action_items(action_items, [], top_headlines)
        
        template_data = {
            'report_date': report_date,
            'generation_time': generation_time,
            'report_id': report_id or f"report_{datetime.now().strftime('%Y-%m-%d')}",
            'deep_dive_enabled': self.deep_dive_enabled,
            'top_headlines': top_headlines,
            'featured_papers': featured_papers,
            'fintech_items': fintech_items,
            'leaderboard_data': leaderboard_data or [],
            'leaderboard_update_time': leaderboard_update_time,
            'market_insights': market_insights or [],
            'learning_results': learning_results or {},
        }
        
        # æ¸²æŸ“HTMLæ¨¡æ¿
        template = self.env.get_template('report_template.html.jinja')
        report_html = template.render(**template_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_path:
            html_path = str(output_path).replace('.md', '.html')
            self._save_report(report_html, html_path)
            logger.info(f"âœ“ HTMLæŠ¥å‘Šå·²ä¿å­˜åˆ°: {html_path}")
        
        return report_html
    
    def _apply_dynamic_weight(self, item: Any, base_score: float) -> float:
        """
        åº”ç”¨åŠ¨æ€æƒé‡åˆ°åŸºç¡€åˆ†æ•°
        
        Args:
            item: å†…å®¹é¡¹ç›®
            base_score: åŸºç¡€åˆ†æ•°ï¼ˆrelevance_score æˆ– headline_priorityï¼‰
        
        Returns:
            åŠ æƒåçš„åˆ†æ•°
        """
        if not self.weight_adjuster:
            return base_score
        
        # è·å–æ¥æºæƒé‡
        source = getattr(item, 'source', '')
        source_weight = self.weight_adjuster.get_weight('sources', source)
        
        # è·å–ç±»åˆ«æƒé‡
        category = getattr(item, 'category', '')
        type_weight = self.weight_adjuster.get_weight('content_types', category)
        
        # ç»„åˆæƒé‡ï¼šæ¥æºæƒé‡ * ç±»åˆ«æƒé‡
        combined_weight = source_weight * type_weight
        
        # åº”ç”¨æƒé‡
        weighted_score = base_score * combined_weight
        
        logger.debug(
            f"æƒé‡åº”ç”¨: {source} ({source_weight:.2f}) Ã— {category} ({type_weight:.2f}) "
            f"= {combined_weight:.2f} | {base_score:.1f} â†’ {weighted_score:.1f}"
        )
        
        return weighted_score
    
    def _select_featured_papers(self, processed_items: List, top_count: int = 8, used_keys: Optional[Set[str]] = None) -> List:
        """
        é€‰æ‹©è®ºæ–‡ç²¾é€‰ï¼Œåªä¿ç•™Hugging Face Paperså’ŒPapers with Code
        
        ç­–ç•¥ï¼š
        1. åªåŒ…å«Hugging Face Paperså’ŒPapers with Codeï¼ˆæ’é™¤arXivç›´æ¥æ¥æºï¼‰
        2. æ¥æºä¼˜å…ˆçº§ï¼šHugging Face Papers > Papers with Code
        3. 70% ç›¸å…³æ€§ + 30% æ¢ç´¢æ€§ï¼ˆé¿å…ä¿¡æ¯èŒ§æˆ¿ï¼‰
        4. å»é‡ï¼Œé¿å…é‡å¤
        
        Args:
            processed_items: å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            top_count: éœ€è¦é€‰æ‹©çš„è®ºæ–‡æ•°é‡
            used_keys: å·²ä½¿ç”¨çš„å»é‡é”®é›†åˆï¼ˆä¼šè¢«æ›´æ–°ï¼‰
            
        Returns:
            è®ºæ–‡æ¡ç›®åˆ—è¡¨
        """
        if used_keys is None:
            used_keys = set()
        dedupe_pool = used_keys
        
        # ç­›é€‰è®ºæ–‡ç±»å†…å®¹ï¼Œåªä¿ç•™Hugging Face Paperså’ŒPapers with Code
        papers = [
            item for item in processed_items 
            if getattr(item, 'category', '') == 'paper'
            and self._make_dedupe_key(item) not in dedupe_pool
            and self._is_curated_paper_source(getattr(item, 'source', ''))
        ]
        
        # å®šä¹‰æ¥æºä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        def get_source_priority(source: str) -> int:
            source_lower = source.lower()
            if 'hugging face' in source_lower:
                return 2  # æœ€é«˜ä¼˜å…ˆçº§ï¼šç¤¾åŒºç­›é€‰
            elif 'papers with code' in source_lower:
                return 1  # æ¬¡é«˜ä¼˜å…ˆçº§ï¼šå¸¦ä»£ç å®ç°
            else:
                return 0
        
        # åº”ç”¨åŠ¨æ€æƒé‡æ’åºï¼Œå¹¶è€ƒè™‘æ¥æºä¼˜å…ˆçº§
        for paper in papers:
            base_score = getattr(paper, 'personal_priority', getattr(paper, 'relevance_score', 0))
            weighted_score = self._apply_dynamic_weight(paper, base_score)
            
            # æ¥æºä¼˜å…ˆçº§åŠ æˆï¼ˆHugging Face +2åˆ†ï¼ŒPapers with Code +1åˆ†ï¼‰
            source_priority = get_source_priority(getattr(paper, 'source', ''))
            if source_priority == 2:  # Hugging Face Papers
                weighted_score += 2.0
            elif source_priority == 1:  # Papers with Code
                weighted_score += 1.0
            
            setattr(paper, 'weighted_score', weighted_score)
            setattr(paper, 'source_priority', source_priority)
        
        # ç­–ç•¥ï¼š70% é«˜ç›¸å…³ + 30% æ¢ç´¢
        target_relevant = int(top_count * 0.7)  # 5-6ç¯‡é«˜ç›¸å…³
        target_exploration = top_count - target_relevant  # 2-3ç¯‡æ¢ç´¢æ€§
        
        featured_papers = []
        seen_sources: Dict[str, int] = {}
        max_per_source = 3  # æ¯ä¸ªæ¥æºæœ€å¤š3ç¯‡è®ºæ–‡
        
        # ç¬¬ä¸€é˜¶æ®µï¼šé€‰æ‹©é«˜ç›¸å…³è®ºæ–‡ï¼ˆé€‰é¡¹3: é™ä½é˜ˆå€¼ä» >= 7 åˆ° >= 6ï¼‰
        relevant_papers = [p for p in papers if getattr(p, 'personal_priority', 0) >= 6]
        relevant_papers.sort(key=lambda x: getattr(x, 'weighted_score', 0), reverse=True)
        
        for paper in relevant_papers:
            if len(featured_papers) >= target_relevant:
                break
            
            source_key = self._normalize_source(getattr(paper, 'source', ''))
            dedupe_key = self._make_dedupe_key(paper)
            
            if dedupe_key in dedupe_pool:
                continue
            
            # é™åˆ¶åŒä¸€æ¥æºçš„æ•°é‡
            if seen_sources.get(source_key, 0) >= max_per_source:
                continue
            
            featured_papers.append(paper)
            dedupe_pool.add(dedupe_key)
            seen_sources[source_key] = seen_sources.get(source_key, 0) + 1
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ·»åŠ æ¢ç´¢æ€§è®ºæ–‡ï¼ˆçƒ­é—¨ä½†ä¸ä¸€å®šé«˜åº¦ç›¸å…³ï¼‰
        # ç­–ç•¥ï¼šä¼˜å…ˆ Hugging Face Papers > Papers with Code
        exploration_papers = [
            p for p in papers 
            if getattr(p, 'personal_priority', 0) < 6  # ä¸é‚£ä¹ˆç›¸å…³
            and self._make_dedupe_key(p) not in dedupe_pool
        ]
        
        # æŒ‰æ¥æºä¼˜å…ˆçº§å’Œè´¨é‡æ’åº
        exploration_papers.sort(
            key=lambda x: (
                getattr(x, 'source_priority', 0),  # æ¥æºä¼˜å…ˆçº§æœ€é‡è¦
                getattr(x, 'relevance_score', 0),  # å…¶æ¬¡æ˜¯ç›¸å…³æ€§
                getattr(x, 'weighted_score', 0)    # æœ€åæ˜¯åŠ æƒåˆ†æ•°
            ),
            reverse=True
        )
        
        for paper in exploration_papers:
            if len(featured_papers) >= top_count:
                break
            
            source_key = self._normalize_source(getattr(paper, 'source', ''))
            dedupe_key = self._make_dedupe_key(paper)
            
            if dedupe_key in dedupe_pool:
                continue
            
            if seen_sources.get(source_key, 0) >= max_per_source:
                continue
            
            featured_papers.append(paper)
            dedupe_pool.add(dedupe_key)
            seen_sources[source_key] = seen_sources.get(source_key, 0) + 1
            
            logger.info(f"ğŸ” æ·»åŠ æ¢ç´¢æ€§è®ºæ–‡: {paper.title} (priority={getattr(paper, 'personal_priority', 0)})")
        
        # ç»Ÿè®¡æ¥æºåˆ†å¸ƒ
        source_stats = {}
        for paper in featured_papers:
            source = getattr(paper, 'source', 'Unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        relevant_count = len([p for p in featured_papers if getattr(p, 'personal_priority', 0) >= 6])
        exploration_count = len(featured_papers) - relevant_count
        
        logger.info(f"âœ“ é€‰æ‹©äº† {len(featured_papers)} ç¯‡è®ºæ–‡ç²¾é€‰ï¼ˆ{relevant_count} ç›¸å…³ + {exploration_count} æ¢ç´¢ï¼‰")
        logger.info(f"  æ¥æºåˆ†å¸ƒ: {', '.join([f'{k}: {v}' for k, v in sorted(source_stats.items(), key=lambda x: x[1], reverse=True)])}")
        
        return featured_papers
    
    def _select_fintech_items(self, processed_items: List, top_count: int = 10, used_keys: Optional[Set[str]] = None) -> List:
        """
        é€‰æ‹© Fintech ç›¸å…³çš„å†…å®¹
        
        Args:
            processed_items: å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            top_count: éœ€è¦é€‰æ‹©çš„æ•°é‡
            used_keys: å·²ä½¿ç”¨çš„å»é‡é”®é›†åˆï¼ˆä¼šè¢«æ›´æ–°ï¼‰
            
        Returns:
            Fintech ç›¸å…³æ¡ç›®åˆ—è¡¨
        """
        if used_keys is None:
            used_keys = set()
        dedupe_pool = used_keys
        
        # Fintech å…³é”®è¯åˆ—è¡¨ï¼ˆæ‰©å±•åŒ¹é…ï¼‰
        fintech_keywords = [
            'Capital One', 'JPMorgan', 'Goldman', 'Morgan Stanley', 
            'American Express', 'PayPal', 'Stripe', 'Square', 
            'Fidelity', 'BlackRock', 'Vanguard', 'Fintech', 
            'Fintech Times', 'The Fintech Times', 'Finextra', 
            'TechCrunch Fintech', 'fintech', 'FinTech', 'FINtech',
            'financial technology', 'banking tech', 'payment tech',
            'asset management', 'wealth management', 'robo-advisor',
            'fraud detection', 'credit risk', 'compliance automation'
        ]
        
        # VC/Startup å…³é”®è¯ï¼ˆæ‰©å±•åŒ¹é…ï¼‰
        vc_keywords = [
            'Y Combinator', 'YC', 'a16z', 'Sequoia', 
            'Launch HN', 'Crunchbase', 'TechCrunch Fintech',
            'ycombinator', 'Y Combinator Blog'
        ]
        
        # Fintech ç›¸å…³æºåç§°ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
        fintech_sources = [
            'Fintech News', 'The Fintech Times', 'Finextra',
            'TechCrunch Fintech', 'Crunchbase News - Fintech'
        ]
        
        # ç­›é€‰ Fintech ç›¸å…³çš„å†…å®¹
        fintech_items = []
        for item in processed_items:
            source = getattr(item, 'source', '')
            title = getattr(item, 'title', '')
            summary = getattr(item, 'summary', '') or getattr(item, 'ai_summary', '')
            
            # æ£€æŸ¥æºåç§°æ˜¯å¦å®Œå…¨åŒ¹é…
            is_fintech_source = any(fs in source for fs in fintech_sources)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é… Fintech æˆ– VC å…³é”®è¯ï¼ˆåœ¨æºã€æ ‡é¢˜æˆ–æ‘˜è¦ä¸­ï¼‰
            is_fintech = is_fintech_source or any(
                kw.lower() in source.lower() or 
                kw.lower() in title.lower() or 
                kw.lower() in summary.lower() 
                for kw in fintech_keywords
            )
            is_vc = any(
                kw.lower() in source.lower() or 
                kw.lower() in title.lower() or 
                kw.lower() in summary.lower()
                for kw in vc_keywords
            )
            
            if (is_fintech or is_vc) and self._make_dedupe_key(item) not in dedupe_pool:
                fintech_items.append(item)
                dedupe_pool.add(self._make_dedupe_key(item))
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        fintech_items.sort(
            key=lambda x: getattr(x, 'personal_priority', getattr(x, 'relevance_score', 0)),
            reverse=True
        )
        
        # è®°å½•æ—¥å¿—
        logger.info(f"âœ“ é€‰æ‹©äº† {len(fintech_items)} æ¡ Fintech ç›¸å…³å†…å®¹ï¼ˆTop {top_count}ï¼‰")
        if fintech_items:
            source_stats = {}
            for item in fintech_items[:top_count]:
                source = getattr(item, 'source', 'unknown')
                source_stats[source] = source_stats.get(source, 0) + 1
            logger.info(f"  æ¥æºåˆ†å¸ƒ: {', '.join([f'{k}: {v}' for k, v in sorted(source_stats.items(), key=lambda x: x[1], reverse=True)])}")
        
        # è¿”å› Top N
        return fintech_items[:top_count]
    
    def _select_top_headlines(self, processed_items: List, top_count: int = 10, used_keys: Optional[Set[str]] = None) -> List:
        """
        é€‰æ‹©å¤´æ¡åˆ—è¡¨ï¼Œæ¥æºï¼šä¸»æµåª’ä½“ã€ä¸ªäººåšå®¢ã€å®˜æ–¹åšå®¢ã€Hacker News
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆçº§ï¼šä¸»æµåª’ä½“ > å®˜æ–¹åšå®¢ > Newsletter/ä¸ªäººåšå®¢ > Hacker News
        2. headlineç±»åˆ«ä¼˜å…ˆï¼ŒæŒ‰headline_priorityæ’åº
        3. å¦‚æœheadlineä¸è¶³ï¼Œè¡¥å……é«˜åˆ†article/projectï¼ˆæ’é™¤è®ºæ–‡ï¼‰
        4. æ¥æºå¤šæ ·æ€§ï¼šæ¯ä¸ªæ¥æºæœ€å¤š3æ¡ï¼ˆæ‰©å±•è‡³15-20æ¡æ€»é‡ï¼‰
        
        Args:
            processed_items: å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            top_count: éœ€è¦é€‰æ‹©çš„å¤´æ¡æ•°é‡
            used_keys: å·²ä½¿ç”¨çš„å»é‡é”®é›†åˆï¼ˆä¼šè¢«æ›´æ–°ï¼‰
            
        Returns:
            å¤´æ¡æ¡ç›®åˆ—è¡¨
        """
        if used_keys is None:
            used_keys = set()
        dedupe_pool = used_keys
        
        # æ¯ä¸ªæ¥æºæœ€å¤š3æ¡ï¼ˆé€‚åº”15-20æ¡çš„æ€»é‡ï¼‰
        max_per_source = 3

        # å…ˆæ”¶é›†æ‰€æœ‰ headline ç±»åˆ«çš„æ¡ç›®ï¼Œå¹¶è¿‡æ»¤è®ºæ–‡æ¥æº
        all_headlines = [item for item in processed_items if item.category == 'headline']
        logger.debug(f"ğŸ“Š å‘ç° {len(all_headlines)} æ¡ headline ç±»åˆ«çš„æ¡ç›®")
        for item in all_headlines[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
            source = getattr(item, 'source', '')
            is_paper = self._is_curated_paper_source(source)
            logger.debug(f"  - {item.title[:50]}... | æ¥æº: {source} | æ˜¯è®ºæ–‡æº: {is_paper}")
        
        headlines = [
            item for item in processed_items 
            if item.category == 'headline'
            and 'Towards Data Science' not in item.source
            and not self._is_curated_paper_source(getattr(item, 'source', ''))  # æ’é™¤è®ºæ–‡æ¥æº
            and not (getattr(item, 'is_release', False) and not getattr(item, 'promote_release', False))
            and self._make_dedupe_key(item) not in dedupe_pool
        ]
        logger.info(f"âœ“ è¿‡æ»¤åå‰©ä½™ {len(headlines)} æ¡ headline")
        headlines.sort(key=lambda x: getattr(x, 'headline_priority', 0), reverse=True)
        
        seen_sources: Dict[str, int] = {}
        unique_headlines = []
        
        for item in headlines:
            source_key = self._normalize_source(item.source)
            dedupe_key = self._make_dedupe_key(item)
            
            if dedupe_key in dedupe_pool:
                continue
            if seen_sources.get(source_key, 0) >= max_per_source:
                continue
            if getattr(item, 'is_release', False) and not getattr(item, 'promote_release', False):
                continue
                
            unique_headlines.append(item)
            dedupe_pool.add(dedupe_key)
            seen_sources[source_key] = seen_sources.get(source_key, 0) + 1
            
            if len(unique_headlines) >= top_count:
                break
        
        # ç¬¬äºŒæ­¥ï¼šè¡¥å……é«˜è´¨é‡article/projectï¼ˆåŒ…å«GitHub Releaseï¼Œæ’é™¤è®ºæ–‡ï¼‰
        if len(unique_headlines) < top_count:
            others = [
                item for item in processed_items 
                if item.category in ['article', 'project', 'framework']  # åŒ…å«frameworkä»¥è·å–GitHub Release
                and getattr(item, 'category', '') != 'paper'  # æ˜ç¡®æ’é™¤è®ºæ–‡ç±»åˆ«
                and not self._is_curated_paper_source(getattr(item, 'source', ''))  # æ’é™¤è®ºæ–‡æ¥æº
                and getattr(item, 'relevance_score', 0) >= 6  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šå†…å®¹
                and 'Towards Data Science' not in item.source
                and self._make_dedupe_key(item) not in dedupe_pool
            ]
            others.sort(key=lambda x: getattr(x, 'relevance_score', 0), reverse=True)
            
            for item in others:
                if len(unique_headlines) >= top_count:
                    break
                
                source_key = self._normalize_source(item.source)
                dedupe_key = self._make_dedupe_key(item)
                
                if dedupe_key in dedupe_pool:
                    continue
                if seen_sources.get(source_key, 0) >= max_per_source:
                    continue
                if getattr(item, 'is_release', False) and not getattr(item, 'promote_release', False):
                    continue
                    
                unique_headlines.append(item)
                dedupe_pool.add(dedupe_key)
                seen_sources[source_key] = seen_sources.get(source_key, 0) + 1
        
        # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœä»ä¸è¶³ï¼Œè¡¥å……å…¶ä»–å†…å®¹ï¼ˆæ’é™¤modelå’Œpaperï¼‰
        if len(unique_headlines) < top_count:
            remaining = [
                item for item in processed_items 
                if item not in unique_headlines
                and getattr(item, 'category', '') not in ['model', 'paper']  # æ’é™¤modelå’Œpaperç±»åˆ«
                and not self._is_curated_paper_source(getattr(item, 'source', ''))  # æ’é™¤è®ºæ–‡æ¥æº
                and 'Towards Data Science' not in getattr(item, 'source', '')
                and self._make_dedupe_key(item) not in dedupe_pool
            ]
            remaining.sort(key=lambda x: getattr(x, 'relevance_score', 0), reverse=True)
            
            for item in remaining:
                if len(unique_headlines) >= top_count:
                    break
                
                # è¿‡æ»¤çº¯ç‰ˆæœ¬å·çš„ Release
                if getattr(item, 'is_release', False) and not getattr(item, 'promote_release', False):
                    continue
                
                source_key = self._normalize_source(getattr(item, 'source', ''))
                if seen_sources.get(source_key, 0) >= max_per_source:
                    logger.debug(f"â­ è·³è¿‡æ¥æº {source_key}ï¼šå·²è¾¾ä¸Šé™ ({max_per_source}æ¡)")
                    continue
                
                dedupe_key = self._make_dedupe_key(item)
                if dedupe_key not in dedupe_pool:
                    unique_headlines.append(item)
                    dedupe_pool.add(dedupe_key)
                    seen_sources[source_key] = seen_sources.get(source_key, 0) + 1
        
        # è®°å½•è¯¦ç»†ç­›é€‰ä¿¡æ¯
        category_dist = {}
        source_dist = {}
        for item in unique_headlines:
            category_dist[item.category] = category_dist.get(item.category, 0) + 1
            source_key = self._normalize_source(item.source)
            source_dist[source_key] = source_dist.get(source_key, 0) + 1
        
        logger.info(f"âœ“ Top {top_count}ç­›é€‰å®Œæˆ: {len(unique_headlines)}æ¡")
        logger.info(f"  åˆ†ç±»åˆ†å¸ƒ: {category_dist}")
        logger.info(f"  æ¥æºåˆ†å¸ƒ: {source_dist}")
        
        # è­¦å‘Šï¼šå¦‚æœarXivå æ¯”è¿‡é«˜
        arxiv_count = source_dist.get('arxiv', 0)
        if arxiv_count > len(unique_headlines) * 0.4:  # è¶…è¿‡40%
            logger.warning(f"âš ï¸  arXivè®ºæ–‡å æ¯”è¿‡é«˜: {arxiv_count}/{len(unique_headlines)} = {arxiv_count/len(unique_headlines)*100:.1f}%")
        
        return unique_headlines[:top_count]
    
    def _categorize_items(self, items: List) -> Dict[str, List]:
        """
        æŒ‰ç±»åˆ«åˆ†ç»„æ¡ç›®
        
        Args:
            items: å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            
        Returns:
            åˆ†ç±»åçš„å­—å…¸
        """
        categorized = {
            'headline': [],
            'framework': [],
            'model': [],
            'article': [],
            'project': [],
            'other': []
        }
        
        for item in items:
            category = getattr(item, 'category', 'other')
            if category in categorized:
                categorized[category].append(item)
            else:
                categorized['other'].append(item)
        
        # æ¯ä¸ªç±»åˆ«æŒ‰ç›¸å…³æ€§æ’åº
        for category in categorized:
            categorized[category].sort(
                key=lambda x: getattr(x, 'relevance_score', 0),
                reverse=True
            )
        
        return categorized
    
    def _generate_stats(self, items: List, categorized: Dict) -> Dict:
        """
        ç”Ÿæˆç»Ÿè®¡æ•°æ®
        
        Args:
            items: å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            categorized: åˆ†ç±»åçš„å­—å…¸
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        high_relevance = [
            item for item in items
            if getattr(item, 'relevance_score', 0) >= 8
        ]
        
        actionable = [
            item for item in items
            if getattr(item, 'actionable', False)
        ]
        
        return {
            'total_sources': len(set(getattr(item, 'source', '') for item in items)),
            'total_items': len(items),
            'processed_items': len(items),
            'high_relevance_items': len(high_relevance),
            'actionable_items': len(actionable)
        }
    
    def _save_report(self, content: str, output_path: str):
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            content: æŠ¥å‘Šå†…å®¹
            output_path: è¾“å‡ºè·¯å¾„
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def generate_email_body(self, processed_items: List) -> str:
        """
        ç”Ÿæˆé‚®ä»¶æ­£æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            processed_items: å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
            
        Returns:
            é‚®ä»¶HTMLå†…å®¹
        """
        top3 = sorted(processed_items, key=lambda x: x.relevance_score, reverse=True)[:3]
        
        html = f"""
        <html>
        <head>
            <style>
                body {{ font-family: Arial, sans-serif; line-height: 1.6; }}
                .item {{ margin-bottom: 30px; padding: 15px; background: #f5f5f5; }}
                .title {{ font-size: 18px; font-weight: bold; color: #333; }}
                .meta {{ color: #666; font-size: 14px; margin-top: 5px; }}
                .summary {{ margin-top: 10px; }}
                .impact {{ margin-top: 10px; padding: 10px; background: #fff3cd; }}
            </style>
        </head>
        <body>
            <h1>ğŸš€ æœ¬å‘¨AIæŠ€æœ¯å¤´æ¡ (Top 3)</h1>
            <p>ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}</p>
        """
        
        for i, item in enumerate(top3, 1):
            html += f"""
            <div class="item">
                <div class="title">{i}. {item.title}</div>
                <div class="meta">æ¥æºï¼š{item.source} | <a href="{item.link}">æŸ¥çœ‹è¯¦æƒ…</a></div>
                <div class="summary">{item.ai_summary}</div>
                <div class="impact"><strong>ğŸ’¡ å¯¹ä½ çš„å½±å“ï¼š</strong>{item.impact_analysis}</div>
            </div>
            """
        
        html += """
            <hr>
            <p><small>å®Œæ•´å‘¨æŠ¥è¯·æŸ¥çœ‹é™„ä»¶ | AI Weekly Report Generator</small></p>
        </body>
        </html>
        """
        
        return html

    def _build_paper_radar(self, processed_items: List, used_keys: Set[str]) -> List[Dict[str, Any]]:
        """æ„å»ºè®ºæ–‡é›·è¾¾åˆ—è¡¨ï¼Œä¼˜å…ˆå±•ç¤ºæ ¸å¿ƒAIç ”ç©¶è®ºæ–‡"""
        radar_candidates = []
        for item in processed_items:
            source = getattr(item, 'source', '') or ''
            category = getattr(item, 'category', '') or ''
            article_type = getattr(item, 'article_type', '') or ''

            is_research_source = any(keyword in source.lower() for keyword in ["arxiv", "papers with code", "cs.ai", "cs.lg", "neurips", "iclr", "icml"])
            is_research_item = article_type in {"technical", "research"}

            if category in {'framework', 'model', 'project'}:
                continue

            if not (is_research_source or is_research_item):
                continue

            summary = getattr(item, 'summary', '') or getattr(item, 'ai_summary', '') or ''
            personal_note = getattr(item, 'why_matters_to_you', '') or getattr(item, 'impact_analysis', '') or ''

            if not summary:
                # å¦‚æœç¼ºå°‘æ‘˜è¦ï¼Œè·³è¿‡ä»¥ç¡®ä¿ä¿¡å™ªæ¯”
                continue

            dedupe_key = self._make_dedupe_key(item)
            if dedupe_key in used_keys:
                continue

            radar_candidates.append({
                'title': getattr(item, 'title', 'æœªå‘½åè®ºæ–‡'),
                'url': getattr(item, 'url', getattr(item, 'link', '')),
                'source': source,
                'summary': summary.strip(),
                'personal_note': personal_note.strip(),
                'published_date': self._format_date(getattr(item, 'published_date', '')),
                'personal_priority': getattr(item, 'personal_priority', getattr(item, 'relevance_score', 0)) or 0,
                '_dedupe_key': dedupe_key,
            })

        radar_candidates.sort(key=lambda x: x['personal_priority'], reverse=True)
        top_items = radar_candidates[:3]

        for entry in top_items:
            used_keys.add(entry['_dedupe_key'])
            entry.pop('_dedupe_key', None)

        return top_items

    @staticmethod
    def _format_date(date_obj: Any) -> str:
        if isinstance(date_obj, datetime):
            return date_obj.strftime('%Y-%m-%d')
        if isinstance(date_obj, str):
            return date_obj[:10]
        return ''

    def _filter_action_items(self, action_items: Dict[str, List], must_read_items: List, top_headlines: List) -> Dict[str, List]:
        """
        è¿‡æ»¤è¡ŒåŠ¨æ¸…å•ï¼Œæ’é™¤å·²åœ¨å¿…çœ‹å†…å®¹å’Œæœ¬å‘¨å¤´æ¡ä¸­å‡ºç°çš„æ¡ç›®
        
        Args:
            action_items: åŸå§‹è¡ŒåŠ¨æ¸…å•
            must_read_items: å¿…çœ‹å†…å®¹åˆ—è¡¨
            top_headlines: æœ¬å‘¨å¤´æ¡åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„è¡ŒåŠ¨æ¸…å•
        """
        # æ”¶é›†å·²ä½¿ç”¨çš„URL
        used_urls = set()
        for item in must_read_items:
            url = getattr(item, 'url', None) or getattr(item, 'link', None)
            if url:
                used_urls.add(url)
        
        for item in top_headlines:
            url = getattr(item, 'url', None) or getattr(item, 'link', None)
            if url:
                used_urls.add(url)
        
        # è¿‡æ»¤must_doå’Œnice_to_have
        filtered_must_do = []
        if action_items.get('must_do'):
            for action in action_items['must_do']:
                action_url = action.get('url', '')
                if action_url and action_url not in used_urls:
                    filtered_must_do.append(action)
        
        filtered_nice_to_have = []
        if action_items.get('nice_to_have'):
            for action in action_items['nice_to_have']:
                action_url = action.get('url', '')
                if action_url and action_url not in used_urls:
                    filtered_nice_to_have.append(action)
        
        return {
            'must_do': filtered_must_do,
            'nice_to_have': filtered_nice_to_have
        }

    def _normalize_source(self, source: str) -> str:
        """
        æ ‡å‡†åŒ–æ¥æºåç§°ï¼Œç”¨äºæ¥æºå¤šæ ·æ€§æ£€æŸ¥
        
        Args:
            source: åŸå§‹æ¥æºåç§°
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ¥æºåç§°
        """
        if not source:
            return 'unknown'
        
        source_lower = source.lower()
        
        # ç»Ÿä¸€arXivæ¥æº
        if 'arxiv' in source_lower:
            return 'arxiv'
        
        # ç»Ÿä¸€Redditæ¥æº
        if 'reddit' in source_lower or 'r/' in source_lower:
            return 'reddit'
        
        # ç»Ÿä¸€Hacker Newsæ¥æº
        if 'hacker news' in source_lower or 'hn' in source_lower:
            return 'hacker_news'
        
        # ç»Ÿä¸€GitHubæ¥æº
        if 'github' in source_lower:
            return 'github'
        
        # è¿”å›åŸå§‹æ¥æºï¼ˆå»é™¤ç‰ˆæœ¬å·ç­‰ï¼‰
        return source.split()[0].split('(')[0].strip()
    
    def _make_dedupe_key(self, item: Any) -> str:
        """
        ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å»é‡é”®ï¼Œç”¨äºé¿å…é‡å¤æ¡ç›®ã€‚
        
        Args:
            item: æ¡ç›®å¯¹è±¡
            
        Returns:
            å»é‡é”®å­—ç¬¦ä¸²
        """
        return make_dedupe_key(item)
    
    def _is_curated_paper_source(self, source: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç²¾é€‰è®ºæ–‡æ¥æºï¼ˆHugging Face Papersæˆ–Papers with Codeï¼‰
        
        Args:
            source: æ¥æºåç§°
            
        Returns:
            æ˜¯å¦ä¸ºç²¾é€‰è®ºæ–‡æ¥æº
        """
        source_lower = source.lower()
        # æ”¯æŒå®Œæ•´åç§°å’Œç¼©å†™å½¢å¼
        return ('hugging face' in source_lower or 
                'hugging' in source_lower or 
                'papers with code' in source_lower or 
                'papers' in source_lower and 'code' in source_lower or
                source_lower.startswith('papers'))

