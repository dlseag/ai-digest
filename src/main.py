"""
AI Weekly Report Generator - Main Entry Point
ä¸»ç¨‹åºï¼šæ•´åˆæ‰€æœ‰æ¨¡å—ç”Ÿæˆå‘¨æŠ¥
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import asdict, is_dataclass
import yaml
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.collectors.rss_collector import RSSCollector
from src.collectors.github_collector import GitHubCollector
from src.collectors.hackernews_collector import HackerNewsCollector
from src.collectors.reddit_collector import RedditCollector
from src.collectors.news_collector import NewsCollector
from src.collectors.producthunt_collector import ProductHuntCollector
from src.collectors.twitter_collector import TwitterCollector
from src.collectors.leaderboard_collector import LeaderboardCollector  # æ’è¡Œæ¦œé‡‡é›†å™¨
from src.collectors.market_insights_collector import MarketInsightsCollector  # å¸‚åœºæ´å¯Ÿé‡‡é›†å™¨
from src.processors.ai_processor import AIProcessor
from src.processors.ai_processor_batch import AIProcessorBatch  # æ‰¹é‡å¤„ç†å™¨
from src.generators.report_generator import ReportGenerator
from src.learning.learning_engine import LearningEngine
from src.learning.config_manager import ConfigManager
from src.learning.explicit_feedback import ExplicitFeedbackManager
from src.learning.ab_tester import ABTester, Experiment
from src.storage.feedback_db import OptimizationRecord
from src.utils.emailer import send_digest_email
# æš‚æ—¶ç¦ç”¨LangGraphç›¸å…³å¯¼å…¥ï¼ˆéœ€è¦å®Œæ•´å®ç°åå†å¯ç”¨ï¼‰
# from src.agents.briefing_graph import GraphComponents, compile_briefing_graph
# from src.agents.cluster_agent import ClusterAgent
# from src.agents.critique_agent import CritiqueAgent
# from src.agents.differential_agent import DifferentialAgent
# from src.agents.proactive_agent import ProactiveAgent
# from src.agents.state import create_initial_state
# from src.agents.triage_agent import TriageAgent
from src.memory.memory_manager import MemoryManager
from src.memory.user_profile_manager import UserProfileManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('weekly_report_generator.log')
    ]
)
logger = logging.getLogger(__name__)


class WeeklyReportGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, config_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–å‘¨æŠ¥ç”Ÿæˆå™¨
        
        Args:
            config_dir: é…ç½®æ–‡ä»¶ç›®å½•
        """
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # é…ç½®ç›®å½•
        if config_dir is None:
            config_dir = str(project_root / "config")
        self.config_dir = Path(config_dir)
        
        # åŠ è½½é…ç½®
        self.sources_config = self._load_yaml(self.config_dir / "sources.yaml")
        raw_user_profile = self._load_yaml(self.config_dir / "user_profile.yaml")
        raw_learning_config = self._load_yaml(self.config_dir / "learning_config.yaml")
        self.learning_config = (raw_learning_config or {}).get("learning", {})
        self.source_preferences = self.learning_config.get("source_preferences", {})
        self.headline_source_limit = int(self.source_preferences.get("max_headlines_per_source", 2))

        self.user_profile_manager = UserProfileManager(
            self.config_dir / "user_profile.yaml",
            profile_data=raw_user_profile,
        )
        self.user_profile = self.user_profile_manager.get_profile()
        self.filtering_preferences = self.user_profile.get("filtering_preferences", {}) or {}

        # è·å–APIå¯†é’¥
        self.api_key = os.getenv("POE_API_KEY")
        if not self.api_key:
            logger.warning("POE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.rss_collector = None
        self.github_collector = None
        self.ai_processor = None
        self.report_generator = None
        self.learning_engine = LearningEngine(
            config=self.learning_config,
            project_root=project_root,
            user_profile_manager=self.user_profile_manager,
            api_key=self.api_key,
        )
        self.explicit_feedback = ExplicitFeedbackManager(self.learning_engine.db)
        self.ab_tester = ABTester(self.learning_engine.db)
        self.email_settings = self._load_email_settings()
        self.ab_experiments: Dict[str, Experiment] = {
            "narrative_clustering_v1": Experiment(
                id="narrative_clustering_v1",
                hypothesis="Narrative clusteringæå‡ä¸ªæ€§åŒ–å‚ä¸åº¦",
                metric="engagement_score",
                variants={
                    "control": "ä¼ ç»Ÿçº¿æ€§æ‘˜è¦",
                    "treatment": "å™äº‹èšç±» + RAG-Diff æ‘˜è¦",
                },
            )
        }
        self.config_manager = ConfigManager(self.config_dir / "sources.yaml")
        self.memory_manager = MemoryManager()
        self.api_key = os.getenv("POE_API_KEY")
        if not self.api_key:
            logger.warning("POE_API_KEY æœªé…ç½®ï¼ŒLangGraph å·¥ä½œæµå°†æ— æ³•è°ƒç”¨LLM")
        
        logger.info("=" * 60)
        logger.info("AI Weekly Report Generator å¯åŠ¨")
        logger.info("=" * 60)
    
    def _load_yaml(self, file_path: Path) -> dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return {}
    
    def run(
        self,
        days_back: int = 3,
        output_dir: Optional[str] = None,
        learning_only: bool = False,
    ):
        """
        æ‰§è¡Œå®Œæ•´çš„å‘¨æŠ¥ç”Ÿæˆæµç¨‹
        
        Args:
            days_back: é‡‡é›†æœ€è¿‘Nå¤©çš„å†…å®¹
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            # 1. æ•°æ®é‡‡é›†
            logger.info("\n" + "=" * 60)
            logger.info("æ­¥éª¤ 1/5: æ•°æ®é‡‡é›†")
            logger.info("=" * 60)
            all_items = self._collect_data(days_back)
            self._dump_collected_items(all_items, days_back, output_dir)
            
            if not all_items:
                logger.warning("æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®ï¼Œé€€å‡º")
                return
            
            # 1.5 é‡‡é›†æ’è¡Œæ¦œæ•°æ®ï¼ˆç‹¬ç«‹äºæ–°é—»é‡‡é›†ï¼‰
            leaderboard_info = self._collect_leaderboard()
            
            # 1.6 é‡‡é›†å¸‚åœºæ´å¯Ÿï¼ˆæŠ•èµ„è¶‹åŠ¿ã€å¸‚åœºåˆ†æï¼‰
            market_insights = self._collect_market_insights()
            
            # 2. AIå¤„ç†
            logger.info("\n" + "=" * 60)
            logger.info("æ­¥éª¤ 2/5: AIæ™ºèƒ½å¤„ç†")
            logger.info("=" * 60)
            processed_items = self._process_with_ai(all_items)
            
            if not processed_items:
                logger.warning("AIå¤„ç†åæ— æœ‰æ•ˆæ•°æ®ï¼Œé€€å‡º")
                return
            
            action_items = None
            if not learning_only:
                # 3. ç”Ÿæˆè¡ŒåŠ¨æ¸…å•
                logger.info("\n" + "=" * 60)
                logger.info("æ­¥éª¤ 3/5: ç”Ÿæˆè¡ŒåŠ¨æ¸…å•")
                logger.info("=" * 60)
                action_items = self._generate_action_items(processed_items)
            
            # 3.5 è‡ªæˆ‘å­¦ä¹ å¾ªç¯
            learning_results = self._run_learning_cycle(processed_items)
            
            report_path = None
            if not learning_only:
                # 4. ç”Ÿæˆå‘¨æŠ¥
                logger.info("\n" + "=" * 60)
                logger.info("æ­¥éª¤ 4/5: ç”Ÿæˆå‘¨æŠ¥")
                logger.info("=" * 60)
                report_path = self._generate_report(
                    processed_items,
                    action_items or {"must_do": [], "nice_to_have": []},
                    leaderboard_info,
                    market_insights,
                    output_dir,
                    learning_results,
                )
                
                # å®Œæˆ
                logger.info("\n" + "=" * 60)
                logger.info("âœ“ å‘¨æŠ¥ç”Ÿæˆå®Œæˆï¼")
                logger.info(f"âœ“ æŠ¥å‘Šè·¯å¾„: {report_path}")
                logger.info("=" * 60)
                self._send_email_if_configured(report_path)
            else:
                logger.info("\n" + "=" * 60)
                logger.info("âœ“ å·²å®Œæˆå­¦ä¹ å¾ªç¯ (learning-only æ¨¡å¼)")
                logger.info("=" * 60)

            self._log_learning_summary(learning_results)
            return report_path
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘¨æŠ¥å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def run_langgraph(
        self,
        days_back: int = 3,
        output_dir: Optional[str] = None,
        max_iterations: int = 3,
    ) -> Optional[Path]:
        """Experimental workflow powered by LangGraph agents."""

        if not self.api_key:
            raise RuntimeError("POE_API_KEY æœªé…ç½®ï¼Œæ— æ³•è¿è¡Œ LangGraph å·¥ä½œæµ")

        logger.info("\n" + "=" * 60)
        logger.info("LangGraph å·¥ä½œæµï¼šå¼€å§‹æ•°æ®é‡‡é›†")
        logger.info("=" * 60)

        all_items = self._collect_data(days_back)
        self._dump_collected_items(all_items, days_back, output_dir)
        documents = self._prepare_graph_documents(all_items)

        if not documents:
            logger.warning("LangGraph å·¥ä½œæµæœªè·å–åˆ°æœ‰æ•ˆæ–‡æ¡£ï¼Œé€€å‡º")
            return None

        components = GraphComponents(
            triage_agent=TriageAgent(api_key=self.api_key),
            cluster_agent=ClusterAgent(),
            differential_agent=DifferentialAgent(
                vector_store=self.memory_manager.vector_store,
                api_key=self.api_key,
            ),
            critique_agent=CritiqueAgent(api_key=self.api_key),
            proactive_agent=ProactiveAgent(self.learning_engine.db),
        )

        compiled_graph = compile_briefing_graph(components)
        initial_state = create_initial_state(self.user_profile, max_iterations=max_iterations)
        initial_state["raw_documents"] = documents

        logger.info("\n" + "=" * 60)
        logger.info("LangGraph å·¥ä½œæµï¼šå¯åŠ¨æ™ºèƒ½ä½“å›¾")
        logger.info("=" * 60)

        final_state = compiled_graph.invoke(initial_state)
        report_path = self._write_langgraph_report(final_state, output_dir)

        logger.info("\n" + "=" * 60)
        logger.info("âœ“ LangGraph ç®€æŠ¥ç”Ÿæˆå®Œæˆï¼")
        if report_path:
            logger.info(f"âœ“ æŠ¥å‘Šè·¯å¾„: {report_path}")
        logger.info("=" * 60)

        return report_path

    def _dump_collected_items(self, items: list, days_back: int, output_dir: Optional[str]) -> None:
        """å°†åŸå§‹é‡‡é›†ç»“æœå†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œä¾¿äºè°ƒè¯•"""
        try:
            if output_dir is None:
                dump_dir = project_root / "output"
            else:
                dump_dir = Path(output_dir)

            dump_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
            dump_path = dump_dir / f"collected_items_{timestamp}.json"

            payload = {
                "generated_at": datetime.now().isoformat(),
                "days_back": days_back,
                "total_items": len(items),
                "source_breakdown": self._summarize_sources(items),
                "items": [self._serialize_item(item) for item in items]
            }

            with open(dump_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ—‚ï¸ å·²å¯¼å‡ºåŸå§‹é‡‡é›†æ•°æ®: {dump_path}")

        except Exception as e:
            logger.warning(f"å¯¼å‡ºåŸå§‹é‡‡é›†æ•°æ®å¤±è´¥: {str(e)}")

    def _serialize_item(self, item):
        """å°†é‡‡é›†ç»“æœè½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„ç»“æ„"""
        if is_dataclass(item):
            data = asdict(item)
        elif isinstance(item, dict):
            data = dict(item)
        else:
            data = {}
            for attr in dir(item):
                if attr.startswith('_'):
                    continue
                value = getattr(item, attr)
                if callable(value):
                    continue
                data[attr] = value

        return self._normalize_for_json(data)

    def _normalize_for_json(self, value):
        if isinstance(value, datetime):
            return value.isoformat()
        if is_dataclass(value):
            return self._normalize_for_json(asdict(value))
        if isinstance(value, dict):
            return {str(k): self._normalize_for_json(v) for k, v in value.items()}
        if isinstance(value, list):
            return [self._normalize_for_json(v) for v in value]
        if isinstance(value, tuple):
            return [self._normalize_for_json(v) for v in value]
        return value

    def _summarize_sources(self, items: list) -> dict:
        counts = {}
        for item in items:
            source = self._extract_attribute(item, ['source', 'repo_name', 'name']) or 'unknown'
            counts[source] = counts.get(source, 0) + 1
        return counts

    def _prepare_graph_documents(self, items: list) -> List[Dict[str, str]]:
        documents: List[Dict[str, str]] = []
        for index, item in enumerate(items):
            serialized = self._serialize_item(item) or {}
            doc_id = str(
                serialized.get('id')
                or serialized.get('guid')
                or serialized.get('slug')
                or serialized.get('url')
                or f'doc-{index}'
            )
            title = str(
                serialized.get('title')
                or serialized.get('name')
                or serialized.get('headline')
                or serialized.get('repo_name')
                or serialized.get('source')
                or 'æœªå‘½åæ¡ç›®'
            )
            summary = str(
                serialized.get('summary')
                or serialized.get('description')
                or serialized.get('excerpt')
                or ''
            )
            content = str(
                serialized.get('content')
                or serialized.get('body')
                or serialized.get('text')
                or serialized.get('full_text')
                or summary
                or title
            )
            source = str(
                serialized.get('source')
                or serialized.get('feed')
                or serialized.get('repo_name')
                or serialized.get('platform')
                or 'unknown'
            )

            documents.append(
                {
                    'id': doc_id,
                    'title': title.strip(),
                    'summary': summary.strip(),
                    'content': content.strip(),
                    'source': source,
                }
            )
        return documents

    def _extract_attribute(self, item, candidates) -> str:
        if isinstance(item, dict):
            for key in candidates:
                value = item.get(key)
                if value:
                    return str(value)
            return ''

        for key in candidates:
            if hasattr(item, key):
                value = getattr(item, key)
                if value:
                    return str(value)
        return ''
    
    def _collect_data(self, days_back: int) -> list:
        """æ•°æ®é‡‡é›†é˜¶æ®µ"""
        all_items = []
        
        # é‡‡é›†RSS
        logger.info("\nğŸ“¡ é‡‡é›†RSSè®¢é˜…...")
        try:
            rss_sources = self.sources_config.get('rss_feeds', [])
            if rss_sources:
                self.rss_collector = RSSCollector(rss_sources)
                rss_items = self.rss_collector.collect_all(days_back=days_back)
                all_items.extend(rss_items)
                logger.info(f"âœ“ RSSé‡‡é›†å®Œæˆ: {len(rss_items)} æ¡ç›®")
            else:
                logger.warning("æœªé…ç½®RSSæº")
        except Exception as e:
            logger.error(f"RSSé‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†GitHub Releases
        logger.info("\nğŸ“¡ é‡‡é›†GitHub Releases...")
        try:
            github_repos = self.sources_config.get('github_repos', [])
            if github_repos:
                github_token = os.getenv('GITHUB_TOKEN')
                self.github_collector = GitHubCollector(github_repos, github_token)
                
                # æ£€æŸ¥APIé™åˆ¶
                rate_limit = self.github_collector.check_rate_limit()
                if rate_limit:
                    logger.info(f"GitHub APIå‰©ä½™: {rate_limit.get('remaining', 'N/A')}/{rate_limit.get('limit', 'N/A')}")
                
                github_releases = self.github_collector.collect_all(days_back=days_back)
                all_items.extend(github_releases)
                logger.info(f"âœ“ GitHubé‡‡é›†å®Œæˆ: {len(github_releases)} ä¸ªRelease")
            else:
                logger.warning("æœªé…ç½®GitHubä»“åº“")
        except Exception as e:
            logger.error(f"GitHubé‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†Hacker News
        logger.info("\nğŸ“¡ é‡‡é›†Hacker News...")
        try:
            hn_config = self.sources_config.get('hacker_news', {})
            if hn_config.get('enabled', False):
                hn_collector = HackerNewsCollector(
                    query_tags=hn_config.get('query_tags', ['AI', 'LLM']),
                    min_points=hn_config.get('min_points', 50)
                )
                hn_items = hn_collector.collect(days_back=days_back)
                all_items.extend(hn_items)
                logger.info(f"âœ“ HackerNewsé‡‡é›†å®Œæˆ: {len(hn_items)} æ¡ç›®")
            else:
                logger.info("HackerNewsé‡‡é›†æœªå¯ç”¨")
        except Exception as e:
            logger.error(f"HackerNewsé‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†Reddit
        logger.info("\nğŸ“¡ é‡‡é›†Reddit...")
        try:
            reddit_configs = self.sources_config.get('reddit', [])
            if reddit_configs:
                reddit_collector = RedditCollector(reddit_configs)
                reddit_items = reddit_collector.collect_all(days_back=days_back)
                all_items.extend(reddit_items)
                logger.info(f"âœ“ Reddité‡‡é›†å®Œæˆ: {len(reddit_items)} æ¡ç›®")
            else:
                logger.info("æœªé…ç½®Redditæº")
        except Exception as e:
            logger.error(f"Reddité‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†è¡Œä¸šæ–°é—»
        logger.info("\nğŸ“¡ é‡‡é›†è¡Œä¸šæ–°é—»...")
        try:
            news_feeds = self.sources_config.get('news_feeds', [])
            if news_feeds:
                news_collector = NewsCollector(news_feeds)
                news_items = news_collector.collect_all(days_back=days_back)
                all_items.extend(news_items)
                logger.info(f"âœ“ æ–°é—»é‡‡é›†å®Œæˆ: {len(news_items)} æ¡ç›®")
            else:
                logger.info("æœªé…ç½®è¡Œä¸šæ–°é—»æº")
        except Exception as e:
            logger.error(f"æ–°é—»é‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†ProductHunt
        logger.info("\nğŸ“¡ é‡‡é›†ProductHunt...")
        try:
            ph_config = self.sources_config.get('producthunt', {})
            if ph_config:
                ph_collector = ProductHuntCollector(ph_config)
                ph_items = ph_collector.collect(days_back=days_back)
                all_items.extend(ph_items)
                logger.info(f"âœ“ ProductHunté‡‡é›†å®Œæˆ: {len(ph_items)} æ¡ç›®")
            else:
                logger.info("æœªé…ç½®ProductHunt")
        except Exception as e:
            logger.error(f"ProductHunté‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†Twitterä¿¡å·
        logger.info("\nğŸ“¡ é‡‡é›†Twitterä¿¡å·...")
        try:
            twitter_config = self.sources_config.get('twitter', {})
            if twitter_config.get('enabled', False):
                twitter_collector = TwitterCollector(twitter_config)
                twitter_items = twitter_collector.collect()
                all_items.extend(twitter_items)
                logger.info(f"âœ“ Twitteré‡‡é›†å®Œæˆ: {len(twitter_items)} æ¡ç›®")
            else:
                logger.info("Twitteré‡‡é›†æœªå¯ç”¨")
        except Exception as e:
            logger.error(f"Twitteré‡‡é›†å¤±è´¥: {str(e)}")
        
        logger.info(f"\nğŸ“Š æ•°æ®é‡‡é›†æ€»è®¡: {len(all_items)} æ¡ç›®")
        return all_items
    
    def _collect_leaderboard(self) -> dict:
        """é‡‡é›†LMSYSæ’è¡Œæ¦œæ•°æ®"""
        try:
            logger.info("\nğŸ† é‡‡é›†LLMæ€§èƒ½æ’è¡Œæ¦œ...")
            leaderboard_collector = LeaderboardCollector()
            leaderboard_data = leaderboard_collector.collect(top_n=10)
            update_time = leaderboard_collector.get_update_time()
            
            logger.info(f"âœ“ æ’è¡Œæ¦œé‡‡é›†å®Œæˆ: {len(leaderboard_data)} ä¸ªæ¨¡å‹")
            
            return {
                'data': leaderboard_data,
                'update_time': update_time
            }
        except Exception as e:
            logger.error(f"æ’è¡Œæ¦œé‡‡é›†å¤±è´¥: {str(e)}")
            return {
                'data': [],
                'update_time': ''
            }
    
    def _collect_market_insights(self) -> list:
        """é‡‡é›†å¸‚åœºæ´å¯Ÿæ•°æ®"""
        try:
            logger.info("\nğŸ“ˆ é‡‡é›†å¸‚åœºæ´å¯Ÿ...")
            
            # ä»é…ç½®æ–‡ä»¶è·å–å¸‚åœºæ´å¯Ÿæºï¼ˆå¦‚æœæœ‰é…ç½®çš„è¯ï¼‰
            market_sources = self.sources_config.get('market_insights', [])
            
            market_collector = MarketInsightsCollector(market_sources if market_sources else None)
            all_insights = market_collector.collect(days_back=30)
            
            # è·å–Top 3æœ€é‡è¦çš„æ´å¯Ÿ
            top_insights = market_collector.get_top_insights(all_insights, top_n=3)
            
            logger.info(f"âœ“ å¸‚åœºæ´å¯Ÿé‡‡é›†å®Œæˆ: {len(all_insights)} æ¡ï¼Œç­›é€‰ Top {len(top_insights)}")
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆæ¨¡æ¿éœ€è¦ï¼‰
            return [insight.to_dict() for insight in top_insights]
            
        except Exception as e:
            logger.error(f"å¸‚åœºæ´å¯Ÿé‡‡é›†å¤±è´¥: {str(e)}")
            return []
    
    def _process_with_ai(self, items: list) -> list:
        """AIå¤„ç†é˜¶æ®µ - ä½¿ç”¨æ‰¹é‡å¤„ç†ä¼˜åŒ–"""
        try:
            api_key = os.getenv('POE_API_KEY')
            if not api_key:
                logger.error("æœªæ‰¾åˆ°POE_API_KEYç¯å¢ƒå˜é‡")
                return []
            
            # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨Haiku
            model = os.getenv('DEVELOPER_MODEL', 'Claude-Sonnet-4.5')
            
            # ä½¿ç”¨æ‰¹é‡å¤„ç†å™¨ï¼ˆæ–°æ–¹æ¡ˆï¼š1æ¬¡APIè°ƒç”¨ä»£æ›¿158æ¬¡ï¼‰
            batch_processor = AIProcessorBatch(
                api_key=api_key,
                model_name=model,
                user_profile=self.user_profile,
                explicit_feedback_manager=self.explicit_feedback,
            )
            
            logger.info(f"ğŸš€ æ‰¹é‡AIå¤„ç†æ¨¡å¼: {len(items)} æ¡ â†’ ç­›é€‰ Top 25")
            logger.info("ï¼ˆ1æ¬¡APIè°ƒç”¨ï¼Œé¢„è®¡1-2åˆ†é’Ÿï¼‰")
            
            # æ‰¹é‡ç­›é€‰å’Œåˆ†æï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰
            processed_items = batch_processor.batch_select_and_analyze(
                all_items=items,
                top_n=25  # åªç­›é€‰æœ€é‡è¦çš„25æ¡
            )

            self._log_ab_metric(processed_items)
            
            # æ˜¾ç¤ºç›¸å…³æ€§åˆ†å¸ƒ
            high_relevance = len([i for i in processed_items if i.relevance_score >= 8])
            medium_relevance = len([i for i in processed_items if 5 <= i.relevance_score < 8])
            low_relevance = len([i for i in processed_items if i.relevance_score < 5])
            
            logger.info(f"\nğŸ“Š ç›¸å…³æ€§åˆ†å¸ƒ:")
            logger.info(f"  - é«˜ç›¸å…³ (â‰¥8åˆ†): {high_relevance} æ¡")
            logger.info(f"  - ä¸­ç›¸å…³ (5-7åˆ†): {medium_relevance} æ¡")
            logger.info(f"  - ä½ç›¸å…³ (<5åˆ†): {low_relevance} æ¡")
            
            # æ˜¾ç¤ºåˆ†ç±»åˆ†å¸ƒ
            from collections import Counter
            category_dist = Counter([i.category for i in processed_items])
            logger.info(f"\nğŸ“‚ åˆ†ç±»åˆ†å¸ƒ:")
            for category, count in category_dist.most_common():
                logger.info(f"  - {category}: {count} æ¡")
            
            return processed_items
            
        except Exception as e:
            logger.error(f"æ‰¹é‡AIå¤„ç†å¤±è´¥: {str(e)}")
            logger.info("å°è¯•é™çº§åˆ°ä¼ ç»Ÿå¤„ç†æ¨¡å¼...")
            
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿé€æ¡å¤„ç†ï¼ˆå‰30æ¡ï¼‰
            try:
                self.ai_processor = AIProcessor(
                    api_key=api_key,
                    user_profile=self.user_profile,
                    model=model,
                    explicit_feedback_manager=self.explicit_feedback,
                )
                logger.info(f"ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼å¤„ç†å‰30æ¡...")
                processed_fallback = self.ai_processor.process_batch(items[:30])
                self._log_ab_metric(processed_fallback)
                return processed_fallback
            except Exception as fallback_error:
                logger.error(f"é™çº§å¤„ç†ä¹Ÿå¤±è´¥: {str(fallback_error)}")
                return []
    
    def _generate_action_items(self, processed_items: list) -> dict:
        """ç”Ÿæˆè¡ŒåŠ¨æ¸…å•ï¼ˆå»é‡ï¼šæ’é™¤å·²åœ¨å¿…çœ‹å†…å®¹ä¸­çš„æ–°é—»ï¼‰"""
        try:
            # ç®€åŒ–ç‰ˆï¼šä»å¤„ç†ç»“æœä¸­æå–actionable items
            filtering_prefs = self.filtering_preferences
            ignore_keywords = [
                keyword.lower() for keyword in filtering_prefs.get("ignore_keywords", [])
            ]
            minimum_optional_score = filtering_prefs.get("minimum_optional_score", 6)

            # ğŸ”‘ å…³é”®æ”¹è¿›ï¼šå…ˆè¯†åˆ«å‡º"å¿…çœ‹å†…å®¹"ï¼ˆpersonal_priority >= 8ï¼‰
            must_read_urls = set()
            for item in processed_items:
                if item.personal_priority >= 8:  # é™ä½é˜ˆå€¼ä»9åˆ°8
                    must_read_urls.add(item.url)
            
            logger.info(f"ğŸ“Œ è¯†åˆ«åˆ° {len(must_read_urls)} æ¡å¿…çœ‹å†…å®¹ï¼Œå°†ä»å»ºè®®è¡ŒåŠ¨ä¸­æ’é™¤")

            must_do = []
            nice_to_have = []
            
            for item in processed_items:
                # è·³è¿‡å·²ç»åœ¨"å¿…çœ‹å†…å®¹"ä¸­çš„æ–°é—»
                if item.url in must_read_urls:
                    logger.debug(f"è·³è¿‡é‡å¤æ–°é—»ï¼ˆå·²åœ¨å¿…çœ‹å†…å®¹ï¼‰: {item.title}")
                    continue
                
                title_lower = (item.title or "").lower()
                if any(keyword in title_lower for keyword in ignore_keywords):
                    logger.debug(f"è¿‡æ»¤æ‰ä½ä»·å€¼å†…å®¹: {item.title}")
                    continue

                if not item.actionable:
                    continue

                impact_text = item.impact_analysis or item.why_matters_to_you or ""

                if item.relevance_score is None:
                    continue

                if item.relevance_score >= 8:
                    must_do.append({
                        'title': item.title,
                        'action': impact_text,
                        'source': item.source,
                        'url': item.url
                    })
                elif item.relevance_score >= minimum_optional_score:
                    nice_to_have.append({
                        'title': item.title,
                        'action': impact_text,
                        'source': item.source,
                        'url': item.url
                    })
            
            action_items = {
                'must_do': must_do[:5],  # æœ€å¤š5é¡¹
                'nice_to_have': nice_to_have[:5]  # æœ€å¤š5é¡¹
            }
            
            logger.info(f"\nğŸ“‹ è¡ŒåŠ¨æ¸…å•:")
            logger.info(f"  - å¿…åšä»»åŠ¡: {len(action_items['must_do'])} é¡¹")
            logger.info(f"  - å¯é€‰ä»»åŠ¡: {len(action_items['nice_to_have'])} é¡¹")
            
            return action_items
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¡ŒåŠ¨æ¸…å•å¤±è´¥: {str(e)}")
            return {'must_do': [], 'nice_to_have': []}
    
    def _generate_report(
        self,
        processed_items: list,
        action_items: dict,
        leaderboard_info: dict,
        market_insights: list,
        output_dir: Optional[str] = None,
        learning_results: Optional[dict] = None,
    ) -> str:
        """ç”ŸæˆæŠ¥å‘Š"""
        try:
            self.report_generator = ReportGenerator(
                headline_source_limit=self.headline_source_limit
            )
            
            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if output_dir is None:
                output_dir = str(project_root / "output")
            
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            date_str = datetime.now().strftime('%Y-%m-%d')
            output_path = os.path.join(output_dir, f"weekly_report_{date_str}.md")
            
            # ç”ŸæˆæŠ¥å‘Š
            logger.info(f"ğŸ“ ç”ŸæˆMarkdownæŠ¥å‘Š...")
            report = self.report_generator.generate_report(
                processed_items=processed_items,
                action_items=action_items,
                leaderboard_data=leaderboard_info.get('data', []),
                leaderboard_update_time=leaderboard_info.get('update_time', ''),
                market_insights=market_insights,
                output_path=output_path,
                learning_results=learning_results or {},
            )
            
            # æ˜¾ç¤ºç»Ÿè®¡
            logger.info(f"\nğŸ“„ æŠ¥å‘Šç»Ÿè®¡:")
            logger.info(f"  - æ€»å­—æ•°: {len(report)} å­—ç¬¦")
            logger.info(f"  - è¾“å‡ºè·¯å¾„: {output_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            raise

    def _run_learning_cycle(self, processed_items: list) -> dict:
        """è¿è¡Œè‡ªæˆ‘å­¦ä¹ å¾ªç¯"""
        try:
            logger.info("\nğŸ§  è¿è¡Œè‡ªæˆ‘å­¦ä¹ å¾ªç¯...")
            is_weekly = self._is_weekly_report_day()
            learning_results = self.learning_engine.run_cycle(processed_items, is_weekly=is_weekly)
            logger.info("âœ“ è‡ªæˆ‘å­¦ä¹ å¾ªç¯å®Œæˆã€‚")
            return learning_results
        except Exception as e:
            logger.error(f"è‡ªæˆ‘å­¦ä¹ å¾ªç¯å¤±è´¥: {str(e)}")
            return {
                "auto_applied": [],
                "require_review": [],
                "insights": [],
                "priority_adjustments": [],
                "discovery": {},
                "models": {},
                "weekly_summary": None,
                "is_weekly": False,
            }

    def _log_learning_summary(self, learning_results: dict) -> None:
        """è®°å½•å­¦ä¹ å¼•æ“çš„æ€»ç»“"""
        if not learning_results:
            logger.info("\nğŸ“š å­¦ä¹ å¼•æ“æœªè¿è¡Œæˆ–æ— ç»“æœã€‚")
            return

        auto_applied = learning_results.get("auto_applied", [])
        require_review = learning_results.get("require_review", [])
        discovery = learning_results.get("discovery", {})
        models = learning_results.get("models", {})

        logger.info("\nğŸ“š å­¦ä¹ å¼•æ“æ€»ç»“:")
        logger.info(f"  - è‡ªåŠ¨åº”ç”¨ä¼˜åŒ–: {len(auto_applied)} é¡¹")
        logger.info(f"  - å¾…å®¡æŸ¥å»ºè®®: {len(require_review)} é¡¹")
        if discovery:
            logger.info(
                "  - æ–°ä¿¡æ¯æºè¯„ä¼°: %s ä¸ªå€™é€‰ (è‡ªåŠ¨æ·»åŠ  %s ä¸ª)",
                discovery.get("evaluated", 0),
                len(discovery.get("auto_add_candidates", [])),
            )
        if models:
            logger.info(
                "  - æ–°æ¨¡å‹è¯„ä¼°: %s ä¸ª (é‡ç‚¹å…³æ³¨ %s ä¸ª)",
                models.get("evaluated", 0),
                len(models.get("flagged", [])),
            )

        if learning_results.get("weekly_summary"):
            summary = learning_results["weekly_summary"]
            logger.info(
                "  - æœ¬å‘¨æ–°å¢ä¿¡æ¯æº: %s ä¸ªï¼Œç§»é™¤ä¿¡æ¯æº: %s ä¸ª",
                len(summary.get("sources_added", [])),
                len(summary.get("sources_removed", [])),
            )

    def _is_weekly_report_day(self) -> bool:
        weekly_cfg = (self.learning_config or {}).get("weekly_summary", {})
        target_day = weekly_cfg.get("day_of_week", 0)
        try:
            target_day = int(target_day)
        except (TypeError, ValueError):
            target_day = 0
        return datetime.now().weekday() == target_day

    # ------------------------------------------------------------------
    # CLI helpers
    # ------------------------------------------------------------------
    def list_recommendations(self) -> None:
        threshold = (self.learning_config.get("source_discovery", {}) or {}).get(
            "min_quality_for_recommendation", 7.0
        )
        candidates = self.learning_engine.db.get_pending_sources(threshold)
        if not candidates:
            print("æš‚æ— å¾…å®¡æ‰¹çš„æ–°å¢ä¿¡æ¯æºã€‚")
            return

        print("å¾…å®¡æ‰¹ä¿¡æ¯æºåˆ—è¡¨:\n")
        for idx, candidate in enumerate(candidates, 1):
            print(
                f"{idx}. {candidate.get('name') or candidate.get('url')}"
                f" | ç±»å‹: {candidate.get('type')}"
                f" | è´¨é‡: {candidate.get('quality_score', '?')}/10"
                f" | é“¾æ¥: {candidate.get('url')}"
            )

    def apply_recommendation(self, identifier: str) -> None:
        candidates = self.learning_engine.db.get_pending_sources(0)
        target = self._find_candidate(candidates, identifier)
        if not target:
            logger.error("æœªæ‰¾åˆ°åŒ¹é…çš„å€™é€‰ä¿¡æ¯æºï¼š%s", identifier)
            return

        if self.config_manager.add_source(target):
            self.config_manager.save()
            logger.info("å·²æ›´æ–°sources.yamlï¼Œæ–°å¢ä¿¡æ¯æºï¼š%s", target.get("name") or target.get("url"))
        else:
            logger.info("ä¿¡æ¯æºå·²å­˜åœ¨äºé…ç½®ä¸­ï¼Œè·³è¿‡å†™å…¥ã€‚")

        self.learning_engine.db.update_discovered_source_status(target.get("url"), "approved")
        self.learning_engine.db.log_optimization(
            OptimizationRecord(
                optimization_type="add_source_manual",
                target=target.get("url"),
                details={"name": target.get("name"), "type": target.get("type")},
            )
        )

    def reject_recommendation(self, identifier: str) -> None:
        candidates = self.learning_engine.db.get_pending_sources(0)
        target = self._find_candidate(candidates, identifier)
        if not target:
            logger.error("æœªæ‰¾åˆ°åŒ¹é…çš„å€™é€‰ä¿¡æ¯æºï¼š%s", identifier)
            return
        self.learning_engine.db.update_discovered_source_status(target.get("url"), "rejected")
        logger.info("å·²æ‹’ç»ä¿¡æ¯æºï¼š%s", target.get("name") or target.get("url"))

    def print_learning_summary(self) -> None:
        summary = self.learning_engine.generate_weekly_summary()
        print("å­¦ä¹ å¼•æ“æ‘˜è¦ï¼š")
        print(f"- æœ¬å‘¨æ–°å¢ä¿¡æ¯æºï¼š{len(summary.get('sources_added', []))}")
        print(f"- æœ¬å‘¨åœç”¨ä¿¡æ¯æºï¼š{len(summary.get('sources_removed', []))}")
        print(f"- æœ¬å‘¨æ¨¡å‹è¯„ä¼°ï¼š{len(summary.get('models_evaluated', []))}\n")

    def _find_candidate(self, candidates: list, identifier: str) -> Optional[dict]:
        slug = self._slugify(identifier)
        for candidate in candidates:
            url = candidate.get("url", "")
            name = candidate.get("name", "")
            if identifier == url or identifier == name:
                return candidate
            if self._slugify(url) == slug or self._slugify(name) == slug:
                return candidate
        return None

    def _slugify(self, value: str) -> str:
        return "".join(ch.lower() if ch.isalnum() else "-" for ch in value or "").strip("-")

    def _write_langgraph_report(
        self,
        state: Dict,
        output_dir: Optional[str] = None,
    ) -> Optional[Path]:
        output_path = Path(output_dir) if output_dir else project_root / "output"
        output_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        report_path = output_path / f"langgraph_report_{timestamp}.md"

        briefing = state.get("final_briefing") or state.get("briefing_draft") or ""
        differential = state.get("differential_analysis", []) or []
        suggestions = state.get("proactive_suggestions", []) or []

        lines: List[str] = [
            "# ğŸ§  AI Intelligence Briefing (LangGraph Experimental)",
            "",
        ]

        if briefing:
            lines.append(briefing)
            if not briefing.endswith("\n"):
                lines.append("")
        else:
            lines.append("ï¼ˆå½“å‰å°šæœªç”Ÿæˆç®€æŠ¥è‰ç¨¿ï¼‰\n")

        if differential:
            lines.append("## ğŸ” å·®å¼‚åˆ†æ (RAG-Diff)")
            for idx, insight in enumerate(differential, start=1):
                lines.append(f"### ä¸»é¢˜ {idx}")
                if insight.get("new_findings"):
                    lines.append("- æ–°å‘ç°ï¼š" + "ï¼›".join(insight["new_findings"]))
                if insight.get("updates"):
                    lines.append("- æ›´æ–°ï¼š" + "ï¼›".join(insight["updates"]))
                if insight.get("contradictions"):
                    lines.append("- çŸ›ç›¾ï¼š" + "ï¼›".join(insight["contradictions"]))
                if insight.get("meta_analysis"):
                    lines.append(f"- é‡è¦æ€§ï¼š{insight['meta_analysis']}")
                lines.append("")

        if suggestions:
            lines.append("## ğŸš€ ä¸»åŠ¨å»ºè®®")
            for suggestion in suggestions:
                title = suggestion.get("title", "å»ºè®®")
                reason = suggestion.get("reason", "")
                action = suggestion.get("action", "")
                related = suggestion.get("related_topics", [])
                lines.append(f"- **{title}**")
                if reason:
                    lines.append(f"  - åŸå› ï¼š{reason}")
                if action:
                    lines.append(f"  - è¡ŒåŠ¨ï¼š{action}")
                if related:
                    lines.append(f"  - å…³è”ä¸»é¢˜ï¼š{', '.join(related)}")
                lines.append("")

        with open(report_path, "w", encoding="utf-8") as handle:
            handle.write("\n".join(lines))

        return report_path

    def _log_ab_metric(self, processed_items: list) -> None:
        experiment = self.ab_experiments.get("narrative_clustering_v1")
        if not experiment or not processed_items:
            return

        variant = os.getenv("AB_NARRATIVE_VARIANT")
        if not variant:
            variant = "treatment"

        try:
            engagement_score = sum(
                getattr(item, "personal_priority", 0) for item in processed_items
            ) / len(processed_items)
        except Exception:
            engagement_score = 0.0

        self.ab_tester.log_metric(
            experiment,
            variant,
            engagement_score,
        )

    def _load_email_settings(self) -> Dict[str, Any]:
        recipients = os.getenv("DIGEST_EMAIL_TO") or "davidzheng0119@163.com"
        parsed_recipients = [addr.strip() for addr in recipients.split(",") if addr.strip()]

        settings = {
            "recipients": parsed_recipients,
            "smtp_host": os.getenv("DIGEST_SMTP_HOST"),
            "smtp_port": int(os.getenv("DIGEST_SMTP_PORT", "465")),
            "smtp_user": os.getenv("DIGEST_SMTP_USER"),
            "smtp_pass": os.getenv("DIGEST_SMTP_PASS"),
            "sender": os.getenv("DIGEST_EMAIL_FROM"),
        }

        required_fields = ("smtp_host", "smtp_user", "smtp_pass")
        settings["enabled"] = all(settings.get(field) for field in required_fields)
        return settings

    def _compose_email_body(self, report_path: Path) -> str:
        text = report_path.read_text(encoding="utf-8")
        snippet_limit = int(os.getenv("DIGEST_EMAIL_BODY_LIMIT", "8000"))
        if len(text) > snippet_limit:
            return text[:snippet_limit] + "\n\nï¼ˆå†…å®¹è¾ƒé•¿ï¼Œå®Œæ•´å†…å®¹è§é™„ä»¶ï¼‰"
        return text

    def _send_email_if_configured(self, report_path: Optional[str]) -> None:
        if not report_path:
            return

        settings = self.email_settings
        if not settings.get("enabled"):
            logger.info("é‚®ä»¶é€šçŸ¥æœªå¯ç”¨ï¼Œç¼ºå°‘SMTPé…ç½®ï¼Œå·²è·³è¿‡å‘é€ã€‚")
            return

        report_path = Path(report_path)
        try:
            subject = f"AI æƒ…æŠ¥ç®€æŠ¥ | {datetime.now().strftime('%Y-%m-%d')}"
            body = self._compose_email_body(report_path)
            send_digest_email(
                report_path,
                subject,
                settings.get("recipients", []),
                smtp_host=settings["smtp_host"],
                smtp_port=settings.get("smtp_port", 465),
                smtp_user=settings["smtp_user"],
                smtp_password=settings["smtp_pass"],
                sender=settings.get("sender"),
                body_text=body,
            )
        except Exception as e:
            logger.error("å‘é€ç®€æŠ¥é‚®ä»¶å¤±è´¥: %s", e)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Digest Report Generator")
    parser.add_argument("--days-back", type=int, default=3, help="é‡‡é›†æœ€è¿‘Nå¤©çš„æ•°æ®")
    parser.add_argument("--list-recommendations", action="store_true", help="åˆ—å‡ºå¾…å®¡æ‰¹çš„ä¿¡æ¯æº")
    parser.add_argument("--apply-recommendation", help="æ‰¹å‡†å¹¶åŠ å…¥é…ç½®çš„æ–°ä¿¡æ¯æºï¼ˆè¾“å…¥URLæˆ–åç§°ï¼‰")
    parser.add_argument("--reject-recommendation", help="æ‹’ç»å€™é€‰ä¿¡æ¯æºï¼ˆè¾“å…¥URLæˆ–åç§°ï¼‰")
    parser.add_argument("--learning-summary", action="store_true", help="æ‰“å°å­¦ä¹ å¼•æ“çš„å‘¨åº¦æ‘˜è¦")
    parser.add_argument("--learning-only", action="store_true", help="ä»…è¿è¡Œå­¦ä¹ å¾ªç¯ï¼Œè·³è¿‡å‘¨æŠ¥ç”Ÿæˆ")
    parser.add_argument("--use-langgraph", action="store_true", help="ä½¿ç”¨ LangGraph å·¥ä½œæµç”Ÿæˆå®éªŒæ€§ç®€æŠ¥")
    parser.add_argument("--ab-summary", action="store_true", help="è¾“å‡ºå½“å‰ABæµ‹è¯•ç»Ÿè®¡æ‘˜è¦")
    args = parser.parse_args()

    try:
        generator = WeeklyReportGenerator()
        
        if args.list_recommendations:
            generator.list_recommendations()
            return
        if args.apply_recommendation:
            generator.apply_recommendation(args.apply_recommendation)
            return
        if args.reject_recommendation:
            generator.reject_recommendation(args.reject_recommendation)
            return
        if args.learning_summary:
            generator.print_learning_summary()
            return
        if args.ab_summary:
            generator.print_ab_summary()
            return

        if args.use_langgraph:
            if args.learning_only:
                logger.warning("LangGraph æ¨¡å¼æš‚ä¸æ”¯æŒ learning-only å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥é€‰é¡¹ã€‚")
            generator.run_langgraph(days_back=args.days_back)
            return

        generator.run(days_back=args.days_back, learning_only=args.learning_only)
        
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

