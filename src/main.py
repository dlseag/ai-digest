"""
AI Weekly Report Generator - Main Entry Point
ä¸»ç¨‹åºï¼šæ•´åˆæ‰€æœ‰æ¨¡å—ç”Ÿæˆå‘¨æŠ¥
"""

import argparse
import json
import logging
import os
import signal
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import asdict, is_dataclass
import yaml
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.collectors.rss_collector import RSSCollector
from src.collectors.github_collector import GitHubCollector
from src.collectors.hackernews_collector import HackerNewsCollector
from src.collectors.reddit_collector import RedditCollector
from src.collectors.news_collector import NewsCollector
from src.collectors.producthunt_collector import ProductHuntCollector
from src.collectors.twitter_collector import TwitterCollector
from src.collectors.leaderboard_collector import LeaderboardCollector  # æŽ’è¡Œæ¦œé‡‡é›†å™¨
from src.collectors.market_insights_collector import MarketInsightsCollector  # å¸‚åœºæ´žå¯Ÿé‡‡é›†å™¨
from src.processors.ai_processor import AIProcessor
from src.processors.ai_processor_batch import AIProcessorBatch  # æ‰¹é‡å¤„ç†å™¨
from src.generators.report_generator import ReportGenerator
from src.learning.learning_engine import LearningEngine
from src.learning.config_manager import ConfigManager
from src.learning.explicit_feedback import ExplicitFeedbackManager
from src.learning.ab_tester import ABTester, Experiment
from src.learning.feedback_learning import FeedbackLearningEngine
from src.storage.feedback_db import OptimizationRecord
from src.utils.emailer import send_digest_email
from src.graph.briefing_graph import BriefingState, compile_briefing_graph
from src.memory.memory_manager import MemoryManager
from src.memory.user_profile_manager import UserProfileManager
from src.agents.quick_filter_agent import QuickFilterAgent
from src.agents.action_agent import ActionAgent
from src.agents.tool_executor import ToolExecutor
from src.integrations.notion_sync import NotionSyncService, build_notion_title

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('weekly_report_generator.log')
    ]
)
logger = logging.getLogger(__name__)


class WeeklyReportGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, config_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–å‘¨æŠ¥ç”Ÿæˆå™¨
        
        Args:
            config_dir: é…ç½®æ–‡ä»¶ç›®å½•
        """
        # åŠ è½½çŽ¯å¢ƒå˜é‡
        load_dotenv()
        
        # é…ç½®ç›®å½•
        if config_dir is None:
            config_dir = str(project_root / "config")
        self.config_dir = Path(config_dir)
        
        # åŠ è½½é…ç½®
        self.sources_config = self._load_yaml(self.config_dir / "sources.yaml")
        raw_user_profile = self._load_yaml(self.config_dir / "user_profile.yaml")
        raw_learning_config = self._load_yaml(self.config_dir / "learning_config.yaml")
        self.learning_config = (raw_learning_config or {}).get("learning", {})
        self.source_preferences = self.learning_config.get("source_preferences", {})
        self.headline_source_limit = int(self.source_preferences.get("max_headlines_per_source", 2))

        self.user_profile_manager = UserProfileManager(
            self.config_dir / "user_profile.yaml",
            profile_data=raw_user_profile,
        )
        self.user_profile = self.user_profile_manager.get_profile()
        self.filtering_preferences = self.user_profile.get("filtering_preferences", {}) or {}

        # èŽ·å–APIå¯†é’¥
        self.api_key = os.getenv("POE_API_KEY")
        if not self.api_key:
            logger.warning("POE_API_KEY çŽ¯å¢ƒå˜é‡æœªè®¾ç½®")
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.rss_collector = None
        self.github_collector = None
        self.ai_processor = None
        self.report_generator = None
        self.learning_engine = LearningEngine(
            config=self.learning_config,
            project_root=project_root,
            user_profile_manager=self.user_profile_manager,
            api_key=self.api_key,
        )
        self.explicit_feedback = ExplicitFeedbackManager(self.learning_engine.db)
        self.ab_tester = ABTester(self.learning_engine.db)
        self.email_settings = self._load_email_settings()
        self.ab_experiments: Dict[str, Experiment] = {
            "narrative_clustering_v1": Experiment(
                id="narrative_clustering_v1",
                hypothesis="Narrative clusteringæå‡ä¸ªæ€§åŒ–å‚ä¸Žåº¦",
                metric="engagement_score",
                variants={
                    "control": "ä¼ ç»Ÿçº¿æ€§æ‘˜è¦",
                    "treatment": "å™äº‹èšç±» + RAG-Diff æ‘˜è¦",
                },
            ),
            "scoring_threshold_v1": Experiment(
                id="scoring_threshold_v1",
                hypothesis="æé«˜ optional é˜ˆå€¼èƒ½æå‡å»ºè®®è´¨é‡",
                metric="engagement_score",
                variants={
                    "control": "optional_threshold_6",
                    "treatment": "optional_threshold_7",
                },
            ),
        }
        self.ab_variants: Dict[str, str] = {}
        user_identifier = (
            self.user_profile.get("user_info", {}).get("email")
            or self.user_profile.get("user_info", {}).get("name")
            or "default_user"
        )
        for exp_id, experiment in self.ab_experiments.items():
            try:
                assigned = self.ab_tester.assign_variant(user_identifier, experiment)
            except Exception:
                assigned = "control"
            self.ab_variants[exp_id] = assigned

        override_variant = os.getenv("AB_NARRATIVE_VARIANT")
        if override_variant:
            self.ab_variants["narrative_clustering_v1"] = override_variant

        self.config_manager = ConfigManager(self.config_dir / "sources.yaml")
        self.memory_manager = MemoryManager()
        self.notion_sync = NotionSyncService()
        
        # åˆå§‹åŒ– QuickFilterAgentï¼ˆå¦‚æžœ API key å¯ç”¨ï¼‰
        self.quick_filter_agent: Optional[QuickFilterAgent] = None
        if self.api_key:
            try:
                model = os.getenv("QUICK_FILTER_MODEL", "Claude-Haiku-4.5")
                max_batch = int(os.getenv("QUICK_FILTER_BATCH", "12"))
                min_score = int(os.getenv("QUICK_FILTER_MIN_SCORE", "5"))
                self.quick_filter_agent = QuickFilterAgent(
                    api_key=self.api_key,
                    model_name=model,
                    max_batch_size=max_batch,
                    min_score_keep=min_score,
                )
                logger.debug("âœ“ QuickFilterAgent åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"QuickFilterAgent åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        else:
            logger.debug("QuickFilterAgent æœªåˆå§‹åŒ–ï¼šç¼ºå°‘ POE_API_KEY")

        self.briefing_graph = compile_briefing_graph(self)
        
        logger.info("=" * 60)
        logger.info("AI Weekly Report Generator å¯åŠ¨")
        logger.info("=" * 60)
    
    def _load_yaml(self, file_path: Path) -> dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return {}
    
    def run(
        self,
        days_back: int = 3,
        output_dir: Optional[str] = None,
        learning_only: bool = False,
    ):
        """æ‰§è¡Œå®Œæ•´çš„å‘¨æŠ¥ç”Ÿæˆæµç¨‹ï¼ˆåŸºäºŽ LangGraph ç¼–æŽ’ï¼‰"""
        params: Dict[str, Any] = {
            "days_back": days_back,
            "output_dir": output_dir,
            "learning_only": learning_only,
        }

        initial_state: BriefingState = {
            "params": params,
            "errors": [],
        }

        logger.info("\n" + "=" * 60)
        logger.info("å¯åŠ¨ LangGraph å·¥ä½œæµ")
        logger.info("=" * 60)

        try:
            final_state = self.briefing_graph.invoke(initial_state)
        except Exception as exc:  # pragma: no cover - safety net
            logger.error(f"ç”Ÿæˆå‘¨æŠ¥å¤±è´¥: {exc}", exc_info=True)
            raise

        errors = final_state.get("errors") or []
        for message in errors:
            logger.error(message)

        learning_results = final_state.get("learning_results") or {}
        if learning_results:
            self._log_learning_summary(learning_results)

        report_path_value = final_state.get("report_path")
        if learning_only:
            logger.info("\n" + "=" * 60)
            logger.info("âœ“ å·²å®Œæˆå­¦ä¹ å¾ªçŽ¯ (learning-only æ¨¡å¼)")
            logger.info("=" * 60)
        elif report_path_value:
            logger.info("\n" + "=" * 60)
            logger.info("âœ“ å‘¨æŠ¥ç”Ÿæˆå®Œæˆï¼")
            logger.info(f"âœ“ æŠ¥å‘Šè·¯å¾„: {report_path_value}")
            logger.info("=" * 60)
        else:
            logger.warning("æœ¬æ¬¡è¿è¡Œæœªç”Ÿæˆå‘¨æŠ¥ã€‚")

        return Path(report_path_value) if report_path_value else None

    def run_langgraph(
        self,
        days_back: int = 3,
        output_dir: Optional[str] = None,
        max_iterations: int = 3,
    ) -> Optional[Path]:
        """
        [å·²åºŸå¼ƒ] å…¼å®¹æ—§å‚æ•°ï¼Œå½“å‰ä¸Ž run() ç­‰ä»·ã€‚
        
        æ³¨æ„ï¼šé»˜è®¤ run() æ–¹æ³•å·²ä½¿ç”¨ LangGraphï¼Œæ­¤æ–¹æ³•ä»…ä¸ºå‘åŽå…¼å®¹ä¿ç•™ã€‚
        å»ºè®®ç›´æŽ¥ä½¿ç”¨ run() æ–¹æ³•ã€‚
        """
        logger.warning("run_langgraph() å·²åºŸå¼ƒï¼Œé»˜è®¤ run() æ–¹æ³•å·²ä½¿ç”¨ LangGraphã€‚è¯·ç›´æŽ¥ä½¿ç”¨ run()ã€‚")
        return self.run(days_back=days_back, output_dir=output_dir, learning_only=False)

    def _dump_collected_items(self, items: list, days_back: int, output_dir: Optional[str]) -> None:
        """å°†åŽŸå§‹é‡‡é›†ç»“æžœå†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œä¾¿äºŽè°ƒè¯•"""
        try:
            if output_dir is None:
                dump_dir = project_root / "output"
            else:
                dump_dir = Path(output_dir)

            dump_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
            dump_path = dump_dir / f"collected_items_{timestamp}.json"

            payload = {
                "generated_at": datetime.now().isoformat(),
                "days_back": days_back,
                "total_items": len(items),
                "source_breakdown": self._summarize_sources(items),
                "items": [self._serialize_item(item) for item in items]
            }

            with open(dump_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)

            logger.info(f"ðŸ—‚ï¸ å·²å¯¼å‡ºåŽŸå§‹é‡‡é›†æ•°æ®: {dump_path}")

        except Exception as e:
            logger.warning(f"å¯¼å‡ºåŽŸå§‹é‡‡é›†æ•°æ®å¤±è´¥: {str(e)}")

    def _serialize_item(self, item):
        """å°†é‡‡é›†ç»“æžœè½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„ç»“æž„"""
        if is_dataclass(item):
            data = asdict(item)
        elif isinstance(item, dict):
            data = dict(item)
        else:
            data = {}
            for attr in dir(item):
                if attr.startswith('_'):
                    continue
                value = getattr(item, attr)
                if callable(value):
                    continue
                data[attr] = value

        return self._normalize_for_json(data)

    def _normalize_for_json(self, value):
        if isinstance(value, datetime):
            return value.isoformat()
        if is_dataclass(value):
            return self._normalize_for_json(asdict(value))
        if isinstance(value, dict):
            return {str(k): self._normalize_for_json(v) for k, v in value.items()}
        if isinstance(value, list):
            return [self._normalize_for_json(v) for v in value]
        if isinstance(value, tuple):
            return [self._normalize_for_json(v) for v in value]
        return value

    def _summarize_sources(self, items: list) -> dict:
        counts = {}
        for item in items:
            source = self._extract_attribute(item, ['source', 'repo_name', 'name']) or 'unknown'
            counts[source] = counts.get(source, 0) + 1
        return counts

    def _extract_attribute(self, item, candidates) -> str:
        if isinstance(item, dict):
            for key in candidates:
                value = item.get(key)
                if value:
                    return str(value)
            return ''

        for key in candidates:
            if hasattr(item, key):
                value = getattr(item, key)
                if value:
                    return str(value)
        return ''
    
    def _collect_data(self, days_back: int) -> list:
        """æ•°æ®é‡‡é›†é˜¶æ®µ"""
        all_items = []
        
        # é‡‡é›†RSS
        logger.info("\nðŸ“¡ é‡‡é›†RSSè®¢é˜…...")
        try:
            rss_sources = self.sources_config.get('rss_feeds', [])
            # è¿‡æ»¤ enabled=false çš„æº
            enabled_rss_sources = [s for s in rss_sources if s.get('enabled', True)]
            logger.info(f"RSSæº: {len(enabled_rss_sources)} ä¸ªå·²å¯ç”¨ / {len(rss_sources)} ä¸ªæ€»è®¡")
            
            if enabled_rss_sources:
                self.rss_collector = RSSCollector(enabled_rss_sources)
                rss_items = self.rss_collector.collect_all(days_back=days_back)
                all_items.extend(rss_items)
                logger.info(f"âœ“ RSSé‡‡é›†å®Œæˆ: {len(rss_items)} æ¡ç›®")
            else:
                logger.warning("æœªé…ç½®å¯ç”¨çš„RSSæº")
        except Exception as e:
            logger.error(f"RSSé‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†GitHub Releases
        logger.info("\nðŸ“¡ é‡‡é›†GitHub Releases...")
        try:
            github_repos = self.sources_config.get('github_repos', [])
            if github_repos:
                github_token = os.getenv('GITHUB_TOKEN')
                self.github_collector = GitHubCollector(github_repos, github_token)
                
                # æ£€æŸ¥APIé™åˆ¶
                rate_limit = self.github_collector.check_rate_limit()
                if rate_limit:
                    logger.info(f"GitHub APIå‰©ä½™: {rate_limit.get('remaining', 'N/A')}/{rate_limit.get('limit', 'N/A')}")
                
                github_releases = self.github_collector.collect_all(days_back=days_back)
                all_items.extend(github_releases)
                logger.info(f"âœ“ GitHubé‡‡é›†å®Œæˆ: {len(github_releases)} ä¸ªRelease")
            else:
                logger.warning("æœªé…ç½®GitHubä»“åº“")
        except Exception as e:
            logger.error(f"GitHubé‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†Hacker News
        logger.info("\nðŸ“¡ é‡‡é›†Hacker News...")
        try:
            hn_config = self.sources_config.get('hacker_news', {})
            if hn_config.get('enabled', False):
                hn_collector = HackerNewsCollector(
                    query_tags=hn_config.get('query_tags', ['AI', 'LLM']),
                    min_points=hn_config.get('min_points', 50)
                )
                hn_items = hn_collector.collect(days_back=days_back)
                all_items.extend(hn_items)
                logger.info(f"âœ“ HackerNewsé‡‡é›†å®Œæˆ: {len(hn_items)} æ¡ç›®")
            else:
                logger.info("HackerNewsé‡‡é›†æœªå¯ç”¨")
        except Exception as e:
            logger.error(f"HackerNewsé‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†Reddit
        logger.info("\nðŸ“¡ é‡‡é›†Reddit...")
        try:
            reddit_configs = self.sources_config.get('reddit', [])
            if reddit_configs:
                reddit_collector = RedditCollector(reddit_configs)
                reddit_items = reddit_collector.collect_all(days_back=days_back)
                all_items.extend(reddit_items)
                logger.info(f"âœ“ Reddité‡‡é›†å®Œæˆ: {len(reddit_items)} æ¡ç›®")
            else:
                logger.info("æœªé…ç½®Redditæº")
        except Exception as e:
            logger.error(f"Reddité‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†è¡Œä¸šæ–°é—»
        logger.info("\nðŸ“¡ é‡‡é›†è¡Œä¸šæ–°é—»...")
        try:
            news_feeds = self.sources_config.get('news_feeds', [])
            if news_feeds:
                news_collector = NewsCollector(news_feeds)
                news_items = news_collector.collect_all(days_back=days_back)
                all_items.extend(news_items)
                logger.info(f"âœ“ æ–°é—»é‡‡é›†å®Œæˆ: {len(news_items)} æ¡ç›®")
            else:
                logger.info("æœªé…ç½®è¡Œä¸šæ–°é—»æº")
        except Exception as e:
            logger.error(f"æ–°é—»é‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†ProductHunt
        logger.info("\nðŸ“¡ é‡‡é›†ProductHunt...")
        try:
            ph_config = self.sources_config.get('producthunt', {})
            if ph_config:
                ph_collector = ProductHuntCollector(ph_config)
                ph_items = ph_collector.collect(days_back=days_back)
                all_items.extend(ph_items)
                logger.info(f"âœ“ ProductHunté‡‡é›†å®Œæˆ: {len(ph_items)} æ¡ç›®")
            else:
                logger.info("æœªé…ç½®ProductHunt")
        except Exception as e:
            logger.error(f"ProductHunté‡‡é›†å¤±è´¥: {str(e)}")
        
        # é‡‡é›†Twitterä¿¡å·
        logger.info("\nðŸ“¡ é‡‡é›†Twitterä¿¡å·...")
        try:
            twitter_config = self.sources_config.get('twitter', {})
            if twitter_config.get('enabled', False):
                twitter_collector = TwitterCollector(twitter_config)
                twitter_items = twitter_collector.collect()
                all_items.extend(twitter_items)
                logger.info(f"âœ“ Twitteré‡‡é›†å®Œæˆ: {len(twitter_items)} æ¡ç›®")
            else:
                logger.info("Twitteré‡‡é›†æœªå¯ç”¨")
        except Exception as e:
            logger.error(f"Twitteré‡‡é›†å¤±è´¥: {str(e)}")
        
        logger.info(f"\nðŸ“Š æ•°æ®é‡‡é›†æ€»è®¡: {len(all_items)} æ¡ç›®")
        return all_items
    

    def _collect_market_insights(self) -> list:
        """é‡‡é›†å¸‚åœºæ´žå¯Ÿæ•°æ®"""
        try:
            logger.info("\nðŸ“ˆ é‡‡é›†å¸‚åœºæ´žå¯Ÿ...")
            market_sources = self.sources_config.get('market_insights', [])
            market_collector = MarketInsightsCollector(market_sources if market_sources else None)
            all_insights = market_collector.collect(days_back=30)
            top_insights = market_collector.get_top_insights(all_insights, top_n=3)
            logger.info(f"âœ“ å¸‚åœºæ´žå¯Ÿé‡‡é›†å®Œæˆ: {len(all_insights)} æ¡ï¼Œç­›é€‰ Top {len(top_insights)}")
            return [insight.to_dict() for insight in top_insights]
        except Exception as e:
            logger.error(f"å¸‚åœºæ´žå¯Ÿé‡‡é›†å¤±è´¥: {str(e)}")
            return []

    
    def _collect_leaderboard(self) -> dict:
        """é‡‡é›†LMSYSæŽ’è¡Œæ¦œæ•°æ®"""
        try:
            logger.info("\nðŸ† é‡‡é›†LLMæ€§èƒ½æŽ’è¡Œæ¦œ...")
            leaderboard_collector = LeaderboardCollector()
            leaderboard_data = leaderboard_collector.collect(top_n=10)
            update_time = leaderboard_collector.get_update_time()
            
            logger.info(f"âœ“ æŽ’è¡Œæ¦œé‡‡é›†å®Œæˆ: {len(leaderboard_data)} ä¸ªæ¨¡åž‹")
            
            return {
                'data': leaderboard_data,
                'update_time': update_time
            }
        except Exception as e:
            logger.error(f"æŽ’è¡Œæ¦œé‡‡é›†å¤±è´¥: {str(e)}")
            return {
                'data': [],
                'update_time': ''
            }
    
    def _quick_filter_items(self, items: list) -> tuple[list, dict]:
        """Use a lightweight LLM pass to filter obvious noise before heavy processing."""
        total = len(items)
        if total == 0:
            return [], {"input_total": 0, "kept": 0, "dropped": 0, "avg_score": 0.0, "strategy": "empty"}

        # å¦‚æžœ QuickFilterAgent æœªåˆå§‹åŒ–ï¼ˆç¼ºå°‘ API key æˆ–åˆå§‹åŒ–å¤±è´¥ï¼‰ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ
        if self.quick_filter_agent is None:
            logger.debug("âš ï¸ å¿«é€Ÿåˆè¯„è·³è¿‡ï¼šQuickFilterAgent æœªåˆå§‹åŒ–")
            return list(items), {
                "input_total": total,
                "kept": total,
                "dropped": 0,
                "avg_score": 8.0,
                "strategy": "no_agent",
            }

        try:
            top_k = int(os.getenv("QUICK_FILTER_TOP_K", "60"))
            filtered, stats = self.quick_filter_agent.filter_items(items, top_k=top_k)
            logger.info(
                "âš¡ å¿«é€Ÿåˆè¯„: è¾“å…¥=%s, ä¿ç•™=%s, ä¸¢å¼ƒ=%s, å¹³å‡åˆ†=%s (ç­–ç•¥=%s)",
                stats.get("input_total"),
                stats.get("kept"),
                stats.get("dropped"),
                stats.get("avg_score"),
                stats.get("strategy"),
            )
            if stats.get("dropped"):
                try:
                    self.explicit_feedback.record_auto_feedback(
                        rule=f"å¿«é€Ÿåˆè¯„ä¸¢å¼ƒ {stats.get('dropped')} æ¡ä½Žåˆ†å†…å®¹",
                        desired_behavior="ä¿ç•™ä¸ŽLLMå·¥ç¨‹å¯†åˆ‡ç›¸å…³ä¸”å¾—åˆ†è¾ƒé«˜çš„æ¡ç›®ã€‚",
                        context=str(stats),
                        correction_type="quick_filter",
                    )
                except Exception:
                    logger.debug("è®°å½•è‡ªåŠ¨åé¦ˆå¤±è´¥ï¼Œå·²å¿½ç•¥")
            return filtered, stats
        except Exception as exc:  # pragma: no cover - defensive fallback
            logger.error("å¿«é€Ÿåˆè¯„å¤±è´¥: %s", exc, exc_info=True)
            return list(items), {
                "input_total": total,
                "kept": total,
                "dropped": 0,
                "avg_score": 8.0,
                "strategy": "exception",
            }

    def _is_release_candidate(self, item: Any) -> bool:
        category = (getattr(item, "category", "") or "").lower()
        if category in {"framework", "model"}:
            return True

        title = (getattr(item, "title", "") or "").lower()
        if "release" in title or title.startswith("v"):
            return True

        source = (getattr(item, "source", "") or "").lower()
        url = (getattr(item, "url", getattr(item, "link", "")) or "").lower()

        # æ£€æŸ¥æ˜¯å¦ä¸º GitHub Releaseï¼ˆé€šè¿‡ URL åˆ¤æ–­ï¼‰
        if "/releases/" in url or "/tag/" in url:
            return True

        return False

    def _should_promote_release(self, item: Any) -> bool:
        if not self._is_release_candidate(item):
            return True

        # è¿‡æ»¤çº¯ç‰ˆæœ¬å·çš„ Releaseï¼ˆå¦‚ b7071, v1.80.0.rc.1, 1.5.0ï¼‰
        title = (getattr(item, "title", "") or "").strip()
        import re
        # åŒ¹é…çº¯ç‰ˆæœ¬å·æ¨¡å¼ï¼šb7071, v1.2.3, 1.2.3, v1.2.3-rc.1 ç­‰
        version_pattern = r'^[bv]?\d+(\.\d+)*(-[a-z]+(\.\d+)?)?$'
        if re.match(version_pattern, title, re.IGNORECASE):
            logger.debug(f"â­ è·³è¿‡çº¯ç‰ˆæœ¬å· Release: {title}")
            return False

        tags = getattr(item, "tags", None) or []
        if any(tag in {"critical_release", "force_release"} for tag in tags):
            return True

        text = " ".join(
            part.lower()
            for part in [getattr(item, "title", ""), getattr(item, "summary", ""), getattr(item, "description", "")]
            if part
        )

        keyword_config = (
            self.user_profile.get("report_generation_rules", {}).get("critical_release_keywords")
            if hasattr(self, "user_profile") else None
        )
        critical_keywords = keyword_config or ["security", "ç´§æ€¥", "critical", "æ¼æ´ž", "cve", "é‡å¤§", "breaking"]

        if any(keyword in text for keyword in critical_keywords):
            return True

        return False

    def _expand_long_articles(self, items: list) -> list:
        """Split very long summaries into smaller chunks so the batch prompt stays within limits."""
        import re
        from dataclasses import replace

        expanded: List[Any] = []
        max_chars = int(os.getenv('LONG_ARTICLE_MAX_CHARS', '1600'))
        overlap = int(os.getenv('LONG_ARTICLE_OVERLAP', '200'))

        for item in items:
            summary = self._extract_attribute(item, ['summary', 'description']) or ''
            if len(summary) <= max_chars:
                expanded.append(item)
                continue

            sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ!?\.])\s+', summary)
            chunks: List[str] = []
            current = ''
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                candidate = f"{current} {sentence}".strip() if current else sentence
                if len(candidate) <= max_chars:
                    current = candidate
                else:
                    if current:
                        chunks.append(current.strip())
                    current = sentence
            if current:
                chunks.append(current.strip())

            if not chunks:
                expanded.append(item)
                continue

            merged: List[str] = []
            for chunk in chunks:
                if not merged:
                    merged.append(chunk)
                    continue
                if len(chunk) < overlap:
                    merged[-1] = f"{merged[-1]} {chunk}".strip()
                else:
                    merged.append(chunk)

            original_title = self._extract_attribute(item, ['title', 'name']) or 'Long Article'
            for part_idx, chunk in enumerate(merged, start=1):
                new_title = f"{original_title} [Part {part_idx}]" if len(merged) > 1 else original_title
                try:
                    if hasattr(item, '__dataclass_fields__'):
                        new_item = replace(item, summary=chunk)
                        if hasattr(new_item, 'content'):
                            setattr(new_item, 'content', chunk)
                        if hasattr(new_item, 'title'):
                            setattr(new_item, 'title', new_title)
                    elif isinstance(item, dict):
                        new_item = dict(item)
                        new_item['summary'] = chunk
                        new_item['title'] = new_title
                    else:
                        new_item = item
                except Exception:
                    new_item = item
                expanded.append(new_item)

        return expanded
    
    def _process_with_ai(self, items: list) -> list:
        """AIå¤„ç†é˜¶æ®µ - ä½¿ç”¨æ‰¹é‡å¤„ç†ä¼˜åŒ–"""
        try:
            api_key = os.getenv('POE_API_KEY')
            if not api_key:
                logger.error("æœªæ‰¾åˆ°POE_API_KEYçŽ¯å¢ƒå˜é‡")
                return []
            
            # ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–æ¨¡åž‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨Sonnet 4.5
            model = os.getenv('DEVELOPER_MODEL', 'Claude-Sonnet-4.5')
            
            # ä½¿ç”¨æ‰¹é‡å¤„ç†å™¨ï¼ˆæ–°æ–¹æ¡ˆï¼š1æ¬¡APIè°ƒç”¨ä»£æ›¿158æ¬¡ï¼‰
            batch_processor = AIProcessorBatch(
                api_key=api_key,
                model_name=model,
                user_profile=self.user_profile,
                explicit_feedback_manager=self.explicit_feedback,
            )
            
            logger.info(f"ðŸš€ æ‰¹é‡AIå¤„ç†æ¨¡å¼: {len(items)} æ¡ â†’ ç­›é€‰ Top 60")
            logger.info(f"ðŸ“‹ ä½¿ç”¨æ¨¡åž‹: {model}")
            logger.info("ï¼ˆ1æ¬¡APIè°ƒç”¨ï¼Œé¢„è®¡2-3åˆ†é’Ÿï¼‰")

            expanded_items = self._expand_long_articles(items)
            if len(expanded_items) != len(items):
                logger.info("ðŸ§µ é•¿æ–‡åˆ†æ®µ: %s â†’ %s", len(items), len(expanded_items))

            if not expanded_items:
                logger.warning("é•¿æ–‡åˆ†æ®µåŽæ²¡æœ‰å¯å¤„ç†çš„æ¡ç›®")
                return []

            # é€‰é¡¹1: å¢žåŠ AIå¤„ç†æ•°é‡åˆ°60æ¡ï¼ˆç¡®ä¿è¦†ç›–è®ºæ–‡ï¼‰
            top_n = min(60, len(expanded_items))
            processed_items = batch_processor.batch_select_and_analyze(
                all_items=expanded_items,
                top_n=top_n
            )

            release_debug = []
            for processed_item in processed_items:
                is_release = self._is_release_candidate(processed_item)
                promote_release = self._should_promote_release(processed_item)
                setattr(processed_item, "is_release", is_release)
                setattr(processed_item, "promote_release", promote_release)
                if is_release:
                    release_debug.append(f"{processed_item.title}=>{promote_release}")

            if release_debug:
                logger.info("ðŸ§® Releaseè¿‡æ»¤: %s", ", ".join(release_debug))

            self._log_ab_metric(processed_items)
            
            # æ˜¾ç¤ºç›¸å…³æ€§åˆ†å¸ƒ
            high_relevance = len([i for i in processed_items if i.relevance_score >= 8])
            medium_relevance = len([i for i in processed_items if 5 <= i.relevance_score < 8])
            low_relevance = len([i for i in processed_items if i.relevance_score < 5])
            
            logger.info(f"\nðŸ“Š ç›¸å…³æ€§åˆ†å¸ƒ:")
            logger.info(f"  - é«˜ç›¸å…³ (â‰¥8åˆ†): {high_relevance} æ¡")
            logger.info(f"  - ä¸­ç›¸å…³ (5-7åˆ†): {medium_relevance} æ¡")
            logger.info(f"  - ä½Žç›¸å…³ (<5åˆ†): {low_relevance} æ¡")
            
            # æ˜¾ç¤ºåˆ†ç±»åˆ†å¸ƒ
            from collections import Counter
            category_dist = Counter([i.category for i in processed_items])
            logger.info(f"\nðŸ“‚ åˆ†ç±»åˆ†å¸ƒ:")
            for category, count in category_dist.most_common():
                logger.info(f"  - {category}: {count} æ¡")
            
            return processed_items
            
        except Exception as e:
            logger.error(f"æ‰¹é‡AIå¤„ç†å¤±è´¥: {str(e)}")
            logger.info("å°è¯•é™çº§åˆ°ä¼ ç»Ÿå¤„ç†æ¨¡å¼...")
            
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿé€æ¡å¤„ç†ï¼ˆé€‰é¡¹1+2ï¼šå¤„ç†60æ¡ï¼Œè®ºæ–‡ä¼˜å…ˆï¼‰
            try:
                self.ai_processor = AIProcessor(
                    api_key=api_key,
                    user_profile=self.user_profile,
                    model=model,
                    explicit_feedback_manager=self.explicit_feedback,
                )
                logger.info("âš ï¸ æ‰¹é‡æ¨¡å¼å¤±è´¥ï¼Œåˆ‡æ¢è‡³ä¼ ç»Ÿå¤„ç†ï¼ˆTop 60ï¼Œè®ºæ–‡ä¼˜å…ˆï¼‰...")

                # é€‰é¡¹1: æ‰©å¤§å¤„ç†æ•°é‡åˆ°60æ¡
                fallback_pool = (
                    expanded_items if 'expanded_items' in locals() and expanded_items else items
                )
                top_n_fallback = min(60, len(fallback_pool))

                # é€‰é¡¹2: ä¸ºè®ºæ–‡ç±»åˆ«å•ç‹¬å¤„ç†ï¼Œç¡®ä¿è‡³å°‘15ç¯‡è®ºæ–‡
                paper_items = []
                news_items = []
                for item in fallback_pool:
                    category = getattr(item, 'category', item.get('category', '') if hasattr(item, 'get') else '')
                    if category == 'paper':
                        paper_items.append(item)
                    else:
                        news_items.append(item)

                # è®ºæ–‡å†…éƒ¨ä¼˜å…ˆçº§ï¼šHugging Face Papers > Papers with Code > arXiv
                def get_paper_priority(item):
                    source = getattr(item, 'source', item.get('source', '') if hasattr(item, 'get') else '').lower()
                    if 'hugging face' in source:
                        return 3
                    elif 'papers with code' in source:
                        return 2
                    elif 'arxiv' in source:
                        return 1
                    else:
                        return 0
                
                # æŒ‰æ¥æºä¼˜å…ˆçº§æŽ’åºè®ºæ–‡
                paper_items.sort(key=get_paper_priority, reverse=True)

                paper_quota = min(15, len(paper_items))
                news_quota = top_n_fallback - paper_quota
                prioritized_items = paper_items[:paper_quota] + news_items[:news_quota]

                # ç»Ÿè®¡è®ºæ–‡æ¥æº
                paper_sources = {}
                for p in paper_items[:paper_quota]:
                    source = getattr(p, 'source', 'Unknown')
                    paper_sources[source] = paper_sources.get(source, 0) + 1

                logger.info(
                    "ðŸ“„ ä¼ ç»Ÿæ¨¡å¼ä¼˜å…ˆå¤„ç†: %s ç¯‡è®ºæ–‡ + %s æ¡æ–°é—»",
                    len(paper_items[:paper_quota]),
                    len(news_items[:news_quota]),
                )
                if paper_sources:
                    logger.info(f"  è®ºæ–‡æ¥æº: {', '.join([f'{k}: {v}' for k, v in sorted(paper_sources.items(), key=lambda x: x[1], reverse=True)])}")

                processed_fallback = self.ai_processor.process_batch(prioritized_items)

                for processed_item in processed_fallback:
                    is_release = self._is_release_candidate(processed_item)
                    promote_release = self._should_promote_release(processed_item)
                    setattr(processed_item, "is_release", is_release)
                    setattr(processed_item, "promote_release", promote_release)

                self._log_ab_metric(processed_fallback)
                return processed_fallback
            except Exception as fallback_error:
                logger.error(f"é™çº§å¤„ç†ä¹Ÿå¤±è´¥: {str(fallback_error)}")
                return []
    
    def _generate_action_items(self, processed_items: list, use_agent: bool = True) -> dict:
        """ç”Ÿæˆè¡ŒåŠ¨æ¸…å•ï¼ˆåŽ»é‡ï¼šæŽ’é™¤å·²åœ¨å¿…çœ‹å†…å®¹ä¸­çš„æ–°é—»ï¼‰"""
        try:
            # Phase 2.1: å¦‚æžœå¯ç”¨ Agentï¼Œä½¿ç”¨ ActionAgent ç”Ÿæˆæ™ºèƒ½å»ºè®®
            if use_agent:
                try:
                    logger.info("ðŸ¤– ä½¿ç”¨ ActionAgent ç”Ÿæˆæ™ºèƒ½è¡ŒåŠ¨å»ºè®®...")
                    
                    # åˆå§‹åŒ– ActionAgent
                    tool_config = self._load_tool_config()
                    tool_executor = ToolExecutor(config=tool_config)
                    action_agent = ActionAgent(tool_executor=tool_executor)
                    
                    # é€‰æ‹©é«˜ä¼˜å…ˆçº§é¡¹ç›®è¿›è¡Œåˆ†æž
                    high_priority_items = [
                        item for item in processed_items
                        if getattr(item, 'relevance_score', 0) >= 7
                        and getattr(item, 'actionable', False)
                    ][:10]  # æœ€å¤šåˆ†æž 10 æ¡
                    
                    if high_priority_items:
                        # ç”Ÿæˆè¡ŒåŠ¨å»ºè®®
                        suggestions = action_agent.generate_action_suggestions(
                            high_priority_items,
                            max_suggestions=5,
                        )
                        
                        if suggestions:
                            logger.info(f"âœ“ ActionAgent ç”Ÿæˆäº† {len(suggestions)} ä¸ªè¡ŒåŠ¨å»ºè®®")
                            
                            # è½¬æ¢ä¸ºçŽ°æœ‰æ ¼å¼
                            must_do = []
                            nice_to_have = []
                            
                            for suggestion in suggestions:
                                action_item = {
                                    'title': suggestion.get('title', ''),
                                    'action': suggestion.get('description', ''),
                                    'type': suggestion.get('type', 'other'),
                                    'executed': suggestion.get('executed', False),
                                    'result': suggestion.get('result', {}),
                                    'tool_call': suggestion.get('tool_call'),  # ä¿ç•™å·¥å…·è°ƒç”¨ä¿¡æ¯
                                    'url': suggestion.get('result', {}).get('data', {}).get('url', ''),
                                }
                                
                                # æ ¹æ®æ‰§è¡ŒçŠ¶æ€åˆ†ç±»
                                if suggestion.get('executed'):
                                    must_do.append(action_item)
                                else:
                                    nice_to_have.append(action_item)
                            
                            return {
                                'must_do': must_do[:5],
                                'nice_to_have': nice_to_have[:5],
                                'agent_generated': True,
                            }
                except Exception as e:
                    logger.warning(f"âš ï¸  ActionAgent ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
                    # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
            
            # ä¼ ç»Ÿæ–¹æ³•ï¼šä»Žå¤„ç†ç»“æžœä¸­æå–actionable items
            filtering_prefs = self.filtering_preferences
            ignore_keywords = [
                keyword.lower() for keyword in filtering_prefs.get("ignore_keywords", [])
            ]
            minimum_optional_score = filtering_prefs.get("minimum_optional_score", 6)

            ab_variant = self.ab_variants.get("scoring_threshold_v1")
            if ab_variant == "treatment":
                minimum_optional_score = max(minimum_optional_score, 7)
            else:
                minimum_optional_score = max(minimum_optional_score, 6)

            # ðŸ”‘ å…³é”®æ”¹è¿›ï¼šå…ˆè¯†åˆ«å‡º"å¿…çœ‹å†…å®¹"ï¼ˆpersonal_priority >= 8ï¼‰
            must_read_urls = set()
            for item in processed_items:
                if getattr(item, 'personal_priority', 0) >= 8:  # é™ä½Žé˜ˆå€¼ä»Ž9åˆ°8
                    url = getattr(item, 'url', getattr(item, 'link', ''))
                    if url:
                        must_read_urls.add(url)
            
            logger.info(f"ðŸ“Œ è¯†åˆ«åˆ° {len(must_read_urls)} æ¡å¿…çœ‹å†…å®¹ï¼Œå°†ä»Žå»ºè®®è¡ŒåŠ¨ä¸­æŽ’é™¤")

            must_do = []
            nice_to_have = []
            
            for item in processed_items:
                # è·³è¿‡å·²ç»åœ¨"å¿…çœ‹å†…å®¹"ä¸­çš„æ–°é—»
                url = getattr(item, 'url', getattr(item, 'link', ''))
                if url in must_read_urls:
                    logger.debug(f"è·³è¿‡é‡å¤æ–°é—»ï¼ˆå·²åœ¨å¿…çœ‹å†…å®¹ï¼‰: {getattr(item, 'title', '')}")
                    continue

                if self._is_release_candidate(item) and not self._should_promote_release(item):
                    logger.debug(f"è·³è¿‡æ™®é€šç‰ˆæœ¬æ›´æ–°: {getattr(item, 'title', '')}")
                    continue
 
                title = getattr(item, 'title', '') or ""
                title_lower = title.lower()
                if any(keyword in title_lower for keyword in ignore_keywords):
                    logger.debug(f"è¿‡æ»¤æŽ‰ä½Žä»·å€¼å†…å®¹: {title}")
                    continue

                if not getattr(item, 'actionable', False):
                    continue

                impact_text = getattr(item, 'impact_analysis', '') or getattr(item, 'why_matters_to_you', '') or ""

                relevance_score = getattr(item, 'relevance_score', None)
                if relevance_score is None:
                    continue

                relevance_score = int(relevance_score) if relevance_score else 0
                if relevance_score >= 8:
                    must_do.append({
                        'title': getattr(item, 'title', ''),
                        'action': impact_text,
                        'source': getattr(item, 'source', ''),
                        'url': getattr(item, 'url', getattr(item, 'link', ''))
                    })
                elif relevance_score >= minimum_optional_score:
                    nice_to_have.append({
                        'title': getattr(item, 'title', ''),
                        'action': impact_text,
                        'source': getattr(item, 'source', ''),
                        'url': getattr(item, 'url', getattr(item, 'link', ''))
                    })
            
            action_items = {
                'must_do': must_do[:5],  # æœ€å¤š5é¡¹
                'nice_to_have': nice_to_have[:5]  # æœ€å¤š5é¡¹
            }
            
            logger.info(f"\nðŸ“‹ è¡ŒåŠ¨æ¸…å•:")
            logger.info(f"  - å¿…åšä»»åŠ¡: {len(action_items['must_do'])} é¡¹")
            logger.info(f"  - å¯é€‰ä»»åŠ¡: {len(action_items['nice_to_have'])} é¡¹")
            
            return action_items
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¡ŒåŠ¨æ¸…å•å¤±è´¥: {str(e)}")
            return {'must_do': [], 'nice_to_have': []}
    
    def _load_tool_config(self) -> dict:
        """åŠ è½½å·¥å…·é…ç½®"""
        tool_config = {}
        
        # GitHub é…ç½®
        github_token = os.getenv("GITHUB_TOKEN")
        github_repo = os.getenv("GITHUB_DEFAULT_REPO", "")
        if github_token or github_repo:
            tool_config["github"] = {
                "token": github_token,
                "default_repo": github_repo,
            }
        
        # æ—¥åŽ†é…ç½®
        calendar_email = os.getenv("CALENDAR_EMAIL", "")
        if calendar_email:
            tool_config["calendar"] = {
                "email": calendar_email,
            }
        
        # é˜…è¯»åˆ—è¡¨é…ç½®
        reading_list_integration = os.getenv("READING_LIST_INTEGRATION", "local")
        tool_config["reading_list"] = {
            "integration": reading_list_integration,
            "reading_list_path": str(project_root / "data" / "reading_list.json"),
        }
        
        return tool_config
    
    def _generate_report(
        self,
        processed_items: list,
        action_items: dict,
        leaderboard_info: dict,
        market_insights: list,
        output_dir: Optional[str] = None,
        learning_results: Optional[dict] = None,
    ) -> str:
        """ç”ŸæˆæŠ¥å‘Š"""
        try:
            self.report_generator = ReportGenerator(
                headline_source_limit=self.headline_source_limit
            )
            
            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if output_dir is None:
                output_dir = str(project_root / "output")
            
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            date_str = datetime.now().strftime('%Y-%m-%d')
            output_path = os.path.join(output_dir, f"weekly_report_{date_str}.md")
            
            # ç”ŸæˆæŠ¥å‘ŠID
            report_id = f"report_{date_str}"
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            logger.info(f"ðŸ“ ç”ŸæˆMarkdownæŠ¥å‘Š...")
            report = self.report_generator.generate_report(
                processed_items=processed_items,
                action_items=action_items,
                leaderboard_data=leaderboard_info.get('data', []),
                leaderboard_update_time=leaderboard_info.get('update_time', ''),
                market_insights=market_insights,
                output_path=output_path,
                learning_results=learning_results or {},
            )
            
            # ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆå¸¦è¯„åˆ†åŠŸèƒ½ï¼‰
            logger.info(f"ðŸŒ ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆå¸¦è¯„åˆ†åŠŸèƒ½ï¼‰...")
            html_path = output_path.replace('.md', '.html')
            html_report = self.report_generator.generate_html_report(
                processed_items=processed_items,
                action_items=action_items,
                leaderboard_data=leaderboard_info.get('data', []),
                leaderboard_update_time=leaderboard_info.get('update_time', ''),
                market_insights=market_insights,
                output_path=output_path,
                learning_results=learning_results or {},
                report_id=report_id,
            )
            
            # æ˜¾ç¤ºç»Ÿè®¡
            logger.info(f"\nðŸ“„ æŠ¥å‘Šç»Ÿè®¡:")
            logger.info(f"  - æ€»å­—æ•°: {len(report)} å­—ç¬¦")
            logger.info(f"  - Markdownè·¯å¾„: {output_path}")
            logger.info(f"  - HTMLè·¯å¾„: {html_path}")
            logger.info(f"  - æŠ¥å‘ŠID: {report_id}")
            logger.info(f"\nðŸ’¡ æç¤º: æ‰“å¼€HTMLæ–‡ä»¶å¯ä»¥è¯„åˆ†å’Œè¿½è¸ªé˜…è¯»è¡Œä¸º")
            logger.info(f"   å¯åŠ¨è¿½è¸ªæœåŠ¡å™¨: python src/tracking/tracking_server.py")

            self._sync_report_to_notion(
                date_str=date_str,
                markdown_content=report,
                markdown_path=output_path,
                html_path=html_path,
            )
            
            return output_path
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            raise

    def _sync_report_to_notion(
        self,
        date_str: str,
        markdown_content: str,
        markdown_path: str,
        html_path: str,
    ) -> None:
        """Publish the report to Notion if integration is enabled."""
        if not getattr(self, "notion_sync", None) or not self.notion_sync.is_enabled:
            logger.debug("Notion åŒæ­¥æœªå¯ç”¨ï¼Œè·³è¿‡ã€‚")
            return

        metadata = {
            "report_date": date_str,
            "markdown_path": markdown_path,
            "html_path": html_path,
            "total_chars": str(len(markdown_content)),
        }

        title = build_notion_title(date_str)
        logger.info("ðŸ—‚ï¸ åŒæ­¥æŠ¥å‘Šåˆ° Notionï¼š%s", title)
        success = self.notion_sync.sync_report(
            title=title,
            markdown_content=markdown_content,
            metadata=metadata,
        )
        if not success:
            logger.warning("âš ï¸ Notion åŒæ­¥æœªæˆåŠŸï¼Œå·²è·³è¿‡ã€‚")

    def _run_learning_cycle(self, processed_items: list) -> dict:
        """è¿è¡Œè‡ªæˆ‘å­¦ä¹ å¾ªçŽ¯"""
        try:
            logger.info("\nðŸ§  è¿è¡Œè‡ªæˆ‘å­¦ä¹ å¾ªçŽ¯...")
            is_weekly = self._is_weekly_report_day()
            learning_results = self.learning_engine.run_cycle(processed_items, is_weekly=is_weekly)
            
            # Phase 2.3: è¿è¡Œåé¦ˆå­¦ä¹ 
            try:
                logger.info("ðŸ”„ è¿è¡Œåé¦ˆé—­çŽ¯ä¼˜åŒ–...")
                feedback_engine = FeedbackLearningEngine(
                    db=self.learning_engine.db,
                    weight_adjuster=self.report_generator.weight_adjuster if hasattr(self, 'report_generator') else None,
                )
                
                # åˆ†æžåé¦ˆæ¨¡å¼
                feedback_patterns = feedback_engine.analyze_feedback_patterns(days=7)
                
                # å¼ºåŒ–æƒé‡
                reinforce_result = feedback_engine.reinforce_weights(days=7)
                
                # èŽ·å–å¯æ“ä½œæ€§æŒ‡æ ‡
                actionability_metrics = feedback_engine.get_actionability_metrics(days=7)
                
                # æ·»åŠ åˆ°å­¦ä¹ ç»“æžœ
                learning_results.setdefault('feedback_learning', {})
                learning_results['feedback_learning'] = {
                    'patterns': feedback_patterns,
                    'reinforcements': reinforce_result,
                    'actionability': actionability_metrics,
                }
                
                logger.info("âœ“ åé¦ˆé—­çŽ¯ä¼˜åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"âš ï¸  åé¦ˆå­¦ä¹ å¤±è´¥: {e}")
            
            logger.info("âœ“ è‡ªæˆ‘å­¦ä¹ å¾ªçŽ¯å®Œæˆã€‚")
            return learning_results
        except Exception as e:
            logger.error(f"è‡ªæˆ‘å­¦ä¹ å¾ªçŽ¯å¤±è´¥: {str(e)}")
            return {
                "auto_applied": [],
                "require_review": [],
                "insights": [],
                "priority_adjustments": [],
                "discovery": {},
                "models": {},
                "weekly_summary": None,
                "is_weekly": False,
            }

    def _log_learning_summary(self, learning_results: dict) -> None:
        """è®°å½•å­¦ä¹ å¼•æ“Žçš„æ€»ç»“"""
        if not learning_results:
            logger.info("\nðŸ“š å­¦ä¹ å¼•æ“Žæœªè¿è¡Œæˆ–æ— ç»“æžœã€‚")
            return

        auto_applied = learning_results.get("auto_applied", [])
        require_review = learning_results.get("require_review", [])
        discovery = learning_results.get("discovery", {})
        models = learning_results.get("models", {})

        logger.info("\nðŸ“š å­¦ä¹ å¼•æ“Žæ€»ç»“:")
        logger.info(f"  - è‡ªåŠ¨åº”ç”¨ä¼˜åŒ–: {len(auto_applied)} é¡¹")
        logger.info(f"  - å¾…å®¡æŸ¥å»ºè®®: {len(require_review)} é¡¹")
        if discovery:
            logger.info(
                "  - æ–°ä¿¡æ¯æºè¯„ä¼°: %s ä¸ªå€™é€‰ (è‡ªåŠ¨æ·»åŠ  %s ä¸ª)",
                discovery.get("evaluated", 0),
                len(discovery.get("auto_add_candidates", [])),
            )
        if models:
            logger.info(
                "  - æ–°æ¨¡åž‹è¯„ä¼°: %s ä¸ª (é‡ç‚¹å…³æ³¨ %s ä¸ª)",
                models.get("evaluated", 0),
                len(models.get("flagged", [])),
            )

        if learning_results.get("weekly_summary"):
            summary = learning_results["weekly_summary"]
            logger.info(
                "  - æœ¬å‘¨æ–°å¢žä¿¡æ¯æº: %s ä¸ªï¼Œç§»é™¤ä¿¡æ¯æº: %s ä¸ª",
                len(summary.get("sources_added", [])),
                len(summary.get("sources_removed", [])),
            )

    def _is_weekly_report_day(self) -> bool:
        weekly_cfg = (self.learning_config or {}).get("weekly_summary", {})
        target_day = weekly_cfg.get("day_of_week", 0)
        try:
            target_day = int(target_day)
        except (TypeError, ValueError):
            target_day = 0
        return datetime.now().weekday() == target_day

    # ------------------------------------------------------------------
    # CLI helpers
    # ------------------------------------------------------------------
    def list_recommendations(self) -> None:
        threshold = (self.learning_config.get("source_discovery", {}) or {}).get(
            "min_quality_for_recommendation", 7.0
        )
        candidates = self.learning_engine.db.get_pending_sources(threshold)
        if not candidates:
            print("æš‚æ— å¾…å®¡æ‰¹çš„æ–°å¢žä¿¡æ¯æºã€‚")
            return

        print("å¾…å®¡æ‰¹ä¿¡æ¯æºåˆ—è¡¨:\n")
        for idx, candidate in enumerate(candidates, 1):
            print(
                f"{idx}. {candidate.get('name') or candidate.get('url')}"
                f" | ç±»åž‹: {candidate.get('type')}"
                f" | è´¨é‡: {candidate.get('quality_score', '?')}/10"
                f" | é“¾æŽ¥: {candidate.get('url')}"
            )

    def apply_recommendation(self, identifier: str) -> None:
        candidates = self.learning_engine.db.get_pending_sources(0)
        target = self._find_candidate(candidates, identifier)
        if not target:
            logger.error("æœªæ‰¾åˆ°åŒ¹é…çš„å€™é€‰ä¿¡æ¯æºï¼š%s", identifier)
            return

        if self.config_manager.add_source(target):
            self.config_manager.save()
            logger.info("å·²æ›´æ–°sources.yamlï¼Œæ–°å¢žä¿¡æ¯æºï¼š%s", target.get("name") or target.get("url"))
        else:
            logger.info("ä¿¡æ¯æºå·²å­˜åœ¨äºŽé…ç½®ä¸­ï¼Œè·³è¿‡å†™å…¥ã€‚")

        self.learning_engine.db.update_discovered_source_status(target.get("url"), "approved")
        self.learning_engine.db.log_optimization(
            OptimizationRecord(
                optimization_type="add_source_manual",
                target=target.get("url"),
                details={"name": target.get("name"), "type": target.get("type")},
            )
        )

    def reject_recommendation(self, identifier: str) -> None:
        candidates = self.learning_engine.db.get_pending_sources(0)
        target = self._find_candidate(candidates, identifier)
        if not target:
            logger.error("æœªæ‰¾åˆ°åŒ¹é…çš„å€™é€‰ä¿¡æ¯æºï¼š%s", identifier)
            return
        self.learning_engine.db.update_discovered_source_status(target.get("url"), "rejected")
        logger.info("å·²æ‹’ç»ä¿¡æ¯æºï¼š%s", target.get("name") or target.get("url"))

    def print_learning_summary(self) -> None:
        summary = self.learning_engine.generate_weekly_summary()
        print("å­¦ä¹ å¼•æ“Žæ‘˜è¦ï¼š")
        print(f"- æœ¬å‘¨æ–°å¢žä¿¡æ¯æºï¼š{len(summary.get('sources_added', []))}")
        print(f"- æœ¬å‘¨åœç”¨ä¿¡æ¯æºï¼š{len(summary.get('sources_removed', []))}")
        print(f"- æœ¬å‘¨æ¨¡åž‹è¯„ä¼°ï¼š{len(summary.get('models_evaluated', []))}\n")

    def _find_candidate(self, candidates: list, identifier: str) -> Optional[dict]:
        slug = self._slugify(identifier)
        for candidate in candidates:
            url = candidate.get("url", "")
            name = candidate.get("name", "")
            if identifier == url or identifier == name:
                return candidate
            if self._slugify(url) == slug or self._slugify(name) == slug:
                return candidate
        return None

    def _slugify(self, value: str) -> str:
        return "".join(ch.lower() if ch.isalnum() else "-" for ch in value or "").strip("-")

    def _log_ab_metric(self, processed_items: list) -> None:
        if not processed_items or not getattr(self, "ab_variants", None):
            return

        try:
            engagement_score = sum(
                getattr(item, "personal_priority", 0) for item in processed_items
            ) / len(processed_items)
        except Exception:
            engagement_score = 0.0

        for exp_id, experiment in self.ab_experiments.items():
            variant = self.ab_variants.get(exp_id)
            if not variant:
                continue
            self.ab_tester.log_metric(
                experiment,
                variant,
                engagement_score,
            )

    def _load_email_settings(self) -> Dict[str, Any]:
        recipients = os.getenv("DIGEST_EMAIL_TO") or "davidzheng0119@163.com"
        parsed_recipients = [addr.strip() for addr in recipients.split(",") if addr.strip()]

        settings = {
            "recipients": parsed_recipients,
            "smtp_host": os.getenv("DIGEST_SMTP_HOST"),
            "smtp_port": int(os.getenv("DIGEST_SMTP_PORT", "465")),
            "smtp_user": os.getenv("DIGEST_SMTP_USER"),
            "smtp_pass": os.getenv("DIGEST_SMTP_PASS"),
            "sender": os.getenv("DIGEST_EMAIL_FROM"),
        }

        required_fields = ("smtp_host", "smtp_user", "smtp_pass")
        settings["enabled"] = all(settings.get(field) for field in required_fields)
        return settings

    def _compose_email_body(self, report_path: Path) -> str:
        text = report_path.read_text(encoding="utf-8")
        snippet_limit = int(os.getenv("DIGEST_EMAIL_BODY_LIMIT", "8000"))
        if len(text) > snippet_limit:
            return text[:snippet_limit] + "\n\nï¼ˆå†…å®¹è¾ƒé•¿ï¼Œå®Œæ•´å†…å®¹è§é™„ä»¶ï¼‰"
        return text

    def _send_email_if_configured(self, report_path: Optional[str]) -> None:
        if not report_path:
            return

        settings = self.email_settings
        if not settings.get("enabled"):
            logger.info("é‚®ä»¶é€šçŸ¥æœªå¯ç”¨ï¼Œç¼ºå°‘SMTPé…ç½®ï¼Œå·²è·³è¿‡å‘é€ã€‚")
            return

        report_path = Path(report_path)
        try:
            subject = f"AI æƒ…æŠ¥ç®€æŠ¥ | {datetime.now().strftime('%Y-%m-%d')}"
            body = self._compose_email_body(report_path)
            send_digest_email(
                report_path,
                subject,
                settings.get("recipients", []),
                smtp_host=settings["smtp_host"],
                smtp_port=settings.get("smtp_port", 465),
                smtp_user=settings["smtp_user"],
                smtp_password=settings["smtp_pass"],
                sender=settings.get("sender"),
                body_text=body,
            )
        except Exception as e:
            logger.error("å‘é€ç®€æŠ¥é‚®ä»¶å¤±è´¥: %s", e)


def timeout_handler(signum, frame):
    """è¶…æ—¶ä¿¡å·å¤„ç†å™¨"""
    raise TimeoutError("æ‰§è¡Œè¶…æ—¶ï¼šä¸»æµç¨‹è¿è¡Œæ—¶é—´è¶…è¿‡é™åˆ¶")


def main():
    """ä¸»å‡½æ•°ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰"""
    parser = argparse.ArgumentParser(description="AI Digest Report Generator")
    parser.add_argument("--days-back", type=int, default=None, help="é‡‡é›†æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆé»˜è®¤ï¼šå‘¨ä¸€3å¤©ï¼Œå…¶ä»–2å¤©ï¼‰")
    parser.add_argument("--list-recommendations", action="store_true", help="åˆ—å‡ºå¾…å®¡æ‰¹çš„ä¿¡æ¯æº")
    parser.add_argument("--apply-recommendation", help="æ‰¹å‡†å¹¶åŠ å…¥é…ç½®çš„æ–°ä¿¡æ¯æºï¼ˆè¾“å…¥URLæˆ–åç§°ï¼‰")
    parser.add_argument("--reject-recommendation", help="æ‹’ç»å€™é€‰ä¿¡æ¯æºï¼ˆè¾“å…¥URLæˆ–åç§°ï¼‰")
    parser.add_argument("--learning-summary", action="store_true", help="æ‰“å°å­¦ä¹ å¼•æ“Žçš„å‘¨åº¦æ‘˜è¦")
    parser.add_argument("--learning-only", action="store_true", help="ä»…è¿è¡Œå­¦ä¹ å¾ªçŽ¯ï¼Œè·³è¿‡å‘¨æŠ¥ç”Ÿæˆ")
    parser.add_argument("--use-langgraph", action="store_true", help="[å·²åºŸå¼ƒ] é»˜è®¤å·²ä½¿ç”¨ LangGraphï¼Œæ­¤å‚æ•°å·²æ— æ•ˆæžœ")
    parser.add_argument("--ab-summary", action="store_true", help="è¾“å‡ºå½“å‰ABæµ‹è¯•ç»Ÿè®¡æ‘˜è¦")
    parser.add_argument("--timeout", type=int, default=600, help="ä¸»æµç¨‹æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰")
    args = parser.parse_args()
    
    # è‡ªåŠ¨åˆ¤æ–­ days_backï¼šå¦‚æžœæœªæŒ‡å®šï¼Œåˆ™å‘¨ä¸€ä½¿ç”¨3å¤©ï¼Œå…¶ä»–æ—¶é—´ä½¿ç”¨2å¤©
    if args.days_back is None:
        today = datetime.now()
        # weekday(): 0=å‘¨ä¸€, 1=å‘¨äºŒ, ..., 6=å‘¨æ—¥
        if today.weekday() == 0:  # å‘¨ä¸€
            args.days_back = 3
            logger.info("ðŸ“… ä»Šå¤©æ˜¯å‘¨ä¸€ï¼Œè‡ªåŠ¨è®¾ç½®æ‰«æè¿‡åŽ» 3 å¤©çš„å†…å®¹")
        else:
            args.days_back = 2
            logger.info(f"ðŸ“… ä»Šå¤©æ˜¯{['å‘¨ä¸€','å‘¨äºŒ','å‘¨ä¸‰','å‘¨å››','å‘¨äº”','å‘¨å…­','å‘¨æ—¥'][today.weekday()]}ï¼Œè‡ªåŠ¨è®¾ç½®æ‰«æè¿‡åŽ» 2 å¤©çš„å†…å®¹")
    else:
        logger.info(f"ðŸ“… æ‰‹åŠ¨æŒ‡å®šæ‰«æè¿‡åŽ» {args.days_back} å¤©çš„å†…å®¹")
    
    # è®¾ç½®ä¸»æµç¨‹è¶…æ—¶ä¿æŠ¤ï¼ˆä»…Unixç³»ç»Ÿï¼‰
    if hasattr(signal, 'SIGALRM') and args.timeout > 0:
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(args.timeout)
        logger.info(f"â± å·²è®¾ç½®ä¸»æµç¨‹è¶…æ—¶ä¿æŠ¤: {args.timeout}ç§’")

    try:
        generator = WeeklyReportGenerator()
        
        if args.list_recommendations:
            generator.list_recommendations()
            return
        if args.apply_recommendation:
            generator.apply_recommendation(args.apply_recommendation)
            return
        if args.reject_recommendation:
            generator.reject_recommendation(args.reject_recommendation)
            return
        if args.learning_summary:
            generator.print_learning_summary()
            return
        if args.ab_summary:
            generator.print_ab_summary()
            return

        if args.use_langgraph:
            logger.warning("--use-langgraph å‚æ•°å·²åºŸå¼ƒï¼Œé»˜è®¤ run() æ–¹æ³•å·²ä½¿ç”¨ LangGraphã€‚")
            # ä¿æŒå‘åŽå…¼å®¹ï¼Œä½†ä½¿ç”¨ç»Ÿä¸€çš„ run() æ–¹æ³•
            generator.run(days_back=args.days_back, learning_only=args.learning_only)
        else:
            # é»˜è®¤ä½¿ç”¨ LangGraph å·¥ä½œæµ
            generator.run(days_back=args.days_back, learning_only=args.learning_only)
        
        # å–æ¶ˆè¶…æ—¶alarm
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
        
    except TimeoutError as e:
        logger.error(f"âŒ {str(e)}")
        logger.error("å»ºè®®ï¼š")
        logger.error("  1. æ£€æŸ¥ç½‘ç»œè¿žæŽ¥")
        logger.error("  2. ä½¿ç”¨ --timeout å‚æ•°å¢žåŠ è¶…æ—¶æ—¶é—´")
        logger.error("  3. æ£€æŸ¥å¡ä½çš„æ•°æ®æºï¼ˆæŸ¥çœ‹æ—¥å¿—ä¸­æœ€åŽå¤„ç†çš„æºï¼‰")
        sys.exit(1)
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
        sys.exit(1)


if __name__ == "__main__":
    main()

