"""
AIæ‰¹é‡å¤„ç†å™¨ - ä¸€æ¬¡æ€§ç­›é€‰å’Œåˆ†ææ‰€æœ‰æ–°é—»
æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼š1æ¬¡APIè°ƒç”¨ä»£æ›¿158æ¬¡
"""

import asyncio
import json
import logging
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass, field

try:
    from json_repair import repair_json
except ImportError:  # pragma: no cover
    repair_json = None

from fastapi_poe import get_bot_response

from src.learning.explicit_feedback import ExplicitFeedbackManager, FewShotExample

logger = logging.getLogger(__name__)


@dataclass
class ProcessedItem:
    """å¤„ç†åçš„æ¡ç›®"""
    source: str
    title: str
    url: str
    published_date: datetime
    summary: str
    relevance_score: int
    category: str
    why_matters: str
    impact_analysis: str
    headline_priority: int = 0
    actionable: bool = False
    personal_priority: int = 5
    project_relevance: Dict[str, int] = field(default_factory=dict)
    why_matters_to_you: str = ""
    related_projects: List[str] = field(default_factory=list)
    priority: int = 5
    deep_dive_recommended: bool = False
    deep_dive_reason: str = ""
    article_type: str = "general"


class AIProcessorBatch:
    """æ‰¹é‡AIå¤„ç†å™¨"""
    
    def __init__(
        self,
        api_key: str = None,
        model_name: str = "Claude-Sonnet-4.5",
        user_profile: Dict = None,
        explicit_feedback_manager: Optional[ExplicitFeedbackManager] = None,
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            api_key: Poe APIå¯†é’¥
            model_name: æ¨¡å‹åç§°
            user_profile: ç”¨æˆ·é…ç½®
        """
        import os
        self.api_key = api_key or os.getenv('POE_API_KEY')
        self.model_name = model_name
        self.user_profile = user_profile or {}
        self.explicit_feedback_manager = explicit_feedback_manager
        
        if not self.api_key:
            raise ValueError("éœ€è¦è®¾ç½®POE_API_KEYç¯å¢ƒå˜é‡")
    
    def batch_select_and_analyze(
        self, 
        all_items: List[Dict], 
        top_n: int = 25
    ) -> List[ProcessedItem]:
        """
        ä¸€æ¬¡æ€§ç­›é€‰å’Œåˆ†ææ‰€æœ‰æ¡ç›®
        
        é€‰é¡¹2: ä¸ºè®ºæ–‡ç±»åˆ«å•ç‹¬å¤„ç†ï¼Œç¡®ä¿è‡³å°‘å¤„ç†ä¸€éƒ¨åˆ†è®ºæ–‡
        
        Args:
            all_items: æ‰€æœ‰é‡‡é›†çš„æ¡ç›®
            top_n: ç­›é€‰å‡ºçš„æ•°é‡
            
        Returns:
            å¤„ç†åçš„æ¡ç›®åˆ—è¡¨
        """
        logger.info(f"ğŸš€ æ‰¹é‡å¤„ç†æ¨¡å¼å¯åŠ¨: {len(all_items)} æ¡æ–°é—» â†’ ç­›é€‰ Top {top_n}")
        
        # é€‰é¡¹2: åˆ†ç¦»è®ºæ–‡å’Œæ–°é—»ï¼Œç¡®ä¿è®ºæ–‡è¢«ä¼˜å…ˆå¤„ç†
        paper_items = []
        news_items = []
        for item in all_items:
            category = getattr(item, 'category', item.get('category', '') if hasattr(item, 'get') else '')
            if category == 'paper':
                paper_items.append(item)
            else:
                news_items.append(item)
        
        logger.info(f"ğŸ“„ è®ºæ–‡: {len(paper_items)} æ¡, ğŸ“° æ–°é—»: {len(news_items)} æ¡")
        
        # è®ºæ–‡å†…éƒ¨ä¼˜å…ˆçº§ï¼šHugging Face Papers > Papers with Code > arXiv
        def get_paper_priority(item):
            source = getattr(item, 'source', item.get('source', '') if hasattr(item, 'get') else '').lower()
            if 'hugging face' in source:
                return 3
            elif 'papers with code' in source:
                return 2
            elif 'arxiv' in source:
                return 1
            else:
                return 0
        
        # æŒ‰æ¥æºä¼˜å…ˆçº§æ’åºè®ºæ–‡
        paper_items.sort(key=get_paper_priority, reverse=True)
        
        # ç¡®ä¿è‡³å°‘å¤„ç†15ç¯‡è®ºæ–‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        paper_quota = min(15, len(paper_items))
        news_quota = top_n - paper_quota
        
        # é‡æ–°ç»„åˆï¼šè®ºæ–‡ä¼˜å…ˆï¼ˆå·²æŒ‰æ¥æºä¼˜å…ˆçº§æ’åºï¼‰
        prioritized_items = paper_items[:paper_quota] + news_items[:news_quota]
        
        # ç»Ÿè®¡è®ºæ–‡æ¥æº
        paper_sources = {}
        for p in paper_items[:paper_quota]:
            source = getattr(p, 'source', 'Unknown')
            paper_sources[source] = paper_sources.get(source, 0) + 1
        
        logger.info(f"âœ“ ä¼˜å…ˆå¤„ç†: {len(paper_items[:paper_quota])} ç¯‡è®ºæ–‡ + {len(news_items[:news_quota])} æ¡æ–°é—»")
        if paper_sources:
            logger.info(f"  è®ºæ–‡æ¥æº: {', '.join([f'{k}: {v}' for k, v in sorted(paper_sources.items(), key=lambda x: x[1], reverse=True)])}")
        
        # æ„å»ºæ–°é—»åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼Œåªå‘é€æ ‡é¢˜å’Œæ‘˜è¦ï¼‰
        news_list = []
        for i, item in enumerate(prioritized_items, 1):
            # å…¼å®¹dataclasså’Œdictï¼Œæ™ºèƒ½æå–æ¥æº
            source = getattr(item, 'source', item.get('source', '') if hasattr(item, 'get') else '')
            
            # å¦‚æœsourceä¸ºç©ºï¼Œå°è¯•ä»repo_nameæˆ–å…¶ä»–å­—æ®µæå–
            if not source or source == 'Unknown':
                # GitHub Releaseé€šå¸¸æœ‰repo_name
                repo_name = getattr(item, 'repo_name', item.get('repo_name', '') if hasattr(item, 'get') else '')
                if repo_name:
                    source = repo_name
                else:
                    source = 'Unknown'
            
            title = getattr(item, 'title', item.get('title', '') if hasattr(item, 'get') else '')[:200]
            
            # ä¼˜å…ˆä½¿ç”¨summaryï¼Œæ²¡æœ‰åˆ™ç”¨description
            if hasattr(item, 'summary'):
                summary = getattr(item, 'summary', '')
            elif hasattr(item, 'description'):
                summary = getattr(item, 'description', '')
            elif hasattr(item, 'get'):
                summary = item.get('summary', item.get('description', ''))
            else:
                summary = ''
            summary = summary[:400]
            
            # å‘å¸ƒæ—¥æœŸ
            pub_date = getattr(item, 'published_date', item.get('published_date', '') if hasattr(item, 'get') else '')
            if isinstance(pub_date, datetime):
                pub_date = pub_date.strftime('%Y-%m-%d')
            
            news_list.append(f"{i}. [{source}] {title}\n   {summary}\n   å‘å¸ƒ: {pub_date}\n")
        
        # æ„å»ºprompt
        user_context = self._build_user_context()
        active_projects = self.user_profile.get('active_projects', [])
        project_names = [proj.get('name') for proj in active_projects if proj.get('name')]
        project_instruction = ""
        if project_names:
            bullet_lines = "\n".join([f"  - {name}" for name in project_names])
            project_instruction = (
                "\né¡¹ç›®ç›¸å…³æ€§è¦æ±‚ï¼š\n"
                "- é’ˆå¯¹ä»¥ä¸‹é¡¹ç›®åˆ†åˆ«ç»™å‡º0-10çš„æ•´æ•°è¯„åˆ†ï¼ˆ0=æ— å…³ï¼Œ10=éœ€è¦ç«‹å³é‡‡å–è¡ŒåŠ¨ï¼‰ï¼š\n"
                f"{bullet_lines}\n"
                "- JSONå­—æ®µ`project_relevance`å¿…é¡»åŒ…å«ä¸Šè¿°æ¯ä¸ªé¡¹ç›®åç§°ä½œä¸ºé”®ã€‚\n"
            )

        few_shot_block = self._build_few_shot_block(news_list)

        prompt = f"""ä½ æ˜¯AIå·¥ç¨‹å¸ˆçš„æŠ€æœ¯åŠ©ç†ã€‚æˆ‘é‡‡é›†äº†{len(all_items)}æ¡AIç›¸å…³æ–°é—»ã€‚

âš ï¸ æ•°æ®ä¸€è‡´æ€§è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. "index" å¿…é¡»å‡†ç¡®å¯¹åº”åŸå§‹æ¡ç›®åºå·
2. "summary" å¿…é¡»æ€»ç»“è¯¥ index å¯¹åº”æ¡ç›®çš„å®é™…å†…å®¹
3. ç»å¯¹ä¸èƒ½æŠŠç¬¬Næ¡çš„å†…å®¹æ€»ç»“æˆç¬¬Mæ¡çš„summary
4. å¦‚æœä¸ç¡®å®šæŸæ¡å†…å®¹ï¼Œè¯·å¦‚å®åæ˜ åŸå§‹ä¿¡æ¯

âš ï¸ Hacker News å†…å®¹å¤„ç†ç‰¹åˆ«è¦æ±‚ï¼š
- å¦‚æœåŸå§‹æ‘˜è¦åªæœ‰"çƒ­é—¨è®¨è®ºï¼šXåˆ†ï¼ŒYæ¡è¯„è®º"ï¼Œè¯·åŸºäºæ ‡é¢˜æ¨æ–­å†…å®¹ä¸»é¢˜
- ç”Ÿæˆä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¸­æ–‡æ‘˜è¦ï¼Œè¯´æ˜è¿™ä¸ªè®¨è®ºå¯èƒ½æ¶‰åŠçš„å†…å®¹å’Œä»·å€¼
- ä¾‹å¦‚ï¼šæ ‡é¢˜"Three kinds of AI products work" â†’ æ‘˜è¦"è®¨è®ºäº†ä¸‰ç§æˆåŠŸçš„AIäº§å“æ¨¡å¼ï¼Œåˆ†æäº†ä»€ä¹ˆæ ·çš„AIäº§å“èƒ½å¤ŸçœŸæ­£ä¸ºç”¨æˆ·åˆ›é€ ä»·å€¼å¹¶è·å¾—å¸‚åœºæˆåŠŸ"

âš ï¸ è¯­è¨€è¦æ±‚ï¼š
- æ‰€æœ‰æ‘˜è¦å¿…é¡»ä½¿ç”¨ä¸­æ–‡
- å³ä½¿åŸæ–‡æ˜¯è‹±æ–‡ï¼Œä¹Ÿè¦ç¿»è¯‘æˆä¸­æ–‡
- ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ï¼ˆå¦‚RAGã€LLMç­‰å¯ä¿ç•™è‹±æ–‡ç¼©å†™ï¼‰

{user_context}{project_instruction}{few_shot_block}

è¯·ç­›é€‰æœ€é‡è¦çš„{top_n}æ¡å¹¶è¯¦ç»†åˆ†æã€‚

æ‰€æœ‰æ–°é—»ï¼š
{''.join(news_list)}

è¿”å›JSONæ•°ç»„ï¼Œæ¯æ¡æ–°é—»åŒ…å«ï¼š
[
  {{
    "index": ç¼–å·(1-{len(all_items)}),
    "summary": "ç”¨ä¸­æ–‡å†™3å¥è¯æ€»ç»“è¯¥indexå¯¹åº”æ¡ç›®çš„å®é™…å†…å®¹ï¼šç¬¬1å¥æ˜¯ä»€ä¹ˆ(What)ã€ç¬¬2å¥ä¸ºä»€ä¹ˆé‡è¦(Why)ã€ç¬¬3å¥å…·ä½“å˜åŒ–(How)ã€‚å¯¹äºHacker Newsè®¨è®ºï¼Œè¯·åŸºäºæ ‡é¢˜æ¨æ–­å¹¶ç”Ÿæˆæœ‰æ„ä¹‰çš„ä¸­æ–‡æ‘˜è¦ï¼Œä¸è¦åªå†™'çƒ­é—¨è®¨è®º'ã€‚æ‰€æœ‰è‹±æ–‡å†…å®¹å¿…é¡»ç¿»è¯‘æˆä¸­æ–‡",
    "category": "headline|framework|article|model|project",
    "headline_priority": 0-10,
    "relevance_score": 0-10,
    "why_matters": "ä¸ºä»€ä¹ˆè¿™ä¸ªæ›´æ–°å¯¹ç”¨æˆ·é‡è¦",
    "impact_analysis": "å…·ä½“å½±å“å’Œå»ºè®®è¡ŒåŠ¨",
    "actionable": true|false,
    "personal_priority": 0-10,
    "project_relevance": {{"é¡¹ç›®åç§°": 0-10}},
    "why_matters_to_you": "ç›´æ¥è¯´æ˜å¯¹Davidçš„ä»·å€¼",
    "related_projects": ["ä¸å†…å®¹é«˜åº¦ç›¸å…³çš„é¡¹ç›®åç§°ï¼Œè¯„åˆ†â‰¥6"],
    "deep_dive_recommended": true|false,
    "deep_dive_reason": "å¦‚æœå»ºè®®æ·±å…¥ç ”ç©¶ï¼Œç”¨1å¥è¯è¯´æ˜åŸå› ï¼›å¦åˆ™ç•™ç©º",
    "article_type": "trend|technical|general"
  }}
]

åˆ†ç±»è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. category="paper": å­¦æœ¯è®ºæ–‡/ç ”ç©¶æˆæœ
   - **æ‰€æœ‰æ¥è‡ªarXivï¼ˆcs.CL/cs.IR/cs.LG/cs.AI/stat.MLï¼‰çš„è®ºæ–‡**
   - **æ‰€æœ‰æ¥è‡ªHugging Face Papersçš„è®ºæ–‡**
   - å­¦æœ¯ç ”ç©¶ã€æŠ€æœ¯æŠ¥å‘Šã€é¢„å°æœ¬
   - **ä¼˜å…ˆçº§æœ€é«˜**ï¼šè®ºæ–‡ç±»å†…å®¹å¿…é¡»ä¿ç•™ï¼Œç”¨äº"è®ºæ–‡ç²¾é€‰"æ¿å—

2. category="headline": å¤´æ¡æ–°é—»/åª’ä½“æŠ¥é“
   - **æ¥è‡ªTechCrunch/VentureBeat/The Verge/MIT Tech Review/Import AIçš„æ–°é—»æŠ¥é“**
   - æ–°æ¨¡å‹å‘å¸ƒã€äº§å“ä¸Šçº¿ã€èèµ„ã€æ”¶è´­ã€é‡å¤§å®•æœºã€è¡Œä¸šæ”¿ç­–
   - å…¬å¸åŠ¨æ€ã€å¸‚åœºåˆ†æã€äº§å“è¯„æµ‹ã€è¡Œä¸šè¶‹åŠ¿æŠ¥é“
   - Hacker Newsçš„çƒ­é—¨è®¨è®ºï¼ˆä½†ä¸åŒ…æ‹¬æ¡†æ¶æ›´æ–°ï¼‰
   - **ä¸¥æ ¼æ’é™¤**ï¼š
     * Towards Data Scienceçš„æ–‡ç« ï¼ˆå¿…é¡»å½’ä¸ºarticleï¼‰
     * GitHub Releaseï¼ˆå¿…é¡»å½’ä¸ºframeworkæˆ–modelï¼‰
     * æ¡†æ¶ç‰ˆæœ¬æ›´æ–°ï¼ˆå¿…é¡»å½’ä¸ºframeworkï¼‰
     * arXivè®ºæ–‡ï¼ˆå¿…é¡»å½’ä¸ºpaperï¼‰

3. category="framework": æ¡†æ¶/SDKæ›´æ–°
   - **æ‰€æœ‰GitHub Releaseçš„æ¡†æ¶æ›´æ–°**ï¼šLangChain/LlamaIndex/LangGraph/OpenAI Python SDKç­‰
   - ç‰ˆæœ¬å·æ ‡é¢˜ï¼ˆå¦‚v1.0.3, langchain-core==1.0.2ï¼‰å¿…é¡»å½’ä¸ºframework

4. category="article": æ·±åº¦æŠ€æœ¯æ–‡ç« /æ•™ç¨‹/æœ€ä½³å®è·µ
   - **æ‰€æœ‰æ¥è‡ªTowards Data Scienceçš„æ–‡ç« **ï¼ˆæ— è®ºæ ‡é¢˜æ˜¯ä»€ä¹ˆï¼‰
   - æ•™ç¨‹ã€How-toæŒ‡å—ã€æŠ€æœ¯æ·±åº¦åˆ†æ
   - **æ’é™¤**ï¼šæ–°é—»æŠ¥é“ã€å­¦æœ¯è®ºæ–‡

5. category="model": æ–°æ¨¡å‹/æ¨ç†å·¥å…·æ›´æ–°
   - **Ollama/vLLMçš„GitHub Release**ï¼ˆå¦‚v0.12.7ï¼‰
   - æ–°æ¨¡å‹å‘å¸ƒï¼ˆä½†åª’ä½“æŠ¥é“é™¤å¤–ï¼‰

6. category="project": å¼€æºé¡¹ç›®ï¼ˆæ–°å‘å¸ƒçš„AIå·¥å…·ã€åº“ï¼‰
   - Hacker Newsçš„"Show HN"é¡¹ç›®å±•ç¤º

headline_priorityè¯„åˆ†ï¼ˆä»…headlineç±»åˆ«ï¼‰ï¼š
- 10åˆ†ï¼šè¡Œä¸šåœ°éœ‡çº§ï¼ˆGPT-5å‘å¸ƒã€OpenAIè¢«æ”¶è´­ï¼‰
- 8-9åˆ†ï¼šé‡å¤§äº‹ä»¶ï¼ˆé‡è¦äº§å“å‘å¸ƒã€ç‹¬è§’å…½èèµ„ã€æŠ€æœ¯çªç ´ï¼‰
- 6-7åˆ†ï¼šé‡è¦æ–°é—»ï¼ˆäº§å“å‘å¸ƒã€å¤§å‚AIåŠ¨æ€ã€é‡è¦æ”¶è´­ã€å¸‚åœºè¶‹åŠ¿ï¼‰
- 4-5åˆ†ï¼šä¸€èˆ¬æ–°é—»ï¼ˆå°å…¬å¸èèµ„ã€äº§å“æ›´æ–°ã€è¡Œä¸šè§‚å¯Ÿï¼‰
- 2-3åˆ†ï¼šæ™®é€šèµ„è®¯

**åª’ä½“æ¥æºåŠ åˆ†**ï¼šæ¥è‡ªTechCrunch/VentureBeat/The Verge/MIT Tech Reviewçš„æ–°é—»+1åˆ†

ç­›é€‰ç­–ç•¥ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
1. **ä¼˜å…ˆé€‰æ‹©headlineç±»åˆ«**ï¼ˆåª’ä½“æ–°é—»ã€é‡å¤§äº‹ä»¶ã€äº§å“å‘å¸ƒï¼‰
2. **ä¸¥æ ¼æ’é™¤framework/modelç±»åˆ«**ï¼šGitHub Releaseã€æ¡†æ¶æ›´æ–°ä¸è¦è¿›å…¥Top 25çš„headlineéƒ¨åˆ†
3. **ä¸¥æ ¼æ’é™¤Towards Data Science**ï¼šæ‰€æœ‰Towards Data Scienceæ–‡ç« å¿…é¡»å½’ä¸ºarticleï¼Œä¸è¿›headline
4. å¹³è¡¡ä¸åŒæ¥æºï¼ˆé¿å…å…¨æ˜¯Hacker Newsï¼‰
5. åŒä¸€æ¥æºçš„å¤šä¸ªç‰ˆæœ¬æ›´æ–°åªä¿ç•™æœ€æ–°/æœ€é‡è¦çš„
6. ç¡®ä¿è‡³å°‘æœ‰3-5æ¡æ¥è‡ªTechCrunch/VentureBeat/The Verge/MIT Tech Reviewçš„åª’ä½“æ–°é—»
7. **ç‰¹åˆ«å…³æ³¨Fintechç›¸å…³å†…å®¹**ï¼ˆå¿…é¡»ä¼˜å…ˆä¿ç•™ï¼‰ï¼š
   - æ¥è‡ªFintech Timesã€Finextraã€TechCrunch Fintechã€Fintech Newsç­‰æºçš„å†…å®¹åº”ç»™äºˆè¾ƒé«˜relevance_scoreï¼ˆâ‰¥6åˆ†ï¼‰
   - æ¶‰åŠé‡‘èç§‘æŠ€ä¼ä¸šAIè½åœ°å®è·µçš„å†…å®¹ï¼ˆVanguardã€BlackRockã€JPMorganã€Capital Oneã€Goldman Sachsã€Stripeã€PayPalç­‰ï¼‰
   - VCæŠ•èµ„çš„AI+Fintechåˆåˆ›å…¬å¸ç›¸å…³å†…å®¹ï¼ˆY Combinatorã€a16zã€Sequoiaç­‰ï¼‰
   - Fintechç›¸å…³çš„AIåº”ç”¨ï¼šSDLCã€å®¢æˆ·æœåŠ¡ã€é£é™©ç®¡ç†ã€æ•°æ®åˆ†æã€ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ–
   - åœ¨ç­›é€‰Top {top_n}æ¡æ—¶ï¼Œåº”ç¡®ä¿åŒ…å«è‡³å°‘5-10æ¡Fintechç›¸å…³å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

ä¸ªäººä¼˜å…ˆçº§ä¸é¡¹ç›®ç›¸å…³æ€§è¯„åˆ†æŒ‡å—ï¼š
- personal_priority 10ï¼šç›´æ¥æ¨è¿›ä¼ä¸šAIè½åœ°æˆ–å½“å‰æ´»è·ƒé¡¹ç›®ï¼Œå¿…é¡»ç«‹å³æ‰§è¡Œ
- personal_priority 7-9ï¼šæ˜¾è‘—å¸®åŠ©Davidçš„å­¦ä¹ é‡ç‚¹æˆ–é¡¹ç›®å†³ç­–ï¼Œå»ºè®®æœ¬å‘¨è·Ÿè¿›
- personal_priority 4-6ï¼šæœ‰å¯å‘æ€§ï¼Œé€‚åˆè®°å½•å¾…æŸ¥
- personal_priority 0-3ï¼šçŸ­æœŸå†…ä»·å€¼è¾ƒä½
- project_relevanceï¼šé’ˆå¯¹æ¯ä¸ªé¡¹ç›®ç»™å‡º0-10åˆ†ï¼Œ10è¡¨ç¤ºç«‹åˆ»å¯åº”ç”¨ï¼Œ7-9è¡¨ç¤ºæœ¬å‘¨å¯è¯„ä¼°ï¼Œ4-6è¡¨ç¤ºä¸­é•¿æœŸå¯å‘ï¼Œ0-3è¡¨ç¤ºæ— å…³

æ·±åº¦ç ”ç©¶æ¨èè§„åˆ™ï¼š
- å¦‚æœå†…å®¹æä¾›å¯ç›´æ¥æ‰§è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€å®éªŒæ­¥éª¤æˆ–ä»£ç ï¼Œè¯·è®¾ç½® deep_dive_recommended = trueï¼Œå¹¶ç»™å‡ºæ˜ç¡®ç†ç”±
- å¦‚æœä¸ªäººä¼˜å…ˆçº§ >= 9 æˆ–å¯¹ä»»ä¸€æ´»è·ƒé¡¹ç›®å½±å“è¯„åˆ† >= 8ï¼Œä¹Ÿåº”æ¨è deep dive
- å…³æ³¨èƒ½å½¢æˆç­–ç•¥å¯¹æ¯”æˆ–å†²å‡»ç°æœ‰è·¯çº¿çš„æ–°é—»ï¼ˆä¾‹å¦‚ï¼šæ‰˜ç®¡RAG vs è‡ªå»ºRAGï¼‰

article_type åˆ†ç±»ï¼š
- trendï¼šè¡Œä¸šè¶‹åŠ¿ã€è§‚ç‚¹è§£è¯»ã€æˆ˜ç•¥åˆ†æ
- technicalï¼šæŠ€æœ¯å®ç°ã€æ¶æ„æ‹†è§£ã€å«ç¤ºä¾‹ä»£ç çš„æ•™ç¨‹
- generalï¼šå…¶ä»–å†…å®¹

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦markdownæ ‡è®°ï¼Œä¸è¦è§£é‡Šã€‚
"""
        
        try:
            # è°ƒç”¨LLM
            logger.info("æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ‰¹é‡åˆ†æ...")
            response_text = asyncio.run(self._call_poe_api(prompt))
            
            # è§£æJSON
            logger.info("è§£æLLMå“åº”...")
            cleaned = self._clean_json_response(response_text)
            try:
                analyses = json.loads(cleaned)
            except json.JSONDecodeError as e:
                logger.warning(f"é¦–æ¬¡è§£æå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤JSON: {str(e)}")
                repaired = self._repair_json_string(cleaned)
                analyses = json.loads(repaired)
            
            logger.info(f"âœ“ LLMè¿”å› {len(analyses)} æ¡åˆ†æç»“æœ")
            
            # è½¬æ¢ä¸ºProcessedItem
            processed = []
            for analysis in analyses:
                idx = analysis.get('index', 0) - 1
                if 0 <= idx < len(all_items):
                    original = all_items[idx]
                    
                    # å…¼å®¹dataclasså’Œdictï¼Œæ™ºèƒ½æå–æ¥æº
                    source = getattr(original, 'source', original.get('source', '') if hasattr(original, 'get') else '')
                    
                    # å¦‚æœsourceä¸ºç©ºï¼Œå°è¯•ä»repo_nameæå–
                    if not source or source == 'Unknown':
                        repo_name = getattr(original, 'repo_name', original.get('repo_name', '') if hasattr(original, 'get') else '')
                        if repo_name:
                            source = repo_name
                        else:
                            source = 'Unknown'
                    
                    title = getattr(original, 'title', original.get('title', '') if hasattr(original, 'get') else '')
                    url = getattr(original, 'url', None) or getattr(original, 'link', None)
                    if url is None and hasattr(original, 'get'):
                        url = original.get('url', original.get('link', ''))
                    url = url or ''
                    
                    pub_date = getattr(original, 'published_date', None)
                    if pub_date is None and hasattr(original, 'get'):
                        pub_date = original.get('published_date', datetime.now())
                    pub_date = pub_date or datetime.now()
                    
                    # è·å–åŸå§‹ summaryï¼Œç”¨äºæ•°æ®ä¸€è‡´æ€§éªŒè¯
                    original_summary = getattr(original, 'summary', None)
                    if original_summary is None and hasattr(original, 'get'):
                        original_summary = original.get('summary', '')
                    original_summary = original_summary or ''

                    project_relevance = analysis.get('project_relevance', {}) or {}
                    if not isinstance(project_relevance, dict):
                        project_relevance = {}
                    normalized_project_relevance = {}
                    for name, score in project_relevance.items():
                        try:
                            int_score = int(float(score))
                        except (TypeError, ValueError):
                            continue
                        normalized_project_relevance[str(name)] = max(0, min(10, int_score))

                    related_projects = analysis.get('related_projects', []) or []
                    if not isinstance(related_projects, list):
                        related_projects = []
                    if not related_projects and normalized_project_relevance:
                        related_projects = [
                            project_name
                            for project_name, score in normalized_project_relevance.items()
                            if isinstance(score, int) and score >= 6
                        ]

                    personal_priority = analysis.get('personal_priority', analysis.get('relevance_score', 5))
                    try:
                        personal_priority = int(float(personal_priority))
                    except (TypeError, ValueError):
                        personal_priority = 5
                    personal_priority = max(0, min(10, personal_priority))

                    why_matters_to_you = analysis.get('why_matters_to_you') or analysis.get('why_matters', '')
                    
                    # æ•°æ®ä¸€è‡´æ€§éªŒè¯ï¼šæ£€æŸ¥ AI è¿”å›çš„ summary æ˜¯å¦ä¸åŸå§‹å†…å®¹åŒ¹é…
                    ai_summary = analysis.get('summary', '')
                    why_matters = analysis.get('why_matters', '')
                    
                    # å¦‚æœ AI è¿”å›çš„ summary ä¸åŸå§‹ title/url æ˜æ˜¾ä¸åŒ¹é…ï¼Œä½¿ç”¨åŸå§‹ summary
                    # æ£€æµ‹å…³é”®è¯ä¸åŒ¹é…ï¼ˆä¾‹å¦‚ï¼štitle è¯´ Anthropicï¼Œä½† summary è¯´ vector databaseï¼‰
                    title_lower = title.lower()
                    ai_summary_lower = ai_summary.lower()
                    
                    # æå– title ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤å¸¸è§è¯å’ŒæŠ€æœ¯åç¼€ï¼‰
                    stop_words = {'the', 'and', 'for', 'with', 'from', 'that', 'this', 'what', 'when', 'where', 
                                 'how', 'why', 'are', 'was', 'were', 'been', 'have', 'has', 'had', 'will', 'would',
                                 'via', 'using', 'based', 'system', 'model', 'learning', 'paper'}
                    title_keywords = set([w for w in title_lower.split() if len(w) > 4 and w not in stop_words])
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å…³é”®è¯å‡ºç°åœ¨ summary ä¸­
                    # å¯¹äºä¸­æ–‡æ‘˜è¦ï¼Œæˆ‘ä»¬æ›´å®½æ¾ä¸€äº›ï¼Œåªè¦æœ‰1-2ä¸ªå…³é”®è¯åŒ¹é…å³å¯
                    keyword_matches = sum(1 for keyword in title_keywords if keyword in ai_summary_lower)
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸¥é‡ä¸åŒ¹é…ï¼š
                    # 1. æœ‰å¤šä¸ªå…³é”®è¯ï¼ˆâ‰¥3ä¸ªï¼‰
                    # 2. ä½†ä¸€ä¸ªéƒ½ä¸åŒ¹é…
                    # 3. ä¸”åŸå§‹ summary å­˜åœ¨ä¸”ä¸æ˜¯å ä½ç¬¦
                    is_serious_mismatch = (
                        len(title_keywords) >= 3 and 
                        keyword_matches == 0 and 
                        original_summary and 
                        len(original_summary) > 20 and
                        'Author/Org:' not in original_summary  # æ’é™¤å ä½ç¬¦å¼çš„åŸå§‹æ‘˜è¦
                    )
                    
                    if is_serious_mismatch:
                        logger.warning(f"âš ï¸  æ•°æ®ä¸ä¸€è‡´ï¼Title: '{title[:50]}...' ä½† AI summary ä¸åŒ¹é…ï¼Œä½¿ç”¨åŸå§‹ summary")
                        final_summary = original_summary
                        # é‡ç½® why_matters å’Œç›¸å…³å­—æ®µï¼Œé¿å…é”™è¯¯ä¿¡æ¯ä¼ æ’­
                        why_matters = f"æ¥è‡ª {source} çš„å†…å®¹"
                        why_matters_to_you = f"æ¥è‡ª {source} çš„å†…å®¹ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ"
                    else:
                        # ä½¿ç”¨ AI ç”Ÿæˆçš„æ‘˜è¦ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡ï¼‰
                        final_summary = ai_summary if ai_summary else original_summary
                    
                    processed.append(ProcessedItem(
                        source=source,
                        title=title,
                        url=url,
                        published_date=pub_date,
                        summary=final_summary,
                        relevance_score=analysis.get('relevance_score', 5),
                        category=analysis.get('category', 'other'),
                        why_matters=why_matters,
                        impact_analysis=analysis.get('impact_analysis', ''),
                        headline_priority=analysis.get('headline_priority', 0),
                        actionable=analysis.get('actionable', False),
                        personal_priority=personal_priority,
                        project_relevance=normalized_project_relevance,
                        why_matters_to_you=why_matters_to_you,
                        related_projects=related_projects,
                        priority=personal_priority,
                        deep_dive_recommended=bool(analysis.get('deep_dive_recommended', False)),
                        deep_dive_reason=analysis.get('deep_dive_reason', ''),
                        article_type=analysis.get('article_type', 'general')
                    ))
                else:
                    logger.warning(f"ç´¢å¼•è¶…å‡ºèŒƒå›´: {idx+1}")
            
            # ç»Ÿè®¡åˆ†ç±»åˆ†å¸ƒ
            category_dist = {}
            for item in processed:
                category_dist[item.category] = category_dist.get(item.category, 0) + 1
            
            logger.info(f"âœ“ æ‰¹é‡å¤„ç†å®Œæˆï¼ç­›é€‰å‡º {len(processed)} æ¡")
            logger.info(f"  åˆ†ç±»åˆ†å¸ƒ: {category_dist}")
            
            return processed
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
            logger.error(f"å“åº”å‰500å­—ç¬¦: {response_text[:500] if response_text else 'None'}")
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    async def _call_poe_api(self, prompt: str) -> str:
        """
        è°ƒç”¨Poe API
        
        Args:
            prompt: æç¤ºè¯
            
        Returns:
            APIå“åº”æ–‡æœ¬
        """
        try:
            full_response = ""
            async for partial in get_bot_response(
                messages=[{"role": "user", "content": prompt}],
                bot_name=self.model_name,
                api_key=self.api_key
            ):
                full_response += partial.text
            
            return full_response
            
        except Exception as e:
            logger.error(f"Poe APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _clean_json_response(self, response_text: str) -> str:
        """
        æ¸…ç†JSONå“åº”
        
        Args:
            response_text: åŸå§‹å“åº”
            
        Returns:
            æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
        """
        if not response_text:
            raise ValueError("å“åº”ä¸ºç©º")
        
        # å»é™¤ç©ºç™½
        cleaned = response_text.strip()
        
        # å»é™¤markdownä»£ç å—æ ‡è®°
        cleaned = cleaned.replace('```json\n', '').replace('```json', '')
        cleaned = cleaned.replace('\n```', '').replace('```', '')
        cleaned = cleaned.strip()
        
        # å°†ä¸­æ–‡å¼•å·/çœç•¥å·æ›¿æ¢ä¸ºæ ‡å‡†å­—ç¬¦
        replacements = {
            "â€œ": '"',
            "â€": '"',
            "â€™": "'",
            "â€˜": "'",
            "â€¦": "...",
        }
        for src, target in replacements.items():
            cleaned = cleaned.replace(src, target)
        
        # å¦‚æœä¸æ˜¯ä»¥[æˆ–{å¼€å¤´ï¼Œå°è¯•æå–JSON
        if not cleaned.startswith('[') and not cleaned.startswith('{'):
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª[æˆ–{
            start_bracket = cleaned.find('[')
            start_brace = cleaned.find('{')
            
            start = -1
            if start_bracket != -1 and start_brace != -1:
                start = min(start_bracket, start_brace)
            elif start_bracket != -1:
                start = start_bracket
            elif start_brace != -1:
                start = start_brace
            
            if start != -1:
                # æŸ¥æ‰¾å¯¹åº”çš„ç»“æŸç¬¦
                if cleaned[start] == '[':
                    end = cleaned.rfind(']') + 1
                else:
                    end = cleaned.rfind('}') + 1
                
                if end > start:
                    cleaned = cleaned[start:end]
        
        # å»é™¤æœ«å°¾å¤šä½™çš„é€—å·ï¼ˆä¾‹å¦‚ [...,] æˆ– {...,}ï¼‰
        cleaned = re.sub(r',\s*(\]}|\})', r'\1', cleaned)
        return cleaned
    
    def _repair_json_string(self, text: str) -> str:
        """
        å°è¯•ä¿®å¤JSONå­—ç¬¦ä¸²ä¸­çš„å¸¸è§æ ¼å¼é—®é¢˜ï¼ˆä¾‹å¦‚æœªè½¬ä¹‰çš„å¼•å·ï¼‰
        """
        if not text:
            return text
        
        if repair_json is not None:
            try:
                return repair_json(text)
            except Exception as err:  # pragma: no cover - ä»…æ—¥å¿—è­¦å‘Š
                logger.warning(f'json_repair è§£æå¤±è´¥: {err}')
        
        result = []
        in_string = False
        escape = False
        length = len(text)
        
        i = 0
        while i < length:
            ch = text[i]
            
            if in_string:
                if escape:
                    result.append(ch)
                    escape = False
                elif ch == '\\':
                    escape = True
                    result.append(ch)
                elif ch == '"':
                    # æŸ¥çœ‹åç»­å­—ç¬¦ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£çš„å­—ç¬¦ä¸²ç»“æŸ
                    j = i + 1
                    while j < length and text[j] in ' \t\r\n':
                        j += 1
                    next_char = text[j] if j < length else ''
                    if next_char in {',', '}', ']'}:
                        result.append(ch)
                        in_string = False
                    else:
                        # è®¤ä¸ºæ˜¯æœªè½¬ä¹‰çš„å¼•å·ï¼Œè‡ªåŠ¨è½¬ä¹‰
                        result.append('\\')
                        result.append('"')
                elif ch == '\n':
                    result.append('\\n')
                elif ch == '\r':
                    # å¿½ç•¥\rï¼Œå·²ç”±\nå¤„ç†
                    pass
                else:
                    result.append(ch)
            else:
                if ch == '"':
                    in_string = True
                result.append(ch)
            i += 1
        
        # å¦‚æœå­—ç¬¦ä¸²æœªæ­£å¸¸ç»“æŸï¼Œè¡¥é½å¼•å·
        if in_string:
            result.append('"')
        
        repaired = ''.join(result)
        # å†æ¬¡å»æ‰å°¾éƒ¨å¤šä½™é€—å·
        repaired = re.sub(r',\s*(\]}|\})', r'\1', repaired)
        return repaired
    
    def _build_user_context(self) -> str:
        """æ„å»ºç”¨æˆ·ä¸Šä¸‹æ–‡æè¿°"""
        if not self.user_profile:
            return """ç”¨æˆ·èƒŒæ™¯ï¼š
- è§’è‰²ï¼šBackend Developer â†’ AI Engineer
- ç»éªŒï¼š20+ years backend development
- èŒä¸šç›®æ ‡ï¼šåœ¨ä¼ä¸šå†…éƒ¨è½åœ°AIåº”ç”¨
- æ´»è·ƒé¡¹ç›®ï¼šmutation-test-killer, ai-digest, rag-practics
"""
        
        user_info = self.user_profile.get('user_info', {})
        career_goals = self.user_profile.get('career_goals', {})
        active_projects = self.user_profile.get('active_projects', [])
        learning_focus = self.user_profile.get('learning_focus', {})
        relevance_criteria = self.user_profile.get('relevance_criteria', {})
        secondary_goals = career_goals.get('secondary', [])
        project_lines = []
        for idx, project in enumerate(active_projects, start=1):
            name = project.get('name', f"é¡¹ç›®{idx}")
            description = project.get('description', '')
            goals = ", ".join(project.get('goals', []))
            tech_stack = ", ".join(project.get('tech_stack', []))
            lines = [f"{idx}. {name}ï¼š{description}"]
            if goals:
                lines.append(f"   ç›®æ ‡ï¼š{goals}")
            if tech_stack:
                lines.append(f"   æŠ€æœ¯æ ˆï¼š{tech_stack}")
            project_lines.append("\n".join(lines))
        projects_block = "\n".join(project_lines) if project_lines else "æš‚æ— æ˜ç¡®é¡¹ç›®"
        current_focus = ", ".join(learning_focus.get('current', [])) or "æŒç»­æ¢ç´¢"
        interested_focus = ", ".join(learning_focus.get('interested_in', [])) or "æŒç»­è¡¥å……"
        high_priority = ", ".join(relevance_criteria.get('high_priority', []))
        medium_priority = ", ".join(relevance_criteria.get('medium_priority', []))
        low_priority = ", ".join(relevance_criteria.get('low_priority', []))
        context = f"""ç”¨æˆ·èƒŒæ™¯ï¼š
- å§“åï¼š{user_info.get('name', 'ç”¨æˆ·')}
- è§’è‰²ï¼š{user_info.get('role', 'Backend Developer â†’ AI Engineer')}
- ç»éªŒï¼š{user_info.get('experience', '20+ years backend development')}
- å½“å‰é˜¶æ®µï¼š{user_info.get('current_stage', 'AIå­¦ä¹ ä¸è½åœ°æ¢ç´¢')}
- èŒä¸šç›®æ ‡ï¼ˆprimaryï¼‰ï¼š{career_goals.get('primary', 'åœ¨ä¼ä¸šå†…éƒ¨è½åœ°AIåº”ç”¨')}
- èŒä¸šç›®æ ‡ï¼ˆsecondaryï¼‰ï¼š{', '.join(secondary_goals) if secondary_goals else 'æŒç»­æ‰©å±•AIèƒ½åŠ›'}

æ´»è·ƒé¡¹ç›®ï¼š
{projects_block}

å­¦ä¹ é‡ç‚¹ï¼š
- å½“å‰å…³æ³¨ï¼š{current_focus}
- æ„Ÿå…´è¶£æ–¹å‘ï¼š{interested_focus}

é«˜ä¼˜å…ˆçº§ä¸»é¢˜ï¼š{high_priority}
ä¸­ä¼˜å…ˆçº§ä¸»é¢˜ï¼š{medium_priority}
ä½ä¼˜å…ˆçº§ä¸»é¢˜ï¼š{low_priority}
"""
        return context

    def _build_few_shot_block(self, news_list: List[str]) -> str:
        if not self.explicit_feedback_manager:
            return ""

        sample_context = "\n".join(news_list[:5])
        return self.explicit_feedback_manager.build_prompt_block(
            sample_context,
            correction_type="batch_selection",
            fallback_type="analysis",
            max_examples=3,
        )

