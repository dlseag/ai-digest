"""
Market Insights Collector
å¸‚åœºæ´å¯Ÿé‡‡é›†å™¨ï¼šé‡‡é›†a16zã€CB Insightsç­‰å¸‚åœºåˆ†æå†…å®¹
"""

import logging
import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass

from src.utils.dedupe import normalize_url, unique_items

logger = logging.getLogger(__name__)


@dataclass
class MarketInsight:
    """å¸‚åœºæ´å¯Ÿæ•°æ®ç±»"""
    title: str
    source: str
    url: str
    published_date: str
    summary: str
    category: str  # 'funding', 'trend', 'analysis', 'report'
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'title': self.title,
            'source': self.source,
            'url': self.url,
            'published_date': self.published_date,
            'summary': self.summary,
            'category': self.category
        }


class MarketInsightsCollector:
    """å¸‚åœºæ´å¯Ÿé‡‡é›†å™¨"""
    
    def __init__(self, sources: List[Dict] = None):
        """
        åˆå§‹åŒ–å¸‚åœºæ´å¯Ÿé‡‡é›†å™¨
        
        Args:
            sources: æ•°æ®æºé…ç½®åˆ—è¡¨
        """
        # é»˜è®¤æ•°æ®æº
        if sources is None:
            sources = [
                {
                    'name': 'a16z AI',
                    'url': 'https://a16z.com/tag/artificial-intelligence/feed/',
                    'category': 'analysis'
                },
                {
                    'name': 'Sequoia AI',
                    'url': 'https://www.sequoiacap.com/feed/',
                    'category': 'analysis'
                },
                {
                    'name': 'AI Index (Stanford)',
                    'url': 'https://aiindex.stanford.edu/feed/',
                    'category': 'report'
                }
            ]
        
        self.sources = sources
        logger.info(f"âœ“ å¸‚åœºæ´å¯Ÿé‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆé…ç½® {len(sources)} ä¸ªæ•°æ®æºï¼‰")
    
    def collect(self, days_back: int = 30) -> List[MarketInsight]:
        """
        é‡‡é›†å¸‚åœºæ´å¯Ÿ
        
        Args:
            days_back: é‡‡é›†æœ€è¿‘Nå¤©çš„å†…å®¹ï¼ˆå¸‚åœºåˆ†æé€šå¸¸æ›´æ–°è¾ƒæ…¢ï¼Œé»˜è®¤30å¤©ï¼‰
            
        Returns:
            å¸‚åœºæ´å¯Ÿåˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“ˆ å¼€å§‹é‡‡é›†å¸‚åœºæ´å¯Ÿï¼ˆæœ€è¿‘ {days_back} å¤©ï¼‰...")
            
            all_insights = []
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            for source in self.sources:
                insights = self._collect_single_source(source, cutoff_date)
                all_insights.extend(insights)
            
            # å»é‡ï¼ˆåŸºäºè§„èŒƒåŒ–URLï¼Œå¿…è¦æ—¶å›é€€åˆ°æ ‡é¢˜ï¼‰
            unique_insights = unique_items(
                all_insights,
                lambda insight: normalize_url(insight.url) or normalize_url(insight.title),
            )
            
            logger.info(f"âœ“ å¸‚åœºæ´å¯Ÿé‡‡é›†å®Œæˆ: æ€»è®¡ {len(unique_insights)} æ¡")
            
            # æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—
            unique_insights.sort(key=lambda x: x.published_date, reverse=True)
            
            return unique_insights
            
        except Exception as e:
            logger.error(f"é‡‡é›†å¸‚åœºæ´å¯Ÿå¤±è´¥: {str(e)}")
            return []
    
    def _collect_single_source(self, source: Dict, cutoff_date: datetime) -> List[MarketInsight]:
        """
        ä»å•ä¸ªæ•°æ®æºé‡‡é›†
        
        Args:
            source: æ•°æ®æºé…ç½®
            cutoff_date: æˆªæ­¢æ—¥æœŸ
            
        Returns:
            å¸‚åœºæ´å¯Ÿåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨requestsè·å–RSSå†…å®¹ï¼ˆç»•è¿‡SSLè¯ä¹¦é—®é¢˜ï¼‰
            response = requests.get(
                source['url'],
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0'}
            )
            response.raise_for_status()
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            
            insights = []
            for entry in feed.entries[:10]:  # æ¯ä¸ªæºæœ€å¤šå–10æ¡
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    published_date = self._parse_date(entry)
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                    if published_date and published_date < cutoff_date:
                        continue
                    
                    # æå–æ‘˜è¦
                    summary = self._extract_summary(entry)
                    
                    # åˆ›å»ºMarketInsightå¯¹è±¡
                    insight = MarketInsight(
                        title=entry.get('title', 'Untitled'),
                        source=source['name'],
                        url=entry.get('link', ''),
                        published_date=published_date.strftime('%Y-%m-%d') if published_date else '',
                        summary=summary,
                        category=source.get('category', 'analysis')
                    )
                    
                    insights.append(insight)
                    
                except Exception as e:
                    logger.debug(f"è§£ææ¡ç›®å¤±è´¥ ({source['name']}): {str(e)}")
                    continue
            
            logger.info(f"âœ“ {source['name']}: {len(insights)} æ¡æ´å¯Ÿ")
            return insights
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–RSSå¤±è´¥ ({source['name']}): {str(e)}")
            return []
        except Exception as e:
            logger.error(f"è§£æRSSå¤±è´¥ ({source['name']}): {str(e)}")
            return []
    
    def _parse_date(self, entry) -> Optional[datetime]:
        """è§£æRSSæ¡ç›®çš„å‘å¸ƒæ—¶é—´"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                from time import mktime
                return datetime.fromtimestamp(mktime(entry.published_parsed))
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                from time import mktime
                return datetime.fromtimestamp(mktime(entry.updated_parsed))
            else:
                # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                return datetime.now()
        except Exception as e:
            logger.debug(f"è§£ææ—¶é—´å¤±è´¥: {str(e)}")
            return datetime.now()
    
    def _extract_summary(self, entry) -> str:
        """æå–RSSæ¡ç›®çš„æ‘˜è¦"""
        try:
            # å°è¯•å¤šä¸ªå­—æ®µ
            if hasattr(entry, 'summary') and entry.summary:
                # æ¸…ç†HTMLæ ‡ç­¾
                import re
                text = re.sub(r'<[^>]+>', '', entry.summary)
                # é™åˆ¶é•¿åº¦
                if len(text) > 300:
                    text = text[:297] + '...'
                return text
            elif hasattr(entry, 'description') and entry.description:
                import re
                text = re.sub(r'<[^>]+>', '', entry.description)
                if len(text) > 300:
                    text = text[:297] + '...'
                return text
            else:
                return 'ï¼ˆæ— æ‘˜è¦ï¼‰'
        except Exception as e:
            logger.debug(f"æå–æ‘˜è¦å¤±è´¥: {str(e)}")
            return 'ï¼ˆæ— æ‘˜è¦ï¼‰'
    
    def get_top_insights(self, all_insights: List[MarketInsight], top_n: int = 3) -> List[MarketInsight]:
        """
        è·å–Top Nå¸‚åœºæ´å¯Ÿ
        
        Args:
            all_insights: æ‰€æœ‰å¸‚åœºæ´å¯Ÿ
            top_n: è¿”å›å‰Næ¡
            
        Returns:
            Top Nå¸‚åœºæ´å¯Ÿ
        """
        # ä¼˜å…ˆçº§æ’åºï¼š
        # 1. æœ€è¿‘å‘å¸ƒçš„
        # 2. æ¥è‡ªçŸ¥åæœºæ„çš„ï¼ˆa16z, Sequoiaï¼‰
        
        prioritized = []
        for insight in all_insights:
            priority_score = 0
            
            # æ¥æºåŠ åˆ†
            if 'a16z' in insight.source.lower():
                priority_score += 10
            elif 'sequoia' in insight.source.lower():
                priority_score += 9
            elif 'stanford' in insight.source.lower():
                priority_score += 8
            
            # ç±»åˆ«åŠ åˆ†
            if insight.category == 'report':
                priority_score += 5
            elif insight.category == 'analysis':
                priority_score += 3
            
            # æ–°é²œåº¦åŠ åˆ†ï¼ˆæœ€è¿‘7å¤©çš„å†…å®¹ï¼‰
            try:
                pub_date = datetime.strptime(insight.published_date, '%Y-%m-%d')
                days_ago = (datetime.now() - pub_date).days
                if days_ago <= 7:
                    priority_score += 5
                elif days_ago <= 14:
                    priority_score += 3
                elif days_ago <= 30:
                    priority_score += 1
            except:
                pass
            
            prioritized.append((priority_score, insight))
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        prioritized.sort(key=lambda x: x[0], reverse=True)
        
        return [insight for _, insight in prioritized[:top_n]]

