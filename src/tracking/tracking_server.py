"""
Tracking Server for Reading Behaviors
é˜…è¯»è¡Œä¸ºè¿½è¸ªæœåŠ¡å™¨
"""

import json
import logging
import os
import asyncio
import re
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
from pathlib import Path
from typing import Optional, Tuple
import sys
from uuid import uuid4
from datetime import datetime, timezone

import requests
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.storage.feedback_db import FeedbackDB
from src.agents.tool_executor import ToolExecutor
from src.learning.feedback_reinforcer import FeedbackReinforcer
from src.utils.llm_client import get_llm_client
from src.memory.hot_cache import HotMemoryCache
from src.memory.cache_sync import sync_hot_cache_to_warm_storage
from src.memory.metrics import get_memory_health_api

logger = logging.getLogger(__name__)
HISTORY_LOG_PATH = Path(__file__).parents[2] / "logs" / "deep_dive_history.jsonl"
HISTORY_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
TRACKING_LOG_CANDIDATES = [
    Path.home() / "Library" / "Logs" / "ai-digest-tracking-server.log",
    Path.home() / "Library" / "Logs" / "ai-digest-tracking-server.error.log",
    Path(__file__).parents[2] / "logs" / "tracking-server.log",
    Path(__file__).parents[2] / "logs" / "tracking-server.error.log",
]


class TrackingHandler(BaseHTTPRequestHandler):
    """è¿½è¸ªè¯·æ±‚å¤„ç†å™¨"""
    
    # ç±»çº§åˆ«å…±äº«çš„æ•°æ®åº“è¿æ¥å’Œå·¥å…·æ‰§è¡Œå™¨
    db = None
    tool_executor = None
    feedback_reinforcer = None
    hot_cache = None
    hot_cache_flush_threshold = 400
    history_log_path = HISTORY_LOG_PATH
    log_candidates = TRACKING_LOG_CANDIDATES
    
    @classmethod
    def set_db(cls, db):
        cls.db = db
    
    @classmethod
    def set_tool_executor(cls, executor):
        cls.tool_executor = executor
    
    @classmethod
    def set_feedback_reinforcer(cls, reinforcer):
        cls.feedback_reinforcer = reinforcer
    
    @classmethod
    def set_hot_cache(cls, cache: HotMemoryCache, flush_threshold: int = 400):
        cls.hot_cache = cache
        cls.hot_cache_flush_threshold = flush_threshold
    
    def do_OPTIONS(self):
        """å¤„ç†CORSé¢„æ£€è¯·æ±‚"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
    
    def do_POST(self):
        """å¤„ç†POSTè¯·æ±‚ï¼ˆè¿½è¸ªæ•°æ®æˆ–æ‰§è¡Œè¡ŒåŠ¨ï¼‰"""
        # è§£æURLè·¯å¾„
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        
        # å¤„ç†è¡ŒåŠ¨æ‰§è¡Œè¯·æ±‚
        if path == '/api/execute_action':
            self._handle_execute_action()
            return
        
        # å¤„ç†è¿½è¸ªè¯·æ±‚
        if path == '/api/track':
            self._handle_track()
            return
        
        # æœªçŸ¥è·¯å¾„
        self.send_response(404)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        
        response = {'status': 'error', 'message': 'Not found'}
        self.wfile.write(json.dumps(response).encode('utf-8'))
    
    def _handle_track(self):
        """å¤„ç†è¿½è¸ªè¯·æ±‚"""
        
        try:
            # è¯»å–è¯·æ±‚ä½“
            content_length = int(self.headers.get('Content-Length', 0))
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode('utf-8'))
            
            # ä¿å­˜è¡Œä¸ºæ•°æ®
            self._store_reading_behavior(data)
            
            # æ£€æŸ¥æ˜¯å¦ä¸º"æƒ³çœ‹æ›´å¤š"è¯·æ±‚
            action = data.get('action', 'unknown')
            feedback_type = data.get('feedback_type')
            
            if action == "feedback" and feedback_type == "more":
                # åŒæ­¥å¤„ç†æ·±åº¦ç ”ç©¶è¯·æ±‚
                deep_dive_result = self._handle_deep_dive_request(data)
                response = {
                    'status': 'success',
                    'message': 'Behavior tracked',
                    'deep_dive': deep_dive_result
                }
            elif action == "feedback" and feedback_type == "architect_analysis":
                # åŒæ­¥å¤„ç†æ¶æ„å¸ˆåˆ†æè¯·æ±‚
                analysis_result = self._handle_architect_analysis_request(data)
                response = {
                    'status': 'success',
                    'message': 'Behavior tracked',
                    'deep_dive': analysis_result  # å¤ç”¨ deep_dive å­—æ®µä»¥ä¿æŒå‰ç«¯å…¼å®¹æ€§
                }
            else:
                response = {'status': 'success', 'message': 'Behavior tracked'}
            
            # è¿”å›æˆåŠŸå“åº”
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            self.wfile.write(json.dumps(response).encode('utf-8'))
            
            item_id = data.get('item_id', 'N/A')
            logger.info(f"âœ“ è¿½è¸ªè¡Œä¸º: {action} - {item_id}")
            
        except Exception as e:
            logger.error(f"è¿½è¸ªå¤±è´¥: {e}", exc_info=True)
            self.send_response(500)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            response = {'status': 'error', 'message': str(e)}
            self.wfile.write(json.dumps(response).encode('utf-8'))
    
    def _handle_execute_action(self):
        """å¤„ç†è¡ŒåŠ¨æ‰§è¡Œè¯·æ±‚"""
        try:
            # è¯»å–è¯·æ±‚ä½“
            content_length = int(self.headers.get('Content-Length', 0))
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode('utf-8'))
            
            # è·å–å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_name = data.get('tool_name')
            arguments = data.get('arguments', {})
            action_id = data.get('action_id', 'unknown')
            
            if not tool_name:
                raise ValueError("ç¼ºå°‘ tool_name")
            
            # æ‰§è¡Œå·¥å…·
            if not self.tool_executor:
                raise ValueError("å·¥å…·æ‰§è¡Œå™¨æœªåˆå§‹åŒ–")
            
            result = self.tool_executor.execute(tool_name, arguments)
            
            # è®°å½•æ‰§è¡Œç»“æœåˆ°æ•°æ®åº“
            self._store_reading_behavior({
                "report_id": data.get('report_id', 'unknown'),
                "item_id": action_id,
                "action": "execute_action",
                "feedback_type": "success" if result.success else "failed",
                "section": "action_items",
                "metadata": json.dumps({
                    "tool_name": tool_name,
                    "arguments": arguments,
                    "result": result.to_dict() if hasattr(result, 'to_dict') else str(result),
                }),
            })
            
            # Phase 2.3: è®°å½•åé¦ˆå¹¶å¼ºåŒ–æƒé‡
            if self.feedback_reinforcer:
                action_type = self._get_action_type_from_tool(tool_name)
                self.feedback_reinforcer.record_action_feedback(
                    action_id=action_id,
                    action_type=action_type,
                    feedback_type="execute",
                    tool_name=tool_name,
                    success=result.success,
                )
            
            # è¿”å›æ‰§è¡Œç»“æœ
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            response = {
                'status': 'success' if result.success else 'error',
                'message': result.message,
                'data': result.to_dict() if hasattr(result, 'to_dict') else {'result': str(result)},
            }
            self.wfile.write(json.dumps(response).encode('utf-8'))
            
            logger.info(f"âœ“ æ‰§è¡Œè¡ŒåŠ¨: {tool_name} - {action_id} ({'æˆåŠŸ' if result.success else 'å¤±è´¥'})")
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œè¡ŒåŠ¨å¤±è´¥: {e}", exc_info=True)
            self.send_response(500)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            response = {'status': 'error', 'message': str(e)}
            self.wfile.write(json.dumps(response).encode('utf-8'))
    
    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        parsed_path = urlparse(self.path)
        
        if parsed_path.path == '/api/deep_dive_history':
            self._handle_deep_dive_history(parsed_path)
            return
        
        if parsed_path.path == '/api/memory/metrics':
            self._handle_memory_metrics()
            return
        
        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        
        response = {'status': 'ok', 'message': 'Tracking server is running'}
        self.wfile.write(json.dumps(response).encode('utf-8'))

    def _handle_memory_metrics(self):
        """è¿”å›è®°å¿†ç³»ç»Ÿå¥åº·çŠ¶æ€å’Œè´¨é‡æŒ‡æ ‡"""
        try:
            result = get_memory_health_api()
            
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            self.wfile.write(json.dumps(result, ensure_ascii=False).encode('utf-8'))
        except Exception as e:
            logger.error(f"è·å–è®°å¿†æŒ‡æ ‡å¤±è´¥: {e}")
            
            self.send_response(500)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            
            response = {'status': 'error', 'message': str(e)}
            self.wfile.write(json.dumps(response).encode('utf-8'))
    
    def _handle_deep_dive_history(self, parsed_path):
        """è¿”å›æœ€è¿‘çš„æ·±åº¦ç ”ç©¶è®°å½•"""
        params = parse_qs(parsed_path.query)
        try:
            limit = int(params.get('limit', ['20'])[0])
        except ValueError:
            limit = 20
        limit = max(1, min(limit, 100))
        
        history = self._read_deep_dive_history(limit)
        
        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        
        response = {'status': 'success', 'history': history}
        self.wfile.write(json.dumps(response).encode('utf-8'))
    
    def _get_action_type_from_tool(self, tool_name: str) -> str:
        """ä»å·¥å…·åç§°è·å–è¡ŒåŠ¨ç±»å‹"""
        mapping = {
            "create_github_issue": "github_issue",
            "send_calendar_invite": "calendar",
            "add_to_reading_list": "reading_list",
        }
        return mapping.get(tool_name, "other")
    
    def _handle_deep_dive_request(self, data: dict) -> dict:
        """åŒæ­¥å¤„ç†æ·±åº¦ç ”ç©¶è¯·æ±‚ï¼Œè¿”å›ç ”ç©¶ç»“æœ"""
        # 1. æå–URLå’Œæ ‡é¢˜
        metadata = data.get("metadata") or {}
        if isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except json.JSONDecodeError:
                metadata = {"raw": metadata}
        
        item_url = data.get("url") or metadata.get("item_url")
        item_title = metadata.get("item_title", "Unknown")
        request_id = str(uuid4())
        started_at = datetime.now(timezone.utc)
        
        if not item_url:
            user_message = "ç¼ºå°‘æ–‡ç« URL"
            self._append_deep_dive_history({
                "request_id": request_id,
                "status": "error",
                "title": item_title,
                "url": None,
                "error_message": "missing url in payload",
                "user_message": user_message,
                "finished_at": datetime.now(timezone.utc).isoformat(),
            })
            return {
                "status": "error",
                "message": user_message,
                "request_id": request_id,
            }
        
        logger.info(f"ğŸ”¬ å¼€å§‹æ·±åº¦ç ”ç©¶: {item_title[:50]}...")
        
        # 2. è°ƒç”¨ research-assistant
        try:
            result = self._run_research_assistant(item_url, item_title)
            logger.info(f"âœ… ç ”ç©¶å®Œæˆ: {item_title[:50]}...")
            duration = (datetime.now(timezone.utc) - started_at).total_seconds()
            self._append_deep_dive_history({
                "request_id": request_id,
                "status": "success",
                "title": item_title,
                "url": item_url,
                "report_path": result["report_path"],
                "duration": duration,
                "finished_at": datetime.now(timezone.utc).isoformat(),
            })
            return {
                "status": "success",
                "markdown": result["markdown"],
                "report_path": result["report_path"],
                "request_id": request_id,
            }
        except Exception as primary_error:
            logger.error(f"æ·±åº¦ç ”ç©¶å¤±è´¥: {primary_error}", exc_info=True)
            try:
                fallback = self._run_llm_fallback(item_url, item_title)
                duration = (datetime.now(timezone.utc) - started_at).total_seconds()
                self._append_deep_dive_history({
                    "request_id": request_id,
                    "status": "success",
                    "title": item_title,
                    "url": item_url,
                    "report_path": fallback["report_path"],
                    "duration": duration,
                    "mode": "fallback_llm",
                    "finished_at": datetime.now(timezone.utc).isoformat(),
                })
                return {
                    "status": "success",
                    "markdown": fallback["markdown"],
                    "report_path": fallback["report_path"],
                    "request_id": request_id,
                    "mode": "fallback_llm",
                    "message": "ç ”ç©¶åŠ©æ‰‹å¤±è´¥ï¼Œå·²ä½¿ç”¨LLMå¤‡ç”¨æ–¹æ¡ˆç”ŸæˆæŠ¥å‘Š",
                }
            except Exception as fallback_error:
                logger.error(f"LLMå¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_error}", exc_info=True)
                error_info = self._format_deep_dive_error(str(fallback_error))
                duration = (datetime.now(timezone.utc) - started_at).total_seconds()
                log_excerpt, log_path = self._read_recent_log_excerpt()
                combined_error = f"{primary_error}; fallback={fallback_error}"
                self._append_deep_dive_history({
                    "request_id": request_id,
                    "status": "error",
                    "title": item_title,
                    "url": item_url,
                    "error_message": combined_error,
                    "user_message": error_info["message"],
                    "log_path": log_path,
                    "log_excerpt": log_excerpt,
                    "duration": duration,
                    "finished_at": datetime.now(timezone.utc).isoformat(),
                })
                error_payload = {
                    "status": "error",
                    "message": error_info["message"],
                    "hint": error_info["hint"] + "ï¼ˆLLMå¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥ï¼‰",
                    "request_id": request_id,
                }
                if log_path:
                    error_payload["log_path"] = log_path
                if log_excerpt:
                    error_payload["log_excerpt"] = log_excerpt
                return error_payload
    
    def _handle_architect_analysis_request(self, data: dict) -> dict:
        """
        å¤„ç†æ¶æ„å¸ˆåˆ†æè¯·æ±‚
        
        ä»AIç³»ç»Ÿæ¶æ„å¸ˆè§†è§’åˆ†ææ–°é—»/è®ºæ–‡ï¼š
        1. æ¶æ„æ¼”è¿›ï¼šè§£å†³äº†ä»€ä¹ˆç—›ç‚¹
        2. è½åœ°åœºæ™¯ï¼šèƒ½è·‘é€šä»€ä¹ˆæ–°çš„Agent Workflow
        3. è®¾è®¡æ¨¡å¼ï¼šéœ€è¦ä»€ä¹ˆæ–°çš„åŸºç¡€è®¾æ–½
        """
        # 1. æå–å…ƒæ•°æ®
        metadata = data.get("metadata") or {}
        if isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except json.JSONDecodeError:
                metadata = {"raw": metadata}
        
        item_url = data.get("url") or metadata.get("item_url")
        item_title = metadata.get("item_title", "Unknown")
        item_source = metadata.get("item_source", "Unknown")
        item_summary = metadata.get("summary", "")
        request_id = str(uuid4())
        started_at = datetime.now(timezone.utc)
        
        if not item_url:
            user_message = "ç¼ºå°‘æ–‡ç« URL"
            self._append_deep_dive_history({
                "request_id": request_id,
                "status": "error",
                "title": item_title,
                "url": None,
                "error_message": "missing url in payload",
                "user_message": user_message,
                "finished_at": datetime.now(timezone.utc).isoformat(),
                "analysis_type": "architect",
            })
            return {
                "status": "error",
                "message": user_message,
                "request_id": request_id,
            }
        
        logger.info(f"ğŸ—ï¸ å¼€å§‹æ¶æ„å¸ˆåˆ†æ: {item_title[:50]}...")
        
        # 2. ä½¿ç”¨LLMç”Ÿæˆæ¶æ„å¸ˆåˆ†æ
        try:
            markdown = self._generate_architect_analysis(item_title, item_url, item_source, item_summary)
            report_path = self._save_deep_dive_report(item_title, markdown, mode="architect")
            
            duration = (datetime.now(timezone.utc) - started_at).total_seconds()
            self._append_deep_dive_history({
                "request_id": request_id,
                "status": "success",
                "title": item_title,
                "url": item_url,
                "report_path": report_path,
                "duration": duration,
                "mode": "architect_analysis",
                "finished_at": datetime.now(timezone.utc).isoformat(),
            })
            
            logger.info(f"âœ… æ¶æ„å¸ˆåˆ†æå®Œæˆ: {item_title[:50]}...")
            
            return {
                "status": "success",
                "markdown": markdown,
                "report_path": report_path,
                "request_id": request_id,
                "mode": "architect_analysis",
            }
        except Exception as e:
            logger.error(f"æ¶æ„å¸ˆåˆ†æå¤±è´¥: {e}", exc_info=True)
            error_info = self._format_deep_dive_error(str(e))
            duration = (datetime.now(timezone.utc) - started_at).total_seconds()
            log_excerpt, log_path = self._read_recent_log_excerpt()
            
            self._append_deep_dive_history({
                "request_id": request_id,
                "status": "error",
                "title": item_title,
                "url": item_url,
                "error_message": str(e),
                "user_message": error_info["message"],
                "log_path": log_path,
                "log_excerpt": log_excerpt,
                "duration": duration,
                "mode": "architect_analysis",
                "finished_at": datetime.now(timezone.utc).isoformat(),
            })
            
            error_payload = {
                "status": "error",
                "message": error_info["message"],
                "hint": error_info["hint"],
                "request_id": request_id,
            }
            if log_path:
                error_payload["log_path"] = log_path
            if log_excerpt:
                error_payload["log_excerpt"] = log_excerpt
            return error_payload
    
    def _generate_architect_analysis(self, title: str, url: str, source: str, summary: str) -> str:
        """
        ä½¿ç”¨LLMç”ŸæˆAIç³»ç»Ÿæ¶æ„å¸ˆè§†è§’çš„åˆ†æ
        """
        llm_client = get_llm_client()
        
        # æ„å»ºæ¶æ„å¸ˆåˆ†æçš„ä¸“ç”¨prompt
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIç³»ç»Ÿæ¶æ„å¸ˆã€‚è¯·ä»ç³»ç»Ÿè®¾è®¡çš„è§’åº¦åˆ†æä»¥ä¸‹AIæ–°é—»/è®ºæ–‡ï¼š

**æ ‡é¢˜**: {title}
**æ¥æº**: {source}
**æ‘˜è¦**: {summary}
**åŸæ–‡é“¾æ¥**: {url}

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š

## 1. ğŸ—ï¸ æ¶æ„æ¼”è¿› (Architecture Evolution)

- è¿™ä¸ªæ–°æ¨¡å‹/å·¥å…·è§£å†³äº†ä»¥å‰AIå¼€å‘ä¸­çš„å“ªä¸ªç—›ç‚¹ï¼Ÿ
- æ˜¯è®°å¿†ä¸¢å¤±ï¼Ÿæ˜¯å¹»è§‰ï¼Ÿæ˜¯ç¼–æ’å¤ªéš¾ï¼Ÿè¿˜æ˜¯æˆæœ¬/å»¶è¿Ÿé—®é¢˜ï¼Ÿ
- åœ¨AIç³»ç»Ÿæ¶æ„çš„å“ªä¸€å±‚ï¼ˆè®¡ç®—å±‚/è®°å¿†å±‚/å·¥å…·å±‚/ç›‘æ§å±‚ï¼‰äº§ç”Ÿäº†å½±å“ï¼Ÿ
- ç›¸æ¯”ä¹‹å‰çš„æ–¹æ¡ˆï¼Œæ¶æ„ä¸Šæœ‰ä»€ä¹ˆæœ¬è´¨æ€§çš„æ”¹è¿›ï¼Ÿ

## 2. ğŸš€ è½åœ°åœºæ™¯ (Practical Applications)

- åŸºäºè¿™ä¸ªæ–°èƒ½åŠ›ï¼Œä»¥å‰åšä¸åˆ°çš„å“ªäº›Agent Workflowç°åœ¨å¯ä»¥è·‘é€šäº†ï¼Ÿ
- å…·ä½“å¯ä»¥åº”ç”¨åœ¨ä»€ä¹ˆåœºæ™¯ï¼Ÿï¼ˆå¦‚ï¼šå®æ—¶å¯¹è¯ã€é•¿æ–‡æ¡£åˆ†æã€å¤šæ­¥æ¨ç†ç­‰ï¼‰
- å¯¹ç°æœ‰AIåº”ç”¨çš„æ”¹è¿›ç©ºé—´åœ¨å“ªé‡Œï¼Ÿ
- æœ‰å“ªäº›å®é™…çš„ä½¿ç”¨æ¡ˆä¾‹æˆ–æ½œåœ¨åº”ç”¨ï¼Ÿ

## 3. âš™ï¸ è®¾è®¡æ¨¡å¼ä¸åŸºç¡€è®¾æ–½ (Design Patterns & Infrastructure)

- å¦‚æœè¦æŠŠè¿™ä¸ªæ–°æŠ€æœ¯é›†æˆåˆ°ä¼ä¸šçº§åº”ç”¨ï¼Œéœ€è¦è€ƒè™‘å“ªäº›æ–°çš„åŸºç¡€è®¾æ–½ï¼Ÿ
- æ˜¯å¦éœ€è¦æ›´å¤§çš„å‘é‡æ•°æ®åº“ï¼Ÿæ–°çš„ç›‘æ§å·¥å…·ï¼Ÿä¸åŒçš„ç¼–æ’æ¡†æ¶ï¼Ÿ
- æœ‰å“ªäº›æ¶æ„ä¸Šçš„æƒè¡¡ï¼ˆTrade-offsï¼‰ï¼Ÿï¼ˆå¦‚ï¼šé€Ÿåº¦vså‡†ç¡®ç‡ã€æˆæœ¬vsæ€§èƒ½ï¼‰
- éœ€è¦ä»€ä¹ˆæ ·çš„æŠ€æœ¯æ ˆå’Œå·¥å…·é“¾æ”¯æŒï¼Ÿ

## 4. ğŸ’¡ ç³»ç»Ÿè®¾è®¡å¯ç¤º

- å¯¹äºæ„å»ºAIç³»ç»Ÿçš„å¼€å‘è€…å’Œæ¶æ„å¸ˆï¼Œè¿™ä¸ªæŠ€æœ¯å¸¦æ¥äº†ä»€ä¹ˆå¯ç¤ºï¼Ÿ
- åœ¨è®¾è®¡AIåº”ç”¨æ—¶ï¼Œåº”è¯¥å¦‚ä½•è€ƒè™‘è¿™ä¸ªæ–°èƒ½åŠ›ï¼Ÿ
- æœ‰å“ªäº›éœ€è¦æ³¨æ„çš„å‘æˆ–æœ€ä½³å®è·µï¼Ÿ

è¯·ç”¨æ¸…æ™°ã€ç»“æ„åŒ–çš„Markdownæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼Œå¸®åŠ©è¯»è€…å»ºç«‹"AIç³»ç»Ÿæ¶æ„å¸ˆ"çš„æ€ç»´æ¨¡å¼ã€‚
åˆ†æè¦å…·ä½“ã€æ·±å…¥ï¼Œé¿å…æ³›æ³›è€Œè°ˆã€‚å¦‚æœæŸä¸ªç»´åº¦ä¸é€‚ç”¨ï¼Œè¯·è¯´æ˜åŸå› ã€‚
"""
        
        logger.info(f"ğŸ“‹ ä½¿ç”¨ LLM è¿›è¡Œæ¶æ„å¸ˆåˆ†æ")
        
        # è°ƒç”¨LLMç”Ÿæˆåˆ†æ
        analysis_text = asyncio.run(llm_client.chat_completion(prompt=prompt)).strip()
        
        # æ„å»ºæœ€ç»ˆçš„MarkdownæŠ¥å‘Š
        markdown = f"""# ğŸ—ï¸ AIç³»ç»Ÿæ¶æ„å¸ˆåˆ†æ

## {title}

**æ¥æº**: {source}  
**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{analysis_text}

---

> **åŸæ–‡é“¾æ¥**: [{title}]({url})
> 
> **è¯´æ˜**: æœ¬åˆ†æä»AIç³»ç»Ÿæ¶æ„å¸ˆçš„è§†è§’å‡ºå‘ï¼Œå¸®åŠ©ç†è§£æ–°æŠ€æœ¯çš„ç³»ç»Ÿè®¾è®¡ä»·å€¼å’Œå®è·µå¯ç¤ºã€‚
"""
        
        return markdown
    
    def _run_research_assistant(self, url: str, title: str) -> dict:
        """è°ƒç”¨ research-assistant ç”ŸæˆæŠ¥å‘Š"""
        import subprocess
        import re
        from datetime import datetime
        
        # å‡†å¤‡ç ”ç©¶åŠ©æ‰‹ç›®å½•ä¸ç»Ÿä¸€è¾“å‡ºç›®å½•
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿æ­£ç¡®
        current_file = Path(__file__).resolve()
        research_root = current_file.parents[3] / "research-assistant"
        # æ‰€æœ‰æ·±åº¦ç ”ç©¶æŠ¥å‘Šç»Ÿä¸€ä¿å­˜åˆ° ai-digest/deep_dive_reports
        output_dir = current_file.parents[2] / "deep_dive_reports"
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ“ æ·±åº¦ç ”ç©¶æŠ¥å‘Šè¾“å‡ºç›®å½•: {output_dir}")
        
        # è°ƒç”¨ research-assistant/main.py
        research_assistant_path = research_root / "main.py"
        
        cmd = [
            sys.executable,
            str(research_assistant_path),
            "--url", url,
            "--report-dir", str(output_dir),
            "--json-output"
        ]
        
        logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd[:4])}...")
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(research_assistant_path.parent)
            )
        except subprocess.TimeoutExpired:
            raise RuntimeError("ç ”ç©¶è¶…æ—¶ï¼ˆ120ç§’ï¼‰")
        
        if result.returncode != 0:
            error_output = result.stderr or result.stdout
            logger.error(f"Research assistant å¤±è´¥: {error_output[:500]}")
            raise RuntimeError(f"Research assistant failed: {error_output[:200]}")
        
        # è§£æ JSON è¾“å‡º
        try:
            output_lines = result.stdout.strip().split('\n')
            # æŸ¥æ‰¾JSONè¾“å‡ºï¼ˆæœ€åä¸€è¡Œåº”è¯¥æ˜¯JSONï¼‰
            json_line = None
            for line in reversed(output_lines):
                if line.strip().startswith('{'):
                    json_line = line
                    break
            
            if not json_line:
                raise ValueError("æœªæ‰¾åˆ°JSONè¾“å‡º")
            
            output_data = json.loads(json_line)
            return {
                "markdown": output_data.get("markdown", ""),
                "report_path": output_data.get("report_path", "")
            }
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"è§£æè¾“å‡ºå¤±è´¥: {e}")
            logger.error(f"è¾“å‡ºå†…å®¹: {result.stdout[:500]}")
            raise RuntimeError(f"æ— æ³•è§£æç ”ç©¶ç»“æœ: {str(e)}")

    def _run_llm_fallback(self, url: str, title: str) -> dict:
        """å½“ research-assistant å¤±è´¥æ—¶ï¼Œä½¿ç”¨ LLM ç›´æ¥ç”ŸæˆæŠ¥å‘Š"""
        logger.info(f"ğŸ” ä½¿ç”¨LLMå¤‡ç”¨æ–¹æ¡ˆè¿›è¡Œæ·±åº¦ç ”ç©¶: {title[:50]}...")
        article_html = self._fetch_article_html(url)
        article_text = self._extract_article_text(article_html)
        if len(article_text) < 200:
            raise RuntimeError("å¤‡ç”¨æ–¹æ¡ˆ: æ— æ³•æå–æ–‡ç« æ­£æ–‡æˆ–æ­£æ–‡è¿‡çŸ­")
        markdown = self._summarize_with_llm(title, url, article_text)
        report_path = self._save_deep_dive_report(title, markdown, mode="llm")
        return {
            "markdown": markdown,
            "report_path": report_path,
        }

    def _fetch_article_html(self, url: str) -> str:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text

    def _extract_article_text(self, html: str) -> str:
        soup = BeautifulSoup(html, 'lxml')
        candidates = [soup.find('article'), soup.find('main'), soup.find('section')]
        candidate = next((c for c in candidates if c), soup.body)
        if not candidate:
            return ''
        paragraphs = []
        for tag in candidate.find_all(['p', 'li']):
            text = tag.get_text(' ', strip=True)
            if len(text) >= 40 and not text.startswith('var '):
                paragraphs.append(text)
        return '\n'.join(paragraphs)

    def _summarize_with_llm(self, title: str, url: str, article_text: str) -> str:
        llm_client = get_llm_client()
        excerpt = article_text.strip()
        if len(excerpt) > 6000:
            excerpt = excerpt[:6000]
        prompt = (
            "ä½ æ˜¯ä¸€åèµ„æ·±AIç ”ç©¶åˆ†æå¸ˆï¼Œè¯·æ ¹æ®æä¾›çš„æ–‡ç« å†…å®¹è¾“å‡ºMarkdownæ ¼å¼çš„æ·±åº¦æŠ¥å‘Šï¼Œå¿…é¡»ç”¨ä¸­æ–‡æ’°å†™ã€‚\n"
            "ç»“æ„ï¼š\n"
            "1. ### å…³é”®ä¿¡æ¯ - åˆ—å‡ºæœ€é‡è¦çš„3-5æ¡ç»“è®º\n"
            "2. ### èƒŒåé€»è¾‘ - è§£é‡Šå…³é”®æŠ€æœ¯/è§‚ç‚¹çš„åŸç†ä¸é™åˆ¶\n"
            "3. ### å¯¹æˆ‘çš„ä»·å€¼ - Davidå…³æ³¨RAGã€Agentã€ä¼ä¸šAIè½åœ°ï¼Œè¯´æ˜å¯å‘\n"
            "4. ### ä¸‹ä¸€æ­¥å»ºè®® - æä¾›å¯æ‰§è¡Œè¡ŒåŠ¨æˆ–è¯´æ˜ä¸å»ºè®®çš„åŸå› \n\n"
            f"æ–‡ç« æ ‡é¢˜: {title}\n"
            f"æ–‡ç« é“¾æ¥: {url}\n"
            f"æ–‡ç« æ­£æ–‡:\n{excerpt}\n"
        )
        summary_text = asyncio.run(llm_client.chat_completion(prompt=prompt)).strip()
        markdown = (
            f"### {title}\n\n"
            f"{summary_text or 'ï¼ˆLLMæœªè¿”å›å†…å®¹ï¼‰'}\n\n"
            f"> åŸæ–‡é“¾æ¥ï¼š[ç‚¹å‡»æŸ¥çœ‹]({url})"
        )
        return markdown


    def _save_deep_dive_report(self, title: str, markdown: str, mode: str = 'llm') -> str:
        safe_title = re.sub(r'[^a-zA-Z0-9]+', '-', title).strip('-') or 'deep-dive'
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{timestamp}_{mode}_{safe_title[:40].lower()}" + '.md'
        # æ‰€æœ‰æ·±åº¦ç ”ç©¶æŠ¥å‘Šç»Ÿä¸€ä¿å­˜åˆ° ai-digest/deep_dive_reports
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿æ­£ç¡®
        current_file = Path(__file__).resolve()
        # tracking_server.py åœ¨ ai-digest/src/tracking/ ä¸‹ï¼Œéœ€è¦å‘ä¸Š2çº§åˆ° ai-digest
        output_dir = current_file.parents[2] / 'deep_dive_reports'
        output_dir.mkdir(parents=True, exist_ok=True)
        report_path = output_dir / filename
        report_path.write_text(markdown, encoding='utf-8')
        logger.info(f"âœ“ æ·±åº¦ç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return str(report_path)


    
    @classmethod
    def _format_deep_dive_error(cls, raw: str) -> dict:
        """å°†åº•å±‚å¼‚å¸¸è½¬æ¢ä¸ºé¢å‘ç”¨æˆ·çš„å‹å¥½æç¤ºå’Œå»ºè®®ã€‚"""
        lowered = raw.lower()
        if "timeout" in lowered or "timed out" in lowered:
            return {
                "message": "ç ”ç©¶è¶…æ—¶ï¼šæ¥æºæœåŠ¡å™¨é•¿æ—¶é—´æ— å“åº”",
                "hint": "è¯·ç¨åå†è¯•ï¼Œæˆ–æ£€æŸ¥ç›®æ ‡ç«™ç‚¹æ˜¯å¦å¯è®¿é—®",
            }
        if any(keyword in lowered for keyword in ("connection", "network", "dns")):
            return {
                "message": "æ— æ³•è¿æ¥åˆ°æ¥æºç«™ç‚¹ï¼Œå¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šæˆ–ç«™ç‚¹ä¸´æ—¶ä¸å¯ç”¨",
                "hint": "è¯·æ£€æŸ¥ç½‘ç»œç¯å¢ƒï¼Œæˆ–å¤åˆ¶é“¾æ¥åœ¨æµè§ˆå™¨ä¸­å°è¯•è®¿é—®",
            }
        if "reddit" in lowered and ("blocked" in lowered or "éœ€è¦ç™»å½•" in lowered):
            return {
                "message": "Reddit æ‹’ç»äº†è‡ªåŠ¨è®¿é—®ï¼Œéœ€ç™»å½•æˆ–å¼€å‘è€…ä»¤ç‰Œ",
                "hint": "è¯·æ”¹ç”¨å…¬å¼€å¯è®¿é—®çš„è®¨è®ºé“¾æ¥ï¼Œæˆ–å°†åŸæ–‡å¤åˆ¶åˆ°è‡ªå»ºæ–‡æ¡£åå†ç ”ç©¶",
            }
        if "403" in raw or "404" in raw:
            paywalled_domains = ("ft.com", "wsj.com", "bloomberg.com", "economist.com")
            if any(domain in raw for domain in paywalled_domains):
                return {
                    "message": "æ¥æºé¡µé¢ä½äºä»˜è´¹å¢™ä¹‹åï¼Œæ— æ³•è‡ªåŠ¨æŠ“å–",
                    "hint": "è¯·æä¾›å…¬å¼€å¯è®¿é—®çš„é“¾æ¥ï¼Œæˆ–å°†æ–‡ç« å†…å®¹å¤åˆ¶åˆ°è‡ªå»ºæ–‡æ¡£åå†å‘èµ·æ·±åº¦ç ”ç©¶",
                }
            return {
                "message": "æ¥æºé¡µé¢æ— æ³•è®¿é—®ï¼ˆå¯èƒ½ä¸å­˜åœ¨æˆ–è¢«é™åˆ¶è®¿é—®ï¼‰",
                "hint": "ç¡®è®¤é“¾æ¥æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•æ›¿æ¢ä¸ºå…¬å¼€å¯è®¿é—®çš„æ¥æº",
            }
        if "research assistant failed" in raw:
            return {
                "message": "ç ”ç©¶åŠ©æ‰‹æ‰§è¡Œå¤±è´¥ï¼Œè¯¦ç»†é”™è¯¯å·²è®°å½•åœ¨è¿½è¸ªæœåŠ¡å™¨æ—¥å¿—",
                "hint": "å¯åœ¨æ—¥å¿—é¢æ¿ä¸­æŸ¥çœ‹è¯¦æƒ…ï¼Œæˆ–ç¨åé‡è¯•",
            }
        return {
            "message": "å†…å®¹è§£æå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ï¼ˆè¯¦ç»†æ—¥å¿—å·²è®°å½•ï¼‰",
            "hint": "å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·å°†æ—¥å¿—æˆªå›¾åé¦ˆç»™å¼€å‘è€…",
        }
    
    @classmethod
    def _append_deep_dive_history(cls, entry: dict) -> None:
        entry.setdefault("recorded_at", datetime.now(timezone.utc).isoformat())
        cls.history_log_path.parent.mkdir(parents=True, exist_ok=True)
        with cls.history_log_path.open('a', encoding='utf-8') as fp:
            fp.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    @classmethod
    def _read_deep_dive_history(cls, limit: int) -> list:
        if not cls.history_log_path.exists():
            return []
        history = []
        with cls.history_log_path.open('r', encoding='utf-8') as fp:
            for line in reversed(fp.readlines()):
                line = line.strip()
                if not line:
                    continue
                try:
                    history.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
                if len(history) >= limit:
                    break
        return history
    
    @classmethod
    def _read_recent_log_excerpt(cls, max_lines: int = 40) -> Tuple[Optional[str], Optional[str]]:
        """è¯»å–æœ€è¿‘çš„è¿½è¸ªæ—¥å¿—å†…å®¹ï¼Œè¿”å›(å†…å®¹, è·¯å¾„)ã€‚"""
        for candidate in cls.log_candidates:
            if not candidate:
                continue
            try:
                if candidate.exists():
                    with candidate.open('r', encoding='utf-8', errors='ignore') as fp:
                        lines = fp.readlines()[-max_lines:]
                        excerpt = ''.join(lines).strip()
                        if excerpt:
                            return excerpt, str(candidate)
            except OSError:
                continue
        return None, None
    
    def log_message(self, format, *args):
        """ç¦ç”¨é»˜è®¤æ—¥å¿—"""
        pass
    
    def _store_reading_behavior(self, data: dict) -> None:
        """ä¼˜å…ˆå†™å…¥çƒ­ç¼“å­˜ï¼Œå¿…è¦æ—¶åˆ·æ–°åˆ°æŒä¹…å±‚ã€‚"""
        if self.hot_cache:
            self.hot_cache.store("reading_behavior", data)
            self._maybe_flush_hot_cache()
        else:
            self.db.save_reading_behavior(data)
    
    def _maybe_flush_hot_cache(self, force: bool = False) -> None:
        if not self.hot_cache:
            return
        if not force and self.hot_cache.get_size("reading_behavior") < self.hot_cache_flush_threshold:
            return
        sync_hot_cache_to_warm_storage(
            self.hot_cache,
            self.db,
            behavior_batch_size=self.hot_cache_flush_threshold,
        )


def run_server(port: int = 8000, tool_config: Optional[dict] = None):
    """è¿è¡Œè¿½è¸ªæœåŠ¡å™¨"""
    # åˆå§‹åŒ–æ•°æ®åº“
    db = FeedbackDB()
    TrackingHandler.set_db(db)
    hot_cache = HotMemoryCache()
    flush_threshold = int(os.getenv("HOT_CACHE_FLUSH_THRESHOLD", "400"))
    TrackingHandler.set_hot_cache(hot_cache, flush_threshold)
    
    # åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨ï¼ˆå¦‚æœæä¾›é…ç½®ï¼‰
    tool_executor = None
    if tool_config:
        from src.agents.tool_executor import ToolExecutor
        tool_executor = ToolExecutor(config=tool_config)
        TrackingHandler.set_tool_executor(tool_executor)
        logger.info("âœ“ å·¥å…·æ‰§è¡Œå™¨å·²åŠ è½½")
    
    # Phase 2.3: åˆå§‹åŒ–åé¦ˆå¼ºåŒ–å™¨
    from src.learning.weight_adjuster import WeightAdjuster
    weight_adjuster = WeightAdjuster()
    feedback_reinforcer = FeedbackReinforcer(db=db, weight_adjuster=weight_adjuster)
    TrackingHandler.set_feedback_reinforcer(feedback_reinforcer)
    logger.info("âœ“ åé¦ˆå¼ºåŒ–å™¨å·²åŠ è½½")
    
    server_address = ('', port)
    httpd = HTTPServer(server_address, TrackingHandler)
    
    logger.info(f"ğŸš€ è¿½è¸ªæœåŠ¡å™¨å¯åŠ¨: http://localhost:{port}")
    logger.info(f"   API ç«¯ç‚¹: http://localhost:{port}/api/track")
    logger.info(f"   è¡ŒåŠ¨æ‰§è¡Œ: http://localhost:{port}/api/execute_action")
    logger.info("   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        logger.info("\nâœ“ è¿½è¸ªæœåŠ¡å™¨å·²åœæ­¢")
        httpd.shutdown()
    finally:
        if hot_cache.get_size():
            logger.info("â†» æ­£åœ¨åˆ·æ–°çƒ­ç¼“å­˜ä¸­çš„è¿½è¸ªæ•°æ®â€¦")
        sync_hot_cache_to_warm_storage(hot_cache, db)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Digest Tracking Server')
    parser.add_argument('--port', type=int, default=8000, help='æœåŠ¡å™¨ç«¯å£')
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    run_server(port=args.port)